---
layout: post
title:  "Numerical Relativity 101: How to simulate spacetime itself"
date:   2024-07-21 18:33:23 +0000
categories: C++
---

Papers:

https://arxiv.org/pdf/1307.7391 ccz4 paper, contains some good equation references
https://iopscience.iop.org/article/10.1088/1361-6382/ac7e16/pdf covariant bssn
https://www2.yukawa.kyoto-u.ac.jp/~masaru.shibata/PRD52.5428.pdf cG mentions the motivation behind the gamma term
stability paper https://arxiv.org/pdf/0707.0339
http://wwwteor.mi.infn.it/~molinari/TESI/Danieli_tesi.pdf 4.27 relates conjugate momentum and extrinsic curvature
https://arxiv.org/pdf/2210.10103 adm stuff
https://arxiv.org/pdf/gr-qc/0604035 adm evolution equations

Ok planola time:

1. Basic introduction to the ADM formalism. Need to cover projection, the concept of time + gauge conditions, physical interpretation of the gauge conditions as an eularian observer, and the concept of a hypersurface
2. Introduce the adm equations
3. Introduce the vanilla BSSN equations
4. Talk through all the notation
5. Swap to structuring a simulation
6. Talk through the high level flow
7. Initial conditions: waves
8. Boundary conditions: periodic
9. Evolution equations: Bssn, momentum constraint damping, algebraic constraints, kreiss-oliger dissipation
10. Gauge condition evolution
11. Some kind of diagnostic (semi analytic tests?)

Hi there! Today we're going to be getting into numerical relativity, which is the art of simulating the evolution of spacetime itself. While there a quite a few known solutions to the equations of general relativity at this point (eg the Schwarzschild metric), many of the most interesting regions of general relativity can only be explored by exploring how spacetime itself evolves dynamically through simulations. Binary black hole collisions are a perfect example of this, and are a big part of the reason why we're building this

These kinds of simulations are traditionally run on supercomputers, but luckily you likely have a supercomputer in the form of a GPU[^specs] in your PC. The amount of extra performance we can get over a traditional CPU based implementation is likely to be a few orders of magnitude higher if you know what you're doing, so in this article I'm going to present to you how to get this done

[^specs]: You'll need 8+ GB of vram for future bbh collisions, though you can get away with minimal specs for this article. This was written and tested on a 6700xt, but previous work was on an r9 390. Any OpenCL 1.2 compatible device will work here, though I do use 64-bit atomics for accumulating diagnostics

This article is not going to assume you know all that much about general relativity, but it does require a lot of implementing

# The Einstein Field Equations Aren't Very Simulateable

The equations for general relativity are known as the [Einstein field equations](https://en.wikipedia.org/wiki/Einstein_field_equations#Mathematical_form) (EFEs). These equations are somewhat unique - they simply dictate how 4 dimensional spacetime should look declaratively - but do not provide a way to evolve a given slice of spacetime (which we might call 'now') forwards into the future. On top of that, unlike in virtually every other theory where time is a background parameter - in general relativity there isn't any independent notion of time or forwards as such[^timeorientable]. Time itself is part of the equations

[^timeorientable]: While we're looking at relatively well behaved spacetimes, in the general case relativistic manifolds are not necessarily time orientable, or simply connected - like a wormhole. These are both our of reach where we're going

The puzzle is how to extract some concept of forwards in time from the equations of general relativity to figure out how we head forwards from one moment to the next, so we can construct a system of partial differential equations that can be evolved

Here we'll be looking at the ADM (Arnowitt–Deser–Misner) formalism. It splits up a 4d spacetime into a 3+1 split: three spatial dimensions, and it singles out a time dimension in a specific way. Do note that this isn't the only way of doing it, and there are a number of successful formalisms, but ADM is the one with the most available information (and the most understandable in my opinion)

## 3+1 split

The idea is that we split our 4d spacetime into a series of 3d[^nd] surfaces (a 'foliation'), then define how one sheet evolves into the next. This evolution is defined by two variables: the lapse $\alpha$, and the shift $\beta^i$[^lapseshift], which together define our concept of forwards in time. These are known as the gauge variables. The basics of the adm formalism is to define how spacetime evolves in the direction pointed to by the gauge variables

From now on, all latin indices $_{ijk}$ have the value 1-3, and refer to quantities on our 3d slice, whereas greek indices $_{\mu\nu\kappa}$ take on the value 0-3 and refer to quantities in our 4d spacetime

ADM also introduces two other variables: The 3-metric $\gamma_{ij}$, and the extrinsic curvature $K_{ij}$. $\gamma_{ij}$ is the metric tensor associated with the 3d slice of spacetime, and specifies its curvature in 3d space. $K_{ij}$ represents how that 3d slice is embedded into our 4d spacetime. It is related to the 'velocity' of the metric tensor, and can be thought of as momentum variable

The evolution equations for ADM are then defined by taking the derivatives of $\gamma_{ij}$, and $K_{ij}$, in the direction pointed to by our lapse and shift

[^nd]: Everything here is also applicable to N dimensional spacetimes, but I know very little about anything other than 4d general relativity

[^lapseshift]: These are also often frequently labelled $N$, for the lapse, and $N^i$, for the shift

### Details

The general line element in ADM is this:

$$ds^2 = -\alpha^2 dt^2 + \gamma_{ij} (dx^i + \beta^i dt)(dx^j + \beta^j)$$

$\gamma_{ij}$ is used as a regular metric tensor on any quantity in our 3d slice. That is to say, if we have some quantity $Q_i$, we can raise and lower its indices with $\gamma_{ij}$ as follows

$$Q^i = \gamma^{ij} Q_j\\
Q_i = \gamma_{ij} Q^j$$

If our 4-metric is $g_{\mu\nu}$, then our 3-metric is $\gamma_{ij} = g_{ij}$, ie the spatial parts. While $\gamma^{ij} = (\gamma_{ij})^{-1}$, $\gamma^{ij} \neq g^{ij}$

### Definitions

The 3-metric, aka the induced metric

$$\gamma_{ij} = g_{ij}$$

We can recover the 4-metric as such:

https://indico.cern.ch/event/505595/contributions/1183661/attachments/1332828/2003830/sperhake.pdf

Todo: ate without table

$$
\begin{align}
g_{00} &= -\alpha + \beta_m \beta^m \\
g_{i0} &= \beta_i\\
g_{0j} &= \beta_j\\
g_{ij} &= \gamma_{ij}\\
\end{align}
$$

$$
\begin{align}
g^{00} &= -\alpha^2 \\
g^{i0} &= \alpha^{-2} \beta^i\\
g^{0j} &= \alpha^{-2} \beta^j\\
g^{ij} &= \gamma^{ij} - \alpha^{-2} \beta^i \beta^j\\
\end{align}
$$

$\Sigma_t$ = spatial hypersurface

Timelike normal:

$$
\begin{align}
n^\mu &= (\frac{1}{\alpha}, -\frac{\beta^i}{\alpha})\\
n_\mu &= (-\alpha, 0, 0, 0)
\end{align}
$$

Our partial derivative operator in time:

$$(\partial_t)^\mu = \alpha n^\mu + \beta^\mu$$

ADM projector, used for taking a 4-quantity and finding the associated projected quantity on the 3-surface ($\delta$ is the kronecker delta):

$$\bot^\mu_{\;\;\nu} = \delta^\mu_{\;\;\nu} + n^\mu n_{\nu}$$

Also sometimes called $\gamma^\mu_{\;\;\nu}$

## Equations of motion

Once everything is said and done, the ADM equations of motion can be brought into the following form[^reffyadm]:

[^reffyadm]: [https://arxiv.org/pdf/gr-qc/0604035](https://arxiv.org/pdf/gr-qc/0604035) 5

$$\begin{align}
\partial_t \gamma_{ij} &= \mathcal{L}_\beta \gamma_{ij} - 2 \alpha K_{ij}\\
\partial_t K_{ij} &= \mathcal{L}_\beta K_{ij} - D_i D_j \alpha + \alpha (R_{ij} - 2 K_{il} K^l_j + K K_{ij})
\end{align}
$$

We must also provide adequate gauge conditions (evolution equations for the gauge variables), and then in theory we have a fully working formalism for evolving spacetime

Unfortunately, there's a small problem

# The ADM equations of motion are bad

Part of the reason why we're not going into this more is that - unfortunately - these equations of motion are unusable to us. They are *very* numerically unstable, to the point where for our purposes there's no point trying to solve them. Understanding this issue, and solving it was an outstanding problem for decades, and there is an absolutely enormous amount written in the literature on this topic. A lot of this article series is going to be dealing with the very poor character of the equations

Luckily, because its not the early 2000s - and because the astrophysics community rocks and makes all this information public - we get to skip to the part where we have a workable set of equations

## Enter: The BSSN formalism

The BSSN formalism is the most widely used formalism - it is a direct derivative of the ADM formalism (which is why we spent time on this at all!), and most practical formalisms are substantially similar to the BSSN formalism. It was the first very successful formalism to enter the picture - though it was unclear precisely why it worked so well for a long time[^seehere]

[^seehere]: [https://arxiv.org/pdf/0707.0339](https://arxiv.org/pdf/0707.0339) this paper contains a lot of useful details, but the short version is that the BSSN equations are a partially constrained evolution system, and managing the constraints is critical to a correct evolution scheme

There isn't one canonical BSSN formalism however, so its time to run through our options

## Conformal decomposition

The two ADM variables are: $\gamma_{ij}$ the metric tensor, and $\pi_{ij}$ the generalised conjugate momentum. The BSSN formalism swaps $\pi_{ij}$ for the related variable $K_{ij}$[^reffy], the extrinsic curvature tensor, as its root variable for ADM

[^reffy]: [Reference](http://wwwteor.mi.infn.it/~molinari/TESI/Danieli_tesi.pdf) 4.27

One of the insights of the BSSN formalism is that we can perform a 'conformal decomposition' of our variables $\gamma_{ij}$ and $K_{ij}$, which renders the equations more numerically stable. The conformal metric tensor is called $\tilde{\gamma}_{ij}$, and is related to the regular metric tensor by a conformal transform - We'll get back to $K_{ij}$. There are three conformal transforms I know of which are in use:

$$
\begin{align}
\tilde{\gamma}_{ij} &= e^{-4\phi} \gamma_{ij}\\
\tilde{\gamma}_{ij} &= \chi \gamma_{ij}\\
\tilde{\gamma}_{ij} &= W^2 \gamma_{ij}\\
\end{align}
$$

All defining the same conformal metric. The definition of the conformal variables in these schemes is as follows:

$$
\begin{align}
\phi &= TODO\\
\chi &= det(\gamma)^{-1/3}\\
W &= det(\gamma)^{-1/6}\\
\end{align}
$$

With this definition, the determinant of our conformal metric tensor is $\tilde{\gamma} = 1$. The inverse of the metric tensors follow a similar relation:

$$
\begin{align}
e^{-4\phi} &\tilde{\gamma}^{ij} &= \gamma^{ij}\\
\chi &\tilde{\gamma}^{ij} &= \gamma^{ij}\\
W^2 &\tilde{\gamma}^{ij} &= \gamma^{ij}\\
\end{align}
$$

The $\phi$ conformal decomposition was the first to be used, but it tends to infinity at the singularity which makes it not ideal - and appears to be falling out of use. The second version to be used was $\chi$ (range, [0,1]) which tends to 0 at the singularity, and is relatively widespread in more modern code. Lastly $W^2$ entered the picture - ostensibly it has better convergence[^converge] than $\chi$, but the really neat part about it is that renders the equations of motion of BSSN in a much less singular form. It also does not suffer from some of the issues that X has, like unphysical negative values

[^converge]: See [High-spin binary black hole mergers](https://arxiv.org/pdf/0709.2160) for this line of reasoning

Because of this, the $W^2$ formalism is clearly the best choice for us. Do note that some papers interchange notation here unfortunately, and eg $\phi$ and $\chi$ are sometimes used to mean *any* conformal factor

## Notation

$\tilde{D}_i$ = The covariant derivative, calculated in the [usual fashion](https://en.wikipedia.org/wiki/Covariant_derivative#Covariant_derivative_by_field_type) from the conformal metric tensor $\tilde{\gamma}_{ij}$. $D_i$ is the covariant derivative associated with $\gamma_{ij}$. Tilde's are very load bearing in the BSSN formalism

$\tilde{\Gamma}^i_{jk}$ are the conformal christoffel symbols of the second kind, calculated in the usual fashion from $\tilde{\gamma_{ij}}$. $\Gamma^i_{jk}$ are the christoffel symbols associated with $\gamma_{ij}$

$\tilde{D}^i$, is used to mean $\tilde{\gamma}^{ij} \tilde{D}_j$. Similarly, $D^i$, is used to mean $\gamma^{ij} D_j$, which is equivalent to $W^2\tilde{\gamma}^{ij} D_j$

While non specific to BSSN, the symmetrising operator $_{()}$ crops up here unintuitively. This is defined as $A_{(ij)}$ = $0.5 (A_{ij} + A_{ji})$. A more complex example is $2\tilde{\Gamma}^k_{m(i} \tilde{\Gamma}_{j)kn} = 0.5 * 2 (\tilde{\Gamma}^k_{mi} \tilde{\Gamma}_{jkn} + \tilde{\Gamma}^k_{mj} \tilde{\Gamma}_{ikn})$

$^{TF}$ is used to denote the trace free part of a tensor. $Q_{ij}^{TF} = Q_{ij} - \frac{1}{3} g_{ij} g^{mn} Q_{mn}$. Note that for some scalar $A$, $(AQ)^{TF} == A(Q^{TF})$, and it does not matter if you use the conformal metric, or the non conformal metric as the metric tensor here - as the conformal factor cancels out. This is also sometimes notated $Q_{<ij>}$

todo: note that we always mean the christoffel symbol, never the analytic, but note the substitution

## Variables

The BSSN variables are defined as a conformal decomposition of the ADM variables, $\gamma_{ij}$ and $K_{ij}$, via a conformal factor (here $W^2$). Our full set of variables is defined as follows:

$$
\begin{align}
&\tilde{\gamma}_{ij} &&= W^2 \gamma_{ij}\\
&\tilde{A}_{ij} &&= W^2 (K_{ij} - \frac{1}{3} \gamma_{ij} K)\\
&K &&= \gamma^{mn} K_{mn}\\
&W^2 &&= det(\gamma)^{-1/6}\\
&\tilde{\Gamma}^i &&= \tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}\\
&\alpha_{bssn} &&= \alpha_{adm}\\
&\beta^i_{bssn} &&= \beta^i_{adm}\\
\end{align}
$$

Conformal variables in general have a tilde over them, and have their indices are raised and lowered with the conformal metric tensor. In this definition, the trace of the conformal metric is $\tilde{\gamma} = 1$, and $\tilde{A}_{ij}$ is trace-free. $K$ is the trace of $K_{ij}$. $K_{ij}$, $\gamma_{ij}$, $\tilde{A}_{ij}$, and $\tilde{\gamma}_{ij}$ are all symmetric

$\tilde{\Gamma}^i$ is analytically equal to $\tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}$, but is evolved numerically instead. This is because $\partial_k \tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}$ is numerically unstable TODO: CITE

In total, we have ended up with the following constraints, that ideally are satisfied for all time, but in practice will be numerically violated

### Constraints

#### Momentum

$$\mathcal{M}_i = \tilde{\gamma}^{mn} \tilde{D}_m \tilde{A}_{ni} - \frac{2}{3} \partial_i K - \frac{3}{2} \tilde{A}^m_{\;\;i} \frac{\partial_m \chi}{\chi} = 0$$

#### Hamiltonian

$$H = R + \frac{2}{3} K^2 - \tilde{A}^{mn} \tilde{A}_{mn} = 0$$

#### Algebraic

$$\begin{align}
\mathcal{S} &= \tilde{\gamma} - 1 &= 0\\
\mathcal{A} &= \tilde{\gamma}^{mn} \tilde{A}_{mn} &= 0
\end{align}
$$

#### Christoffel symbol

$$
\mathcal{G}_i = \tilde{\Gamma}^i - \tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn} = 0
$$

Constraint damping is critical to the BSSN formalism, so we'll get back to these

## Equations of motion

I'm going to present to you the canonical form of the BSSN equations, and then discuss how you might want to rearrange them a bit in practice below

$$
\begin{align}
\partial_t W &= \beta^m \partial_m W + \frac{1}{3} W (\alpha K - \partial_m \beta^m) \\
\partial_t \tilde{\gamma}_{ij} &= \beta^m \partial_m \tilde{\gamma}_{ij}+ 2 \tilde{\gamma}_{m(i} \partial_{j)} \beta^m - \frac{2}{3} \tilde{\gamma}_{ij} \partial_m \beta^m - 2 \alpha \tilde{A}_{ij} \\
\partial_t K &= \beta^m \partial_m K - W^2 \tilde{\gamma}^{mn} D_m D_n \alpha + \alpha \tilde{A}^{mn} \tilde{A}_{mn} + \frac{1}{3} \alpha K^2\\
\partial_t \tilde{A}_{ij} &= \beta^m \partial_m \tilde{A}_{ij}+ 2 \tilde{A}_{m(i} \partial_{j)} \beta^m - \frac{2}{3} \tilde{A}_{ij} \partial_m \beta^m + \alpha K \tilde{A}_{ij} \\&- 2 \alpha \tilde{A}_{im} \tilde{A}^m_{\;\;j} + W^2 (\alpha \mathcal{R}_{ij} - D_i D_j \alpha)^{TF} \\
\partial_t \tilde{\Gamma}^i &= \beta^m \partial_m \tilde{\Gamma}^i + \frac{2}{3} \tilde{\Gamma}^i \partial_m \beta^m - \tilde{\Gamma}^m \partial_m \beta^i + \tilde{\gamma}^{mn} \partial_m \partial_n \beta^i + \frac{1}{3} \tilde{\gamma}^{im} \partial_m \partial_n \beta^n \\
&-\tilde{A}^{im}(6\alpha \frac{\partial_m W}{W} + 2 \partial_m \alpha) + 2 \alpha \tilde{\Gamma}^i_{mn} - \frac{4}{3} \alpha \tilde{\gamma}^{im} \partial_m K
\end{align}
$$

With the secondary expressions:

$$
\begin{align}
\mathcal{R}_{ij} &= \tilde{\mathcal{R}}_{ij} + \mathcal{R}^W_{ij}\\
\mathcal{R}^W_{ij} &= \frac{1}{W^2} (W (\tilde{D}_i \tilde{D}_j W + \tilde{\gamma}_{ij} \tilde{D}^m \tilde{D}_m W) - 2 \tilde{\gamma}_{ij} \partial^m W \partial_m W)\\
\tilde{\mathcal{R}}_{ij} &= -\frac{1}{2} \tilde{\gamma}^{mn} \partial_m \partial_n \tilde{\gamma}_{ij} + \tilde{\gamma}_{m(i} \partial _{j)} \tilde{\Gamma}^m + \tilde{\Gamma}^m \tilde{\Gamma}_{(ij)m}+ \tilde{\gamma}^{mn} (2 \tilde{\Gamma}^k_{m(i} \tilde{\Gamma}_{j)kn} + \tilde{\Gamma}^k_{im} \tilde{\Gamma}_{kjn})\\
D_iD_j\alpha &= \tilde{D}_i \tilde{D}_j \alpha + \frac{2}{W} \partial_{(i} W \partial_{j)} \alpha\ - \frac{1}{W} \tilde{\gamma}_{ij} \tilde{\gamma}^{mn} \partial_m W \partial_n \alpha
\end{align}
$$

### Implementation considerations

1. The quantity $D_i D_j \alpha$ is never used without being multiplied by $W^2$, which means we can eliminate the divisions by $W$

2. The quantity $\mathcal{R}^W_{ij}$ is also multiplied by $W^2$, allowing us to remove the division

#2 is unique to the $W^2$ formalism, and is a very nice feature of it. In this version of BSSN, there is only one division by $W$, in the $\tilde{\Gamma}^i$ evolution equation, and you must guard against divisions by zero by clamping $W$ to a small value (eg $10^{-4}$)[^technically]

[^technically]: Technically, as $\alpha$ is expected to tend to 0 faster than $W$ tends to 0, the quantity $\frac{\alpha}{W}$ is considered regular. In practice, clamping the conformal factor appears to be the accepted technique, though alarmingly I've found that the exact clamp value can make a large difference to your simulation

## Gauge conditions

The BSSN equations do not specify the evolution equations for the lapse and shift, as these are arbitrary. Despite in theory being arbitrary, in practice you have to very carefully pick your gauge conditions for the kind of problem you're trying to solve

Here are some common gauge conditions:

### General

#### Geodesic slicing

$$\alpha = 1, \beta^i = 0$$

### Lapse ($\alpha$)

#### Harmonic[^harmy]

[^harmy]: [Ref](https://arxiv.org/pdf/2201.08857)

$$\partial_t \alpha = - 2 \alpha^2 K + \beta^m \partial_m \alpha$$

#### 1+log

$$\partial_t \alpha = - \alpha K + \beta^m \partial_m \alpha$$

### Shift ($\beta^i$)

#### Zero shift

$$\beta^i = 0$$

#### Gamma driver (sometimes called Gamma freezing)

So called because it drives $\partial_t\tilde{\Gamma} = 0$

$$\partial_t \beta^i = \frac{3}{4} \tilde{\Gamma}^i + \beta^j \partial_j \beta^i - n \beta^i$$

$n$ is a damping parameter, that is generally set to $2$, or $2M$ where $M$ is the mass of whatever you have in your spacetime

The alternative form of this gauge condition is:

$$
\begin{align}
\partial_t \beta^i &= \beta^m \partial_m \beta^i + \frac{3}{4} B^i\\
\partial_t B^i &= \beta^m \partial_m B^i + \partial_t \tilde{\Gamma}^i - \beta^m \partial_m \tilde{\Gamma}^i - n B^i
\end{align}
$$

As far as I know, nobody has ever really found a compelling reason to use this form of the gamma driver gauge condition (plus, it is slower and requires more memory), so I'm really only mentioning it because it shows up in the literature. The single variable form of the gamma driver condition is the integrated form of the two-variable expression

## Implementation considerations

The self advection terms for both lapse and shift are optional and often are disabled, ie $\beta^m \partial_m \alpha$ and $\beta^j \partial_j \beta^i$

### Gauge condition summary

In the test we will be looking at today, we'll be using the harmonic gauge condition, with zero shift. The binary black hole gauge (often called the moving puncture gauge) is 1+log + Gamma driver, and will be used virtually exclusively in future articles

## The BSSN equations of motion are bad

So, as written, the BSSN equations of motion are - drumroll - very numerically unstable, and attempting to directly implement the PDEs listed above will result in nothing working. I'll attempt to briefly summarise two decades of the conventional wisdom from stability analysis here[^stabilityanalysis] on how to fix this:

[^stabilityanalysis]: This footnote could (and likely will) be an entire article in the future, where we'll test out dozens of modifications. Until then, consider checking out [this](https://arxiv.org/pdf/gr-qc/0204002), [this](https://arxiv.org/pdf/0707.0339), and [the ccz4](https://arxiv.org/pdf/1106.2254) paper (a similar, constraint damped formalism) for more information. There's a lot of decent analysis spread over the literature

1. If the second algebraic constraint $\mathcal{A}$ is violated, ie the trace of the trace-free extrinsic curvature != 0, the violation blows up and your simulation will immediately fail
2. Correcting momentum constraint $\mathcal{M}_i$ violations is very important
3. The christoffel symbol constraint $\mathcal{G}_i$ is quite important to damp
4. The quantity $\partial_k\tilde{\Gamma}^i_{mn}$ is numerically unstable, though the quantity $\tilde{\Gamma}^i_{mn}$ is not

Additionally:

1. Hamiltonian constraint violations are essentially unimportant for stability purposes, and seem to be largely irrelevant for physical accuracy beyond a reasonably justified paranoia
2. The first algebraic constraint $\mathcal{S}$ is relatively unimportant to enforce, but its easy to do - so we may as well

Because only the derivatives of $\partial_k \tilde{\Gamma}^{i}_{mn}$ are numerically unstable, expressions involving undifferentiated $\tilde{\Gamma}^i$'s are frequently replaced in the literature with the analytic equivalent. This is one technique to enforce the $\mathcal{G}^i$ constraint, though it is somewhat 'old fashioned'[^interestingy], and I will be avoiding it other than for testing in the future

[^interestingy]: I don't mean this in a derogatory way, and I find the stability analysis here absolutely fascinating. This substitution appears to somewhat restrict what kinds of spacetimes you are able to simulate, and is often quoted as only being good for octant symmetry specifically. My own experience of this is that while its good for the $\mathcal{G}$ constraint, it is also more numerically unstable than alternatives

Its time to deal with constraint stability directly now

### The algebraic constraints

There are three methods of enforcing the algebraic constraints

1. Directly
2. Damping
3. Solving

#### Directly

This is by far the most common method of handling the algebraic constraints. To do this, you enforce the relation $\tilde{\gamma} = 1$ and $\tilde{\gamma}^{mn} \tilde{A}_{mn} = 0$ directly, by applying the following expressions

$$
\begin{align}
\tilde{\gamma}_{ij} &= \frac{\tilde{\gamma}_{ij}}{\tilde{\gamma}^\frac{1}{3}} \\
\tilde{A}_{ij} &= \tilde{A}_{ij}^{TF}
\end{align}
$$

This is what will be used in the article. It is important to apply the algebraic constraints *after* numerical dissipation, otherwise numerical dissipation will cause the constraints to be violated again

#### Damping

This method has been floating around in the literature for a while, and damps the algebraic constraints. [This](https://arxiv.org/pdf/1106.2254) paper contains a discussion of the idea (before 26), and we'll be using the specific form provided by [this paper](https://arxiv.org/pdf/gr-qc/0204002) in the appendix. This means that we modify our evolution equations as follows

$$\begin{align}
\partial_t \tilde{\gamma}_{ij} &= \partial_t \tilde{\gamma}_{ij} + k_1\alpha \tilde{\gamma}_{ij} \mathcal{S}\\
\partial_t \tilde{A}_{ij} &= \partial_t \tilde{A}_{ij} + k_2\alpha \tilde{\gamma}_{ij} \mathcal{A}
\end{align}
$$

Where k1 < 0, k2 < 0, and the constraints are evaluated from the relevant constraint expressions

Note that because this only damps the $\mathcal{A}$ constraint instead of elimiating it, we must also modify the evolution equation for $\tilde{\gamma}_{ij}$ as follows, to prevent things from blowing up:

$$
\partial_t \tilde{\gamma}_{ij} = \beta^m \partial_m \tilde{\gamma}_{ij}+ 2 \tilde{\gamma}_{m(i} \partial_{j)} \beta^m - \frac{2}{3} \tilde{\gamma}_{ij} \partial_m \beta^m - 2 \alpha \tilde{A}^{TF}_{ij} \\
$$

This formulation works well, but you must be careful about your choice of damping factor. Too low and constraint errors pile up, too high and you overcorrect - oscillating wildly. Still, because its integrated into your evolution equations (rather than a separate step), its a fair bit faster than enforcing the constraint directly

#### Solving

I'll mention this in passing: The constraints can also be seen to reduce the number of free components for $\tilde{\gamma}_{ij}$ and $\tilde{A}_{ij}$ from 6 to 5, and instead of enforcing or damping the constraints, we can use them to solve for one of the components instead. For more discussion on this approach, please see [this paper](https://arxiv.org/pdf/1205.5111) (36-37)

While it may seem attractive to reduce the number of evolution equations, on a GPU you will be limited by memory bandwidth, and you're still going to be accessing all of the evolution variables anyway. This only actually saves us two memory writes, and significantly complicates the calculation of derivatives - leading to register/code size blowup which can cause performance problems

### The momentum constraint

The most direct and common way of enforcing this is as follows:

$$
\partial_t \tilde{A}_{ij}  = \partial_t \tilde{A}_{ij} + k_3 \alpha \tilde{D}_{(i} \mathcal{M}_{j)}
$$

Where $k_3$ > 0

### Christoffel symbol constraint

The most straightforward way to apply this is by applying the $\mathcal{G}^i$ constraint as followsw:

$$\partial_t \tilde{\Gamma}^i = \partial_t \tilde{\Gamma}^i - \sigma \mathcal{G}^i \partial_m \beta^m
$$

Where $\sigma$ > 0. This modification is not used in this paper

### Other constraints

Damping the hamiltonian constraint is unnecessary here, but [this](https://arxiv.org/pdf/gr-qc/0204002) paper contains some ideas that work

### Dissipating Numerical Noise

Even with all the constraint damping, the equations can still be numerically unstable - the last tool in our toolbox is kreiss-oliger dissipation

todo:

## Picking an integrator

We're going to diverge from the literature significantly here, for very good reason. The BSSN equations are a classically [stiff](https://en.wikipedia.org/wiki/Stiff_equation) (unstable) set of equations, and they oscillate pretty wildly. Its traditional to use something similar to RK4 to integrate the equations of motion

If you're familiar with solving stiff PDEs, it might seem a bit odd to use RK4 - RK4 is an explicit integrator - and as such has fairly poor stability properties when solving stiff equations. For many kinds of problem, an explicit integrator works great, but not really for unstable problems like this. Instead, we're going to use an L-stable implicit integrator which has much more appropriate convergence properties - backwards euler

Backwards euler has a very GPU-friendly form, making it very amenable to GPU architecture, while using a minimal amount of memory. In all my testing, I haven't found an integrator with better properties than this. Despite the 1st order convergence, it tends to provide better results than RK4 in my experience, but in the future we'll get into the data

As far as I'm aware, there's been no testing of fully implicit integration in numerical relativity - the closest is IMEX stepping from the SXS group, but it does not implicitly solve derivatives due to performance constraints

### Solving implicit equations efficiently

Forwards euler has the form:

$$y_{n+1} = y_n + dt f(t_n, y_n)$$

This is the classic 1st order, explicit integrator. It has poor stability properties, and does not have a high order of convergence

Backwards euler has the form:

$$y_{n+1} = y_n + dt f(t_{n+1}, y_{n+1})$$

This is also a 1st order integrator, but it is implicit - we can see that we have $y_{n+1}$ on both sides of the equation.

### Fixed point iteration

The issue with an implicit integrator is: How do we actually solve this equation? The straightforward answer is to use fixed point iteration

Todo: Fixed point

This also maps very well to gpu architecture, is numerically stable, and in general is extremely fast and robust. There are tricks to speed up fixed point iteration as well, which we'll get into in the future

## Flow of a simulation

There are three components to any good simulation here

1. Initial conditions
2. The boundary condition (which is tied to how we perform differentiation)
3. Integrating our PDEs

### Initial conditions

We need to define on our initial slice what our variables are. For this article, to check everything's working we're going to implement a gauge wave test case, which we can find [here](https://arxiv.org/pdf/1106.2254) (28-29), or [here](https://arxiv.org/pdf/0709.3559) (A.10 + A.6). The metric for this is as follows

$$ds^2 = (1 - H) (-dt^2 + dx^2) + dy^2 + dz^2$$

where

$$H = A \sin(\frac{2 \pi (x - t)}{d})$$

$d$ is the wavelength of our wave, which is set to 1 (our simulation width), and $A$ is the amplitude of the wave which is set to $0.1$. On our initial slice, $t=0$. This defines a 4-metric $g_{\mu\nu}$. While these papers do provide analytic expressions for our initial 3-variables - where's the joy in that?

#### Generic initial conditions

Given a metric $g_{\mu\nu}$, we want to calculate the ADM variables $\gamma_{ij}$, $K_{ij}$[^kijnotes], $\alpha$, and $\beta^i$. From there, we know how to take a conformal decomposition for the BSSN formalism

[^kijnotes]: Where I found $K_{ij}$ from is [here](https://clas.ucdenver.edu/math-clinic/sites/default/files/attached-files/master_project_mach_.pdf) 4-19a, although its a bit of a random source

$$\begin{align}
\gamma_{ij} &= g_{ij}\\
\beta_i &= g_{0i} \\
K_{ij} &= \frac{1}{2 \alpha} (D_i \beta_j + D_j \beta_i - \frac{\partial g_{ij}}{\partial_t})\\
\alpha &= \sqrt{-g_{00} + \beta^m \beta_m}
\end{align}$$

All latin indices run from 1-3, and all greek indices run from 0-3. Note that we find $\beta_i$ not $\beta^i$, which you can find by raising with $\gamma^{ij}$. To initialise our christoffel symbol $\tilde{\Gamma}^i$, simply calculate it analytically once we have $\tilde{\gamma}_{ij}$

You will need to be able to differentiate your metric tensor - either numerically, or analytically, for this to work

### Boundary conditions / Differentiation

Here, we're going to use fourth order[^orderconvergence] finite differentiation on a grid. If you remember, to approximate a derivative, mathematically we can do

[^orderconvergence]: In general, the order of convergence dictates the rate at which our algorithm improves the answer, when eg increasing our grid resolution

$$\frac{\partial f} {\partial x} = \frac{f(x+1) - f(x)}{h}$$

The factor of $h$ is omitted, because we're indexing grid cells. Numerically this is terrible, and instead we'll use fourth order differentiation, with coefficients which we can look up [here](https://en.wikipedia.org/wiki/Finite_difference_coefficient):

$$\frac{\partial f} {\partial x} = \frac{\frac{1}{12} f(x - 2) - \frac{2}{3} f(x - 1) + \frac{2}{3} f(x + 1) - \frac{1}{12} f(x + 2)}{h}$$

This tends to produce a decent tradeoff of accuracy vs performance. In this simulation, because we're discretising our data onto a grid, this brings up the question of how we take derivatives at the edge of the grid (as taking derivatives involves adjacent grid cells) - solving this question defines the boundary conditions

In this article we're going to be using periodic boundary conditions. To implement this, when implementing derivatives, you simply wrap your coordinates to the other edge of the domain

#### Implementation Considerations

##### Modulo operator

Do be aware that I discovered a pretty sizeable performance bug/compiler limitation when implementing this, where modulo operations are not optimised (even by a constant power of 2!) - which ate nearly all the runtime, so use branches instead

##### Second derivatives

When calculating second derivatives, while you could directly apply a 2d stencil, it is much more efficient to first calculate the first derivatives, store them somewhere, and then calculate the second derivatives from those stored derivatives (by taking the first derivative[^ivesaid] of them). This means that you should only precalculate 1st derivatives for variables for which we take second derivatives, which are the following:

[^ivesaid]: I've said the word derivative too many times. I'm going to go pet my cat

$$\tilde{\gamma}_{ij}, \alpha, \beta^i, W$$

##### Second derivative performance tricks

Second derivatives commute. This means that $\partial_i \partial_j == \partial_j \partial_i$. This is also true for the second covariant derivative of a scalar - though only in that circumstance for covariant derivatives. This brings up a good opportunity for performance improvements - when calculating partial derivatives, consider reordering them so that you lopsidedly differentiate on one variable - this can bring sizeable performance improvements

That is to say, consider implementing: $\partial_{min(i, j)} \partial_{max(i, j)}$ as a general second order differentiation system. This is good for cache (repeatedly accessing the same memory), increasing the amount of eliminateable duplicate code, and ensuring that we maximise the amount we differentiate in the $_0$th direction, which is the fastest due to memory layout

Eg, if we precalculate the first derivative $\partial_i \alpha$, calculating $\partial_0 \partial_i \alpha$ will always be faster than calculating $\partial_i \partial_0 \alpha$. Partly because of memory layout (differentiating in the x direction is virtually free, as its all in cache), and partly because if we *always* do this, we improve our cache hit rate. This is worth about 20% of our runtime

### Integrating the PDEs

Each timestep is broken up into a fixed number of substeps, as we solve the implicit equation for backwards euler via fixed point iteration. In each substep, we:

1. Enforce the algebraic constraints
2. Calculate the derivatives (todo: Think i need to segment this off more)
3. Calculate the momentum constraint
4. Evolve our equations to calculate yn + f(yn+1), the result of the substep

After we've performed a fixed number of iterations, I apply kreiss-oliger dissipation to remove numerical noise

## Implementation

We now have all of the theory out of the way. Lets look at the code for our initial conditions first:

### Initial conditions

dfdfdf

### Algebraic constraint enforcement

### Derivatives + Boundary conditions

The boundary conditions are implemented whenever we need to take a derivative of something, which looks like this.

### Evolution

This is the real kicker

#### Calculating derivatives

We only need to calculate 1st derivatives and save them when we take the second derivative of something

#### Evolution equations by Field

##### $\tilde{\gamma}_{ij}$

##### $\tilde{A}_{ij}$

##### $K$

##### $W$

##### $\tilde{\Gamma}^i$

##### $\alpha$

##### $\beta^i$

Note that structurally for backwards euler we need a buffer for our input (yn+1), one for our output (yn + f(yn+1)), and one to hold the last value (yn)

### Kreiss-Oliger

Finally afterwards we numerically dissipate away the noise via kreiss-oliger

## Results





























