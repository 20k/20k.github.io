---
layout: post
title:  "Numerical Relativity 101: How to simulate spacetime itself"
date:   2024-07-21 18:33:23 +0000
categories: C++
---

Papers:

https://arxiv.org/pdf/1307.7391 ccz4 paper, contains some good equation references
https://iopscience.iop.org/article/10.1088/1361-6382/ac7e16/pdf covariant bssn
https://www2.yukawa.kyoto-u.ac.jp/~masaru.shibata/PRD52.5428.pdf cG mentions the motivation behind the gamma term
stability paper https://arxiv.org/pdf/0707.0339
http://wwwteor.mi.infn.it/~molinari/TESI/Danieli_tesi.pdf 4.27 relates conjugate momentum and extrinsic curvature
https://arxiv.org/pdf/2210.10103 adm stuff
https://arxiv.org/pdf/gr-qc/0604035 adm evolution equations

Ok planola time:

1. Basic introduction to the ADM formalism. Need to cover projection, the concept of time + gauge conditions, physical interpretation of the gauge conditions as an eularian observer, and the concept of a hypersurface
2. Introduce the adm equations
3. Introduce the vanilla BSSN equations
4. Talk through all the notation
5. Swap to structuring a simulation
6. Talk through the high level flow
7. Initial conditions: waves
8. Boundary conditions: periodic
9. Evolution equations: Bssn, momentum constraint damping, algebraic constraints, Kreiss-Oliger dissipation
10. Gauge condition evolution
11. Some kind of diagnostic (semi analytic tests?)

hmm, i'm not convinced that mixing code in is a good idea. I am tired but really what we want is to maintain a relatively good flow, atm dumping code in is distracting the overarching goals

There are quite a few tangents in here: sections which contain sidebars about eg the conformal variables: do i stick this in discussion segments at the end?

Hi there! Today we're going to be getting into numerical relativity, which is the art of simulating the evolution of spacetime itself. While there a quite a few known solutions to the equations of general relativity at this point (eg the Schwarzschild metric), many of the most interesting regions of general relativity can only be explored by exploring how spacetime itself evolves dynamically through simulations. Binary black hole collisions are a perfect example of this, and are a big part of the reason why we're building this

<iframe width="560" height="315" src="https://www.youtube.com/embed/CSpo2S9IVjY?si=3N_d-7cRyzeqi1fS" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

These kinds of simulations are traditionally run on supercomputers, but luckily you likely have a supercomputer in the form of a GPU[^specs] in your PC. The amount of extra performance we can get over a traditional CPU based implementation is likely to be a few orders of magnitude higher if you know what you're doing, so in this article I'm going to present to you how to get this done

[^specs]: You'll need 8+ GB of vram for future bbh collisions, though you can get away with minimal specs for this article. This was written and tested on a 6700xt, but previous work was on an r9 390. Any OpenCL 1.2 compatible device will work here, though I do use 64-bit atomics for accumulating diagnostics

This article is not going to assume you know all that much about general relativity, but it does require a lot of implementing

# Preamble

In this article we'll be using the Einstein summation convention, and I'm going to assume you know how to raise and lower indices. See [this](https://20k.github.io/c++/2024/05/31/schwarzschild.html) article for more details, but the short version is:

1. Repeated indices are summed, eg $v^\mu v_\mu == v^0 v_0 + v^1 v_1 + v^2 v_2 + v^3 v_3$
2. Indices can be raised and lowered by the metric tensor, eg $v^\mu = g^{\mu\nu} v_\nu$, and $v_\mu = g_{\mu\nu} v^\nu$
3. The inverse of the metric $g_{\mu\nu}$ - that is $g^{\mu\nu}$ - is calculated via a matrix inverse, ie $g^{\mu\nu} = (g_{\mu\nu})^{-1}$

# The ADM (Arnowitt–Deser–Misner) formalism

The [Einstein field equations](https://en.wikipedia.org/wiki/Einstein_field_equations#Mathematical_form)[^fieldequations] (EFE's) are what defines the structure of spacetime. In general, people have come up with many analytic solutions to these (with a great deal of effort), including Schwarzschild, Kerr, and a variety of others

![schwarzs](/assets/schwarzs.png)

As-is, the Einstein field equations aren't especially useful for *simulating* spacetime, as they are declarative, and simply describe how a full 4d spacetime should look. In the past, we've taken a known, fixed spacetime like Schwarzschild, and fired rays around it to find out what it looks like - but what if we don't already know the future and past of our spacetime - because we aren't time travelling wizards?

[^fieldequations]: When I was younger, I thought that these were equations that you deployed in an emergency. You know, *in the field*

The ADM formalism is the most widespread approach for handling this. It consists of slicing a 4d spacetime explicitly into a series (foliation) of 3d spacelike slices, and separates out a timelike dimension. It is often referred to as a "3+1" formalism as a result. You can imagine this like a series of stacked sheets that you can traverse

![foliation](/assets/pathy.png)

The direction of forwards in time in the ADM formalism is defined by two variables - called gauge variables. These are $\alpha$ (the lapse), and $\beta^i$[^lapseshift] (a 3-vector, called the shift). These represent a degree of freedom, and are arbitrary (in theory)

Each 3d[^nd] slice has its own metric tensor associated with it called $\gamma_{ij}$ - which works as you'd expect[^expecto] - and is derived from our 4d metric $g_{\mu\nu}$. We also have a variable $K_{ij}$ known as the extrinsic curvature, which represents the embedding of the slice into the larger 4d spacetime. It is directly related to the evolution of $\gamma_{ij}$ in time, and can be thought of as a momentum term

[^expecto]: If you're coming into this with no knowledge of GR, a metric tensor is used to measure distances, and angles, and defines the warping of a manifold. $\gamma_{ij}$ is used on this 3d space, whereas a 4-metric called $g_{\mu\nu}$ is used on a 4d spacetime

[^nd]: Everything here is also applicable to N dimensional spacetimes, but I know very little about anything other than 4d general relativity

[^lapseshift]: These are also often frequently labelled $N$, for the lapse, and $N^i$, for the shift

## Details

From now on, all latin indices $$_{ijk}$$ have the value 1-3, and generally denote quantities on our 3d slice, whereas greek indices $$_{\mu\nu\kappa}$$ take on the value 0-3 and denote quantities in our 4d spacetime

The general line element in ADM is this:

$$ds^2 = -\alpha^2 dt^2 + \gamma_{ij} (dx^i + \beta^i dt)(dx^j + \beta^jdt)$$

$\gamma_{ij}$ can be used to raise and lower indices on a 3d slice as per usual. Eg, given a tensor $Q_i$

$$
\begin{align}
Q^i &= \gamma^{ij} Q_j \\
Q_i &= \gamma_{ij} Q^j
\end{align}
$$

If our 4-metric is $g_{\mu\nu}$, then our 3-metric is $\gamma_{ij} = g_{ij}$, ie the spatial parts. While $\gamma^{ij} = (\gamma_{ij})^{-1}$ (the 3x3 matrix inverse), $\gamma^{ij} \neq g^{ij}$

## Equations of motion

The ADM formalism can give you equations of motion, by defining a Lagrangian, doing a bunch of maths on it, and then hoping you don't go totally mad. Once that's all done, the ADM equations of motion can be brought into the following form[^reffyadm]:

[^reffyadm]: [https://arxiv.org/pdf/gr-qc/0604035](https://arxiv.org/pdf/gr-qc/0604035) 5

$$\begin{align}
\partial_t \gamma_{ij} &= \mathcal{L}_\beta \gamma_{ij} - 2 \alpha K_{ij}\\
\partial_t K_{ij} &= \mathcal{L}_\beta K_{ij} - D_i D_j \alpha + \alpha (R_{ij} - 2 K_{il} K^l_j + K K_{ij})
\end{align}
$$

Where $\mathcal{L}_\beta$ is a lie derivative, and $D_i$ is the covariant derivative associated with $$\gamma_{ij}$$. We must also provide adequate gauge conditions (evolution equations for the gauge variables), and then in theory we have a fully working formalism for evolving a slice of spacetime into the future

Unfortunately, there's a small problem

## The ADM equations of motion are bad

Part of the reason why we're not going into this more is that - unfortunately - these equations of motion are unusable to us. They are *very* numerically unstable, to the point where for our purposes there's no point trying to solve them. Understanding this issue, and solving it was an outstanding problem for decades, and there is an absolutely enormous amount written in the literature on this topic. A lot of this article series is going to be dealing with the very poor character of the equations

Luckily, because its not the early 2000s - and because the astrophysics community rocks and makes all this information public - we get to skip to the part where we have a workable set of equations

# Enter: The BSSN formalism

The BSSN formalism is the most widely used formalism - it is directly derived from the ADM formalism, and most practical formalisms are substantially similar to the BSSN formalism. It was the first very successful formalism to enter the picture - though it was unclear precisely why it worked so well for a long time[^seehere]

[^seehere]: [https://arxiv.org/pdf/0707.0339](https://arxiv.org/pdf/0707.0339) this paper contains a lot of useful details, but the short version is that the BSSN equations are a partially constrained evolution system, and managing the constraints is critical to a correct evolution scheme

There are 4 main ingredients to get the BSSN equations from the ADM equations:

1. You perform what's called a conformal transform - the metric $\gamma_{ij}$ is split into a metric with unit determinant $$\tilde{\gamma}_{ij}$$, and a conformal (scalar) factor (more on this soon)
2. The extrinsic curvature $K_{ij}$ is split up into its trace: $K$, and the conformally transformed trace free part of the extrinsic curvature, $$\tilde{A}_{ij}$$
3. A new redundant variable $$\tilde{\Gamma}^i = \tilde{\gamma}^{mn} \tilde{\Gamma}^{i}_{mn}$$[^conformalchristoffel] is introduced and evolved dynamically, which eliminates a major source of numerical instabilities
4. The hamiltonian constraint is substituted in to eliminate numerically unstable quantities

[^conformalchristoffel]: $\tilde{\Gamma}^i_{jk}$ are the conformal christoffel symbols of the second kind, calculated in the usual fashion from $\tilde{\gamma_{ij}}$. $\Gamma^i_{jk}$ are the christoffel symbols associated with $\gamma_{ij}$

With all this applied, we end up with the BSSN formalism. We'll go through some of the relevant elements of this now

## Conformal decomposition

The main two ADM variables are: $\gamma_{ij}$ the metric tensor, and $K_{ij}$, the extrinsic curvature

[^reffy]: [Reference](http://wwwteor.mi.infn.it/~molinari/TESI/Danieli_tesi.pdf) 4.27

As mentioned previously, one of the insights of the BSSN formalism is that we can perform a 'conformal decomposition' of the variables $\gamma_{ij}$ and $K_{ij}$, which renders the equations more numerically stable. The conformal metric tensor is called $\tilde{\gamma}_{ij}$, and is related to the regular metric tensor by a scalar, called the conformal factor/variable. There are three conformal variables I know of which are in use:

$$
\begin{align}
\tilde{\gamma}_{ij} &= e^{-4\phi} \gamma_{ij}\\
\tilde{\gamma}_{ij} &= \chi \gamma_{ij}\\
\tilde{\gamma}_{ij} &= W^2 \gamma_{ij}\\
\end{align}
$$

All defining the same conformal metric. The definition of the conformal variables in these schemes is as follows:

$$
\begin{align}
\phi &= TODO\\
\chi &= det(\gamma)^{-1/3}\\
W &= det(\gamma)^{-1/6}\\
\end{align}
$$

With this definition, the determinant of our conformal metric tensor is $\tilde{\gamma} = 1$. The inverse of the metric tensor follows a similar relation:

$$
\begin{align}
e^{-4\phi} &\tilde{\gamma}^{ij} &= \gamma^{ij}\\
\chi &\tilde{\gamma}^{ij} &= \gamma^{ij}\\
W^2 &\tilde{\gamma}^{ij} &= \gamma^{ij}\\
\end{align}
$$

$\phi$: This was the first to be used, but it tends to infinity at a black hole's singularity which makes it not ideal - and appears to be falling out of use.

$\chi$: This version tends to 0 at the singularity and in general behaves much better than the $\phi$ version - and is relatively widespread in more modern code.

$W^2$: Ostensibly it has better convergence[^converge] than $\chi$, but the really neat part about it is that renders the equations of motion of BSSN in a much less singular form. It also does not suffer from some of the issues that $\chi$ has, like unphysical negative values

[^converge]: See [High-spin binary black hole mergers](https://arxiv.org/pdf/0709.2160) for this line of reasoning

Because of this, the $W^2$ formalism is clearly the best choice for us. Do note that some papers interchange notation here unfortunately, and eg $\phi$ and $\chi$ are sometimes used to mean *any* conformal factor

## Variables

The full set of BSSN variables after conformally decomposing $\gamma_{ij}$ and $K_{ij}$ is as follows:

$$
\begin{align}
&\tilde{\gamma}_{ij} &&= W^2 \gamma_{ij}\\
&\tilde{A}_{ij} &&= W^2 (K_{ij} - \frac{1}{3} \gamma_{ij} K)\\
&K &&= \gamma^{mn} K_{mn}\\
&W &&= det(\gamma)^{-1/6}\\
&\tilde{\Gamma}^i &&= \tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}\\
&\alpha_{bssn} &&= \alpha_{adm}\\
&\beta^i_{bssn} &&= \beta^i_{adm}\\
\end{align}
$$

Conformal variables have a tilde over them, and have their indices are raised and lowered with the conformal metric tensor. In this definition, the determinant of the conformal metric is $$\tilde{\gamma} = 1$$, and $$\tilde{A}_{ij}$$ is trace-free. $K$ is the trace of $K_{ij}$. $K_{ij}$, $\gamma_{ij}$, $$\tilde{A}_{ij}$$, and $$\tilde{\gamma}_{ij}$$ are all symmetric

$$\tilde{\Gamma}^i$$ is analytically equal to $$\tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}$$, but is evolved numerically instead. This is because $$\partial_k \tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}$$ is numerically unstable TODO: CITE

There are also a number of constraints that must be satisfied for the BSSN formalism to be well posed. The momentum and hamiltonian constraints come from this being derived from a Lagrangian approach, and the others are specific to the BSSN formalism

## Constraints

### Momentum

$$\mathcal{M}_i = \tilde{\gamma}^{mn} \tilde{D}_m \tilde{A}_{ni} - \frac{2}{3} \partial_i K - \frac{3}{2} \tilde{A}^m_{\;\;i} \frac{\partial_m \chi}{\chi} = 0$$

Where $$\tilde{D}_i$$ is the covariant derivative associated with $$\tilde{\gamma}_{ij}$$, calculated in the usual fashion

### Hamiltonian

$$H = R + \frac{2}{3} K^2 - \tilde{A}^{mn} \tilde{A}_{mn} = 0$$

### Algebraic

$$\begin{align}
\mathcal{S} &= \tilde{\gamma} - 1 &= 0\\
\mathcal{A} &= \tilde{\gamma}^{mn} \tilde{A}_{mn} &= 0
\end{align}
$$

Where $$\tilde{\gamma}$$ is the determinant of the metric

### Christoffel symbol

$$
\mathcal{G}^i = \tilde{\Gamma}^i - \tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn} = 0
$$

Ensuring that the constraints don't blow up numerically is key to making the BSSN formalism work, so we'll get back to these

## Equations of motion

I'm going to present to you the canonical form of the BSSN equations, and then discuss how you might want to rearrange them a bit in practice afterwards

$$
\begin{align}
\partial_t W &= \beta^m \partial_m W + \frac{1}{3} W (\alpha K - \partial_m \beta^m) \\
\partial_t \tilde{\gamma}_{ij} &= \beta^m \partial_m \tilde{\gamma}_{ij}+ 2 \tilde{\gamma}_{m(i} \partial_{j)} \beta^m - \frac{2}{3} \tilde{\gamma}_{ij} \partial_m \beta^m - 2 \alpha \tilde{A}_{ij} \\
\partial_t K &= \beta^m \partial_m K - W^2 \tilde{\gamma}^{mn} D_m D_n \alpha + \alpha \tilde{A}^{mn} \tilde{A}_{mn} + \frac{1}{3} \alpha K^2\\
\partial_t \tilde{A}_{ij} &= \beta^m \partial_m \tilde{A}_{ij}+ 2 \tilde{A}_{m(i} \partial_{j)} \beta^m - \frac{2}{3} \tilde{A}_{ij} \partial_m \beta^m + \alpha K \tilde{A}_{ij} \\&- 2 \alpha \tilde{A}_{im} \tilde{A}^m_{\;\;j} + W^2 (\alpha \mathcal{R}_{ij} - D_i D_j \alpha)^{TF} \\
\partial_t \tilde{\Gamma}^i &= \beta^m \partial_m \tilde{\Gamma}^i + \frac{2}{3} \tilde{\Gamma}^i \partial_m \beta^m - \tilde{\Gamma}^m \partial_m \beta^i + \tilde{\gamma}^{mn} \partial_m \partial_n \beta^i + \frac{1}{3} \tilde{\gamma}^{im} \partial_m \partial_n \beta^n \\
&-\tilde{A}^{im}(6\alpha \frac{\partial_m W}{W} + 2 \partial_m \alpha) + 2 \alpha \tilde{\Gamma}^i_{mn} - \frac{4}{3} \alpha \tilde{\gamma}^{im} \partial_m K
\end{align}
$$

With the secondary expressions:

$$
\begin{align}
\mathcal{R}_{ij} &= \tilde{\mathcal{R}}_{ij} + \mathcal{R}^W_{ij}\\
\mathcal{R}^W_{ij} &= \frac{1}{W^2} (W (\tilde{D}_i \tilde{D}_j W + \tilde{\gamma}_{ij} \tilde{D}^m \tilde{D}_m W) - 2 \tilde{\gamma}_{ij} \partial^m W \partial_m W)\\
\tilde{\mathcal{R}}_{ij} &= -\frac{1}{2} \tilde{\gamma}^{mn} \partial_m \partial_n \tilde{\gamma}_{ij} + \tilde{\gamma}_{m(i} \partial _{j)} \tilde{\Gamma}^m + \tilde{\Gamma}^m \tilde{\Gamma}_{(ij)m}+ \tilde{\gamma}^{mn} (2 \tilde{\Gamma}^k_{m(i} \tilde{\Gamma}_{j)kn} + \tilde{\Gamma}^k_{im} \tilde{\Gamma}_{kjn})\\
D_iD_j\alpha &= \tilde{D}_i \tilde{D}_j \alpha + \frac{2}{W} \partial_{(i} W \partial_{j)} \alpha\ - \frac{1}{W} \tilde{\gamma}_{ij} \tilde{\gamma}^{mn} \partial_m W \partial_n \alpha
\end{align}
$$

Additionally:

1. $$^{TF}$$ means to take the trace free part of a tensor, calculated as $$Q_{ij}^{TF} = Q_{ij} - \frac{1}{3} \tilde{\gamma}_{ij} \tilde{\gamma}^{mn} Q_{mn}$$.[^tfnotes]

2. The symmetrising operator crops up here unintuitively. This is defined as $A_{(ij)}$ = $0.5 (A_{ij} + A_{ji})$. A more complex example is $$2\tilde{\Gamma}^k_{m(i} \tilde{\Gamma}_{j)kn} = 0.5 * 2 (\tilde{\Gamma}^k_{mi} \tilde{\Gamma}_{jkn} + \tilde{\Gamma}^k_{mj} \tilde{\Gamma}_{ikn})$$

3. Raised indices on covariant derivatives mean this: $$\tilde{D}^i = \tilde{\gamma}^{ij} \tilde{D}_j$$

[^tfnotes]: Note that for some scalar $A$, $(AQ)^{TF} == A(Q^{TF})$, and it does not matter if you use the conformal metric, or the non conformal metric as the metric tensor here - the conformal factor cancels out. This is also sometimes notated $Q_{\langle ij \rangle}$

### Implementation considerations

1. The quantity $D_i D_j \alpha$ is never used without being multiplied by $W^2$, which means we can eliminate the divisions by $W$

2. The quantity $\mathcal{R}^W_{ij}$ is also multiplied by $W^2$, allowing us to remove the division

#2 is unique to the $W^2$ formalism, and is a very nice feature of it. In this version of BSSN, there is only one division by $W$, in the $\tilde{\Gamma}^i$ evolution equation, and you must guard against divisions by zero by clamping $W$ to a small value (eg $10^{-4}$)[^technically]

[^technically]: Technically, as $\alpha$ is expected to tend to 0 faster than $W$ tends to 0, the quantity $\frac{\alpha}{W}$ is considered regular. In practice, clamping the conformal factor appears to be the accepted technique, though alarmingly I've found that the exact clamp value can make a large difference to your simulation

## Gauge conditions

The ADM/BSSN formalism does not uniquely specify the evolution equations for the lapse and shift, and there is a freedom of choice here. Despite in theory being arbitrary, in practice you have to very carefully pick your gauge conditions for the kind of problem you're trying to solve

Here are some common gauge conditions:

### General

#### Geodesic slicing

$$\alpha = 1, \beta^i = 0$$

### Lapse ($\alpha$)

#### Harmonic slicing

Ref[^harmy]

$$\partial_t \alpha = - 2 \alpha^2 K + \beta^m \partial_m \alpha$$

[^harmy]: [Ref](https://arxiv.org/pdf/2201.08857)

#### 1+log slicing

$$\partial_t \alpha = - \alpha K + \beta^m \partial_m \alpha$$

### Shift ($\beta^i$)

#### Zero shift

$$\beta^i = 0$$

#### Gamma driver (sometimes called Gamma freezing)

So called because it drives $\partial_t\tilde{\Gamma} = 0$

$$\partial_t \beta^i = \frac{3}{4} \tilde{\Gamma}^i + \beta^j \partial_j \beta^i - n \beta^i$$

$n$ is a damping parameter, that is generally set to $2$, or $2M$ where $M$ is the mass of whatever you have in your spacetime

The alternative form of this gauge condition is:

$$
\begin{align}
\partial_t \beta^i &= \beta^m \partial_m \beta^i + \frac{3}{4} B^i\\
\partial_t B^i &= \beta^m \partial_m B^i + \partial_t \tilde{\Gamma}^i - \beta^m \partial_m \tilde{\Gamma}^i - n B^i
\end{align}
$$

As far as I know, nobody has ever really found a compelling reason to use this form of the gamma driver gauge condition (plus, it is slower and requires more memory), so I'm really only mentioning it because it shows up in the literature. The single variable form of the gamma driver condition is the integrated form of the two-variable expression

### Implementation considerations

The self advection terms for both lapse and shift are optional and often are disabled, ie $\beta^m \partial_m \alpha$ and $\beta^j \partial_j \beta^i$

### Gauge condition summary

In the test we will be looking at today, we'll be using the harmonic gauge condition, with zero shift. The binary black hole gauge (often called the moving puncture gauge) is 1+log + Gamma driver, and will be used virtually exclusively in future articles

## The BSSN equations of motion are bad

So, as written, the BSSN equations of motion are - drumroll - very numerically unstable, and attempting to directly implement the PDEs listed above will result in nothing working. I'll attempt to briefly summarise two decades of the conventional wisdom from stability analysis here[^stabilityanalysis] on how to fix this:

[^stabilityanalysis]: This footnote could (and likely will) be an entire article in the future, where we'll test out dozens of modifications. Until then, consider checking out [this](https://arxiv.org/pdf/gr-qc/0204002), [this](https://arxiv.org/pdf/0707.0339), and [the ccz4](https://arxiv.org/pdf/1106.2254) paper (a similar, constraint damped formalism) for more information. There's a lot of decent analysis spread over the literature

1. If the second algebraic constraint $\mathcal{A}$ is violated, ie the trace of the trace-free extrinsic curvature != 0, the violation blows up and your simulation will immediately fail
2. Correcting momentum constraint $\mathcal{M}_i$ violations is very important
3. The christoffel symbol constraint $\mathcal{G}^i$ is quite important to damp
4. The quantity $\partial_k\tilde{\Gamma}^i_{mn}$ is numerically unstable, though the quantity $\tilde{\Gamma}^i_{mn}$ is not

Additionally:

1. Hamiltonian constraint violations are essentially unimportant for stability purposes, and seem to be largely irrelevant for physical accuracy beyond a reasonably justified paranoia
2. The first algebraic constraint $\mathcal{S}$ is relatively unimportant to enforce, but its easy to do - so we may as well

Because only the derivatives of $\partial_k \tilde{\Gamma}^{i}_{mn}$ are numerically unstable, expressions involving undifferentiated $\tilde{\Gamma}^i$'s are frequently replaced in the literature with the analytic equivalent $$\tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}$$. This is one technique to enforce the $\mathcal{G}^i$ constraint, though we're going to avoid this approach because it is a bit limited

### Fixing the BSSN equations

Before we can even get off the ground, we're going to have to at minimum implement the following three things

1. Algebraic constraints enforcement
2. Momentum constraint damping
3. Numerical dissipation

#### The algebraic constraints

There are three methods of enforcing the algebraic constraints

1. Directly
2. Damping
3. Solving

##### Directly

This is by far the most common method of handling the algebraic constraints. To do this, you enforce the relation $\tilde{\gamma} = 1$ and $\tilde{\gamma}^{mn} \tilde{A}_{mn} = 0$ directly, by applying the following expressions

$$
\begin{align}
\tilde{\gamma}_{ij} &= \frac{\tilde{\gamma}_{ij}}{\tilde{\gamma}^\frac{1}{3}} \\
\tilde{A}_{ij} &= \tilde{A}_{ij}^{TF}
\end{align}
$$

This is what will be used in the article. It is important to apply the algebraic constraints just before our evolution step, otherwise numerical dissipation will cause the constraints to become violated again

##### Damping

This method involves damping the algebraic constraints over time. [This](https://arxiv.org/pdf/1106.2254) paper contains a discussion of the idea (before 26), and you can use the specific form provided by [this paper](https://arxiv.org/pdf/gr-qc/0204002) in the appendix. This means that the evolution equations are modified as follows:

$$\begin{align}
\partial_t \tilde{\gamma}_{ij} &= \partial_t \tilde{\gamma}_{ij} + k_1\alpha \tilde{\gamma}_{ij} \mathcal{S}\\
\partial_t \tilde{A}_{ij} &= \partial_t \tilde{A}_{ij} + k_2\alpha \tilde{\gamma}_{ij} \mathcal{A}
\end{align}
$$

Where $k_1$ < 0, $k_2$ < 0, and the constraints are evaluated from the relevant constraint expressions

Note that because this only damps the $\mathcal{A}$ constraint instead of eliminating it, you must also modify the evolution equation for $\tilde{\gamma}_{ij}$ as follows, to prevent everything from blowing up:

$$
\partial_t \tilde{\gamma}_{ij} = \beta^m \partial_m \tilde{\gamma}_{ij}+ 2 \tilde{\gamma}_{m(i} \partial_{j)} \beta^m - \frac{2}{3} \tilde{\gamma}_{ij} \partial_m \beta^m - 2 \alpha \tilde{A}^{TF}_{ij} \\
$$

This formulation works well, but you must be careful about your choice of damping factor. Too low and constraint errors pile up, too high and you overcorrect - oscillating wildly. Still, because its integrated into the evolution equations (rather than as a separate step), its a fair bit faster than enforcing the constraints directly

##### Solving

I'll mention this in passing: The constraints can also be seen to reduce the number of free components for $$\tilde{\gamma}_{ij}$$ and $$\tilde{A}_{ij}$$ from 6 to 5, and instead of enforcing or damping the constraints, you can use them to solve for one of the components instead. For more discussion on this approach, please see [this paper](https://arxiv.org/pdf/1205.5111) (36-37)

While it may seem attractive to reduce the number of evolution equations, on a GPU you will be limited by memory bandwidth, and you're still going to be accessing all of the evolution variables anyway. This only actually saves us two memory writes, and significantly complicates the calculation of derivatives - leading to register/code size blowup

#### The Momentum constraint

The most direct and common way of enforcing this is as follows:

$$
\partial_t \tilde{A}_{ij}  = \partial_t \tilde{A}_{ij} + k_3 \alpha \tilde{D}_{(i} \mathcal{M}_{j)}
$$

Where $k_3$ > 0

#### The Christoffel symbol constraint

The most straightforward way to apply this is by applying the $\mathcal{G}^i$ constraint as follows:

$$\partial_t \tilde{\Gamma}^i = \partial_t \tilde{\Gamma}^i - \sigma \mathcal{G}^i \partial_m \beta^m
$$

Where $\sigma$ > 0. This modification is not used in this article, partly as it is unnecessary, and partly because $\beta^i = 0$ here

#### Other constraints

Damping the hamiltonian constraint is unnecessary here, but [this](https://arxiv.org/pdf/gr-qc/0204002) paper contains some ideas that work

### Dissipating Numerical Noise

Even with all the constraint damping, the equations can still be numerically unstable - the last tool in the toolbox is called Kreiss-Oliger dissipation. This essentially is a blur on your data - it extracts a certain order of frequencies from your simulation grid, and damps them. The [einstein toolkit](https://einsteintoolkit.org/thornguide/CactusNumerical/Dissipation/documentation.html) website gives this definition[^original] - which I always found somewhat confusing personally:

[^original]: I do not have access to the linked paper sadly

$$\partial_t U = \partial_t U + (-1)^{(p+3)/2} \epsilon \frac{h^p}{2^{p+1}} (\frac{\partial^{p+1}}{\partial x^{p+1}} + \frac{\partial^{p+1}}{\partial y^{p+1}} + \frac{\partial^{p+1}}{\partial z^{p+1}})U$$

$\epsilon$ is a damping factor (eg $0.25$), and $h$ is the grid spacing. $p$ is the order of the dissipation, although there appears to be some confusion about what it means[^notclear]. 6th order derivatives are common to use, here I'm going to set $p=5$

[^notclear]: It is not clear to me when people refer to the order of kreiss-oliger dissipation if they mean the derivatives, or $p$. eg in [this](https://authors.library.caltech.edu/8284/1/RINcqg07.pdf) paper, B.4 is called 4th order kreiss-oliger, whereas the cactus toolkit contradicts that definition

$$\partial_t U = \partial_t U + \epsilon \frac{h^5}{64} (\frac{\partial^6}{\partial x^6} + \frac{\partial^6}{\partial y^6} + \frac{\partial^6}{\partial z^6})U$$

There are two major ways of implementing this:

1. Modifying the evolution equations as written above
2. As a pure numerical filter, ie as a frequency removal pass over your data

I pick #2, as it can be implemented more efficiently, and it makes a bit more sense to me

Note, 6th derivatives imply a division by $h^6$ - which is then immediately multiplied by $h^5$, and you may not want to do this numerically

## Picking an integrator

We're going to diverge from the literature significantly here, for very good reason. The BSSN equations are a classically [stiff](https://en.wikipedia.org/wiki/Stiff_equation) (unstable) set of equations, which means that integrators can have a hard time providing accurate results. Its traditional to use something similar to RK4 to integrate the equations of motion

If you're familiar with solving stiff PDEs, it might seem a bit odd to use RK4 - RK4 is an explicit integrator - and as such has fairly poor stability properties when solving stiff equations. For many kinds of problem, an explicit integrator works great, but not really for unstable problems like this. Instead, we're going to use an L-stable implicit integrator which has excellent convergence properties - backwards euler

Backwards euler has a very GPU-friendly form, making it very amenable to GPU architecture, while using a minimal amount of memory. In all my testing, I haven't found an integrator with better properties than this. Despite the 1st order convergence, it tends to provide better results than RK4 in my experience, but in the future we'll get into the data

As far as I'm aware, there's been minimal testing of fully implicit integration in numerical relativity - the closest is [IMEX stepping](https://arxiv.org/pdf/1105.3922) from the SXS group, but relies on splitting equations into explicit, and implicit segments. Here, we're just going to solve the entire set of equations implicitly because its easier. We're limited by memory bandwidth on the GPU, which means that there's less benefit to reducing computational crunch - if our implicit segment contains a full set of memory accesses for every variable, we may as well fully implicitly solve everything

### Solving implicit equations efficiently

Forwards euler has the form:

$$y_{n+1} = y_n + dt f(t_n, y_n)$$

Where $dt$ is a timestep, and $y_n$ is our current set of variables. This is the classic 1st order, explicit integrator. It has poor stability properties, and has first order convergence

Backwards euler has the form:

$$y_{n+1} = y_n + dt f(t_{n+1}, y_{n+1})$$

This is also a first order integrator, but it is implicit - we can see that we have $y_{n+1}$ on both sides of the equation - and it is L stable which is great for us

### Fixed point iteration

The issue with an implicit integrator is: How do we actually solve this equation? The straightforward answer is to use fixed point iteration. In essence, you feed your output to your input repeatedly. Ie, if f(x) gives you your next guess, you iterate the sequence $f(x)$, $f(f(x))$, $f(f(f(x)))$, until you reach your desired accuracy

For our purposes, we'll iterate the sequence:

$$y_{n+1, k+1} = y_n + dt f(y_{k,n+1})$$

Over our iteration variable $k$. To initialise the fixed point iteration, $y_{0, n+1} = y_{n}$. This makes the first step/guess regular euler

This form of fixed point iteration maps very well to being implemented on a GPU, is numerically stable, and in general is extremely fast and robust. There are tricks[^newton] to speed up fixed point iteration as well, which we'll get into in the future

[^newton]: Note that its common to use Newton-Raphson or a full root finding method here for solving implicit equations, and I've had nothing but poor results. The issue is that it is not a massively stable algorithm, and the complexity of evaluating a Newton-Raphson iteration outweighs the increase in convergence when dealing with a small number of iterations (eg 2-3 here). Fixed point iteration can be directly sped up by rearranging it as a relaxation problem, and then successively over relaxing

## Flow of a simulation

There are three components to any good simulation

1. Initial conditions
2. The boundary conditions (which is tied to how we perform differentiation)
3. Integrating our PDEs

### Initial conditions

We need to define on our initial slice what our variables are. For this article, to check everything's working we're going to implement a gauge wave test case, which you can find [here](https://arxiv.org/pdf/1106.2254) (28-29), or [here](https://arxiv.org/pdf/0709.3559) (A.10 + A.6, this is what we're actually implementing). The metric for this is as follows

$$ds^2 = (1 - H) (-dt^2 + dx^2) + dy^2 + dz^2$$

where

$$H = A \sin(\frac{2 \pi (x - t)}{d})$$


$d$ is the wavelength of our wave, which is set to 1 (our simulation width), and $A$ is the amplitude of the wave which is set to $0.1$. On our initial slice, $t=0$. This defines a 4-metric $g_{\mu\nu}$. While these papers do provide analytic expressions for our initial ADM variables - where's the joy in that?

#### Generic initial conditions

Given a metric $g_{\mu\nu}$, we want to calculate the ADM variables $\gamma_{ij}$, $K_{ij}$, $\alpha$, and $\beta^i$. From there, we already know how to take a conformal decomposition for the BSSN formalism. The relation between the ADM variables, and a regular 4d metric tensor can be deduced from the line element, but I'm going to stick it in a table for convenience:

|$g_{\mu\nu}$|$0$|$i$|
|-|-|-|
|$0$|$-\alpha^2 + \beta_m \beta^m$|$\beta_i$|
|$j$|$\beta_j$|$\gamma_{ij}$|

|$g^{\mu\nu}$|$0$|$i$|
|-|-|-|
|$0$|$-\alpha^2$|$\alpha^{-2} \beta^i$|
|$j$|$\alpha^{-2} \beta^j$|$\gamma^{ij} - \alpha^{-2} \beta^i \beta^j$|

With the addition of a definition for $K_{ij}$[^kijnotes], we end up with the following relations for our ADM variables:

[^kijnotes]: Where I found $K_{ij}$ from is [here](https://clas.ucdenver.edu/math-clinic/sites/default/files/attached-files/master_project_mach_.pdf) 4-19a, although its a bit of a random source

$$\begin{align}
\gamma_{ij} &= g_{ij}\\
\beta_i &= g_{0i} \\
K_{ij} &= \frac{1}{2 \alpha} (D_i \beta_j + D_j \beta_i - \frac{\partial g_{ij}}{\partial_t})\\
\alpha &= \sqrt{-g_{00} + \beta^m \beta_m}
\end{align}$$

Note that we find $\beta_i$ not $\beta^i$, which you can find by raising with $\gamma^{ij}$. To initialise the christoffel symbol $\tilde{\Gamma}^i$, simply calculate it analytically once you have $\tilde{\gamma}_{ij}$

### Boundary conditions / Differentiation

Here, we're going to use fourth order[^orderconvergence] finite differentiation on a grid. If you remember, to approximate a derivative, mathematically you can do:

[^orderconvergence]: In general, the order of convergence dictates the rate at which our algorithm improves the answer, when eg increasing our grid resolution

$$\frac{\partial f} {\partial x} = \frac{f(x+1) - f(x)}{h}$$

The factor of $h$ is omitted, because we're indexing grid cells. Numerically this is not ideal, and instead we'll use symmetric fourth order differentiation, with coefficients which we can look up [here](https://en.wikipedia.org/wiki/Finite_difference_coefficient):

$$\frac{\partial f} {\partial x} = \frac{\frac{1}{12} f(x - 2) - \frac{2}{3} f(x - 1) + \frac{2}{3} f(x + 1) - \frac{1}{12} f(x + 2)}{h}$$

This tends to produce a decent tradeoff of accuracy vs performance. In this simulation, because we're discretising our data onto a grid, this brings up the question of how we take derivatives at the edge of the grid (as taking derivatives involves adjacent grid cells) - solving this question defines the boundary conditions

In this article we're going to be using periodic boundary conditions. To implement this, when implementing derivatives, you simply wrap your coordinates to the other edge of the domain

#### Implementation Considerations

##### Modulo operator

Do be aware that I discovered a pretty sizeable performance bug/compiler limitation when implementing this, where modulo operations are not optimised (even by a constant power of 2!) - which ate nearly all the runtime, so use branches instead

##### Second derivatives

When calculating second derivatives, while you could directly apply a 2d stencil, it is much more efficient to first calculate the first derivatives, store them somewhere, and then calculate the second derivatives from those stored derivatives (by taking the first derivative[^ivesaid] of them). This means that you should only precalculate 1st derivatives for variables for which we take second derivatives, which are the following:

[^ivesaid]: I've said the word derivative too many times. I'm going to go pet my cat

$$\tilde{\gamma}_{ij}, \alpha, \beta^i, W$$

##### Second derivative performance tricks

Second derivatives commute. This means that $\partial_i \partial_j == \partial_j \partial_i$. This is also true for the second covariant derivative of a scalar - though only in that circumstance for covariant derivatives. This brings up a good opportunity for performance improvements - when calculating partial derivatives, consider reordering them so that you lopsidedly differentiate in one direction

That is to say, consider implementing: $\partial_{min(i, j)} \partial_{max(i, j)}$ as a general second order differentiation system. This is good for cache (repeatedly accessing the same memory), increasing the amount of eliminateable duplicate code, and ensuring that we maximise the amount we differentiate in the $_0$th direction, which is the fastest due to memory layout

Eg, if we precalculate the first derivative $\partial_i \alpha$, calculating $\partial_0 \partial_i \alpha$ will always be faster than calculating $\partial_i \partial_0 \alpha$. Partly because of memory layout (differentiating in the x direction is virtually free, as it is very cache friendly), and partly because if we *always* do this, we improve our cache hit rate. This is worth about a 20% performance bump[^perfnotes]:

[^perfnotes]: This performance increase is difficult to interpret. In this test case, we use slightly more vGPRs (vector general purpose registers, what you might think of as a traditional cpu register), but significantly reduce our code instruction footprint from 35KB (over the icache size of 32KB), to below our maximum cache size (AMD don't report it annoyingly). This reduces icache stalls, and appears to be very related to the performance increase. Its a big win, as there's 0% chance of us hitting a better vGPR budget[^budget] (as we're in the low 200s)

[^budget]: If you're unfamiliar, the number of threads we can execute on a GPU simultaneously is limited by the number of registers we use, and is known as occupancy (high occupancy = more threads executing). Traditional reasoning is that high occupancy = good as it hides memory latency, although low occupancy can have have performance benefits in some circumstances. GPUs are a pain

### Integrating the PDEs

Each timestep is broken up into a fixed number of substeps, as we solve the implicit equation for backwards euler via fixed point iteration. In each substep, we:

1. Enforce the algebraic constraints
2. Calculate the derivatives (todo: Think i need to segment this off more)
3. Calculate the momentum constraint
4. Evolve our equations to calculate yn + f(yn+1), the result of the substep

After we've performed a fixed number of iterations, I apply Kreiss-Oliger dissipation to remove numerical noise

## Implementation

We now have all of the theory out of the way, phew. Its time to get into the code

### Initial conditions

The code for the initial conditions can be found at todo:s

This is the actual metric tensor we're using, with a generic argument so that its easy to plug dual numbers (which is how I differentiate things here):

```c++
auto wave_function = []<typename T>(const tensor<T, 4>& position)
{
    float A = 0.1f;
    float d = 1;

    auto H = A * sin(2 * std::numbers::pi_v<float> * (position.y() - position.x()) / d);

    metric<T, 4, 4> m;
    m[0, 0] = -1 * (1 - H);
    m[1, 1] = (1 - H);
    m[2, 2] = 1;
    m[3, 3] = 1;

    return m;
};
```

#### Building the ADM variables

This isn't too complicated to implement, though you will need the ability to differentiate your metric tensor (either numerically, or analytically). Here I'm using dual numbers to implement differentiation, via the replay mechanism implemented in previous articles:

```c++
struct adm_variables
{
    metric<valuef, 3, 3> Yij;
    tensor<valuef, 3, 3> Kij;
    valuef gA;
    tensor<valuef, 3> gB;
};

template<typename T>
adm_variables make_adm_variables(T&& func, v4f position)
{
    using namespace single_source;

    adm_variables adm;

    metric<valuef, 4, 4> Guv = func(position);

    tensor<valuef, 4, 4, 4> dGuv;

    for(int k=0; k < 4; k++)
    {
        auto ldguv = diff_analytic(func, position, k);

        for(int i=0; i < 4; i++)
        {
            for(int j=0; j < 4; j++)
            {
                dGuv[k, i, j] = ldguv[i, j];
            }
        }
    }

    metric<valuef, 3, 3> Yij;

    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            Yij[i, j] = Guv[i+1, j+1];
        }
    }

    tensor<valuef, 3, 3, 3> Yij_derivatives;

    for(int k=0; k < 3; k++)
    {
        for(int i=0; i < 3; i++)
        {
            for(int j=0; j < 3; j++)
            {
                Yij_derivatives[k, i, j] = dGuv[k+1, i+1, j+1];
            }
        }
    }

    tensor<valuef, 3, 3, 3> Yij_christoffel = christoffel_symbols_2(Yij.invert(), Yij_derivatives);

    pin(Yij_christoffel);

    tensor<valuef, 3> gB_lower;
    tensor<valuef, 3, 3> dgB_lower;

    for(int i=0; i < 3; i++)
    {
        gB_lower[i] = Guv[0, i+1];

        for(int k=0; k < 3; k++)
        {
            dgB_lower[k, i] = dGuv[k+1, 0, i+1];
        }
    }

    tensor<valuef, 3> gB = raise_index(gB_lower, Yij.invert(), 0);

    pin(gB);

    valuef gB_sum = sum_multiply(gB, gB_lower);

    valuef gA = sqrt(-Guv[0, 0] + gB_sum);

    ///https://clas.ucdenver.edu/math-clinic/sites/default/files/attached-files/master_project_mach_.pdf 4-19a
    tensor<valuef, 3, 3> gBjDi = covariant_derivative_low_vec(gB_lower, dgB_lower, Yij_christoffel);

    pin(gBjDi);

    tensor<valuef, 3, 3> Kij;

    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            Kij[i, j] = (1/(2 * gA)) * (gBjDi[j, i] + gBjDi[i, j] - dGuv[0, i+1, j+1]);
        }
    }

    adm.Yij = Yij;
    adm.Kij = Kij;
    adm.gA = gA;
    adm.gB = gB;

    return adm;
}
```

#### Building the BSSN variables

From here, its straightforward to perform the conformal decomposition, and recover our BSSN variables

```c++
    adm_variables adm = make_adm_variables(wave_function, {0, wpos.x(), wpos.y(), wpos.z()});

    valuef W = pow(adm.Yij.det(), -1/6.f);
    metric<valuef, 3, 3> cY = W*W * adm.Yij;
    valuef K = trace(adm.Kij, adm.Yij.invert());

    tensor<valuef, 3, 3> cA = W*W * (adm.Kij - (1.f/3.f) * adm.Yij.to_tensor() * K);
```

Calculating the christoffel symbol is left as a separate post processing step (because differentiating $$\tilde{\gamma}_{ij}$$ is a bit troublesome), but is directly calculated from the metric tensor:

```c++
tensor<valuef, 3, 3, 3> dcY;

for(int k=0; k < 3; k++)
{
    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            dcY[k, i, j] = diff1(cY[i, j], k, scale.get());
        }
    }
}

auto christoff2 = christoffel_symbols_2(icY, dcY);

tensor<valuef, 3> calculated_cG;

for(int i=0; i < 3; i++)
{
    valuef sum = 0;

    for(int m=0; m < 3; m++)
    {
        for(int n=0; n < 3; n++)
        {
            sum += icY[m, n] * christoff2[i, m, n];
        }
    }

    calculated_cG[i] = sum;
}
```

### Algebraic constraint enforcement

This is very straightforward:

```c++
valuef det_cY_pow = pow(cY.det(), 1.f/3.f);

metric<valuef, 3, 3> fixed_cY = cY / det_cY_pow;
tensor<valuef, 3, 3> fixed_cA = trace_free(cA, fixed_cY, fixed_cY.invert());
```

### Derivatives + Boundary conditions

The boundary conditions are implemented whenever we need to take a derivative of something, and so the two topics are tied together

Below is how I generate the adjacent grid cells, so we can apply finite difference coefficients to them. This is a slightly unusual way to do this, so its worth going over

```c++
///takes a memory access like v[pos, dim], and generates the sequence
///v[pos - {2, 0, 0}, dim], v[pos - {1, 0, 0}, dim], v[pos, dim], v[pos + {1, 0, 0}, dim], v[pos + {2, 0, 0}, dim]
///though in the grid direction pointed to by "direction"
template<std::size_t elements = 5, typename T>
auto get_differentiation_variables(const T& in, int direction)
{
    std::array<T, elements> vars;

    ///for each element, ie x-2, x-1, x, x+1, x+2
    for(int i=0; i < elements; i++)
    {
        ///assign to the original element, ie x
        vars[i] = in;

        ///for every argument, and ourselves
        vars[i].recurse([&i, direction](value_base& v)
        {
            ///check if we're indexing a memory location
            if(v.type == value_impl::op::BRACKET)
            {
                //if so, substitute our memory access with an offset one, for whichever coefficent we want
                auto get_substitution = [&i, direction](const value_base& v)
                {
                    assert(v.args.size() == 8);

                    ///the encoding format for the bracket operator is
                    ///buffer name, argument number, pos[0], pos[1], pos[2], dim[0], dim[1], dim[2]
                    auto buf = v.args[0];

                    ///pull out the position
                    std::array<value_base, 3> pos = {v.args[2], v.args[3], v.args[4]};
                    ///pull out the array dimensions
                    std::array<value_base, 3> dim = {v.args[5], v.args[6], v.args[7]};

                    ///generates the sequence -2, -1, 0, 1, 2 for elements=5, ie grid index offsets
                    int offset = i - (elements - 1)/2;

                    ///update our position
                    pos[direction] = pos[direction] + valuei(offset);

                    ///implement the periodic boundary
                    if(offset > 0)
                        pos[direction] = ternary(pos[direction] >= dim[direction], pos[direction] - dim[direction], pos[direction]);

                    if(offset < 0)
                        pos[direction] = ternary(pos[direction] < valuei(0), pos[direction] + dim[direction], pos[direction]);

                    ///create a new memory indexing operation with our new position
                    value_base op;
                    op.type = value_impl::op::BRACKET;
                    op.args = {buf, value<int>(3), pos[0], pos[1], pos[2], dim[0], dim[1], dim[2]};
                    op.concrete = get_interior_type(T());

                    return op;
                };

                v = get_substitution(v);
            }
        });
    }

    return vars;
}
```

Then this is a traditional 4th order finite difference scheme:

```c++
valuef diff1(const valuef& in, int direction, const valuef& scale)
{
    ///4th order derivatives
    std::array<valuef, 5> vars = get_differentiation_variables(in, direction);

    ///implemented like this, so that we have floating point symmetry in a symmetric grid
    valuef p1 = -vars[4] + vars[0];
    valuef p2 = valuef(8.f) * (vars[3] - vars[1]);

    return (p1 + p2) / (12.f * scale);
}

///this uses the commutativity of partial derivatives to lopsidedly prefer differentiating dy in the x direction
///as this is better on the memory layout
valuef diff2(const valuef& in, int idx, int idy, const valuef& dx, const valuef& dy, const valuef& scale)
{
    using namespace single_source;

    if(idx < idy)
    {
        ///we must use dy, therefore swap all instances of diff1 in idy -> dy
        alias(diff1(in, idy, scale), dy);

        return diff1(dy, idx, scale);
    }
    else
    {
        ///we must use dx, therefore swap all instances of diff1 in idx -> dx
        alias(diff1(in, idx, scale), dx);

        return diff1(dx, idy, scale);
    }
}
```

The way that this is implemented code-wise is slightly unusual. All variables in our simulation are generally bound to a particular grid location, `cY` as a 3x3 tensor is, under the hood, tied to a specific coordinate - ie `cY = cY_buffer[pos, dim]`. This is to minimise the complexity of the equations

Because of this, instead of having a buffer and a position for us to differentiate at, we have a variable which is derived from a buffer, and an expression tree containing a bracket operator - with our initial position and dimension as arguments. The huge upside of this is that we can differentiate compound expressions, like this:

```c++
diff1(cY * cA, 0, scale);
```

You can look through all the internal buffer accesses, and perform the substitution on all of them to correctly differentiate this. To illustrate this in one dimension, you end up with this:

```c++
cY[x - 2] * cA[x - 2]
cY[x - 1] * cA[x - 1]
cY[x] * cA[x]
cY[x + 1] * cA[x + 1]
cY[x + 2] * cA[x + 2]
```

Which is pretty neat

### Momentum constraint

The specific form of the momentum constraint I listed earlier - while the canonical one - involves calculating covariant derivatives which is unnecessary, we can use a slightly better form[^linky]

[^linky]: [here](https://arxiv.org/pdf/1205.5111v1.pdf) (54)

$$\mathcal{M_i} = \partial_j \tilde{A}_i^j - \frac{1}{2} \tilde{\gamma}^{jk} \tilde{A}_{jk,i} + 6 \partial_j \phi \tilde{A}_i^j - \frac{2}{3} \partial_i K$$

```c++
tensor<valuef, 3> calculate_momentum_constraint(bssn_args& args, const valuef& scale)
{
    valuef X = args.W*args.W;

    tensor<valuef, 3> dW;

    for(int i=0; i < 3; i++)
        dW[i] = diff1(args.W, i, scale);

    tensor<valuef, 3> dX = 2 * args.W * dW;

    ///https://arxiv.org/pdf/1205.5111v1.pdf (54)
    tensor<valuef, 3, 3> aij_raised = raise_index(args.cA, args.cY.invert(), 1);

    tensor<valuef, 3> dPhi = -dX / (4 * max(X, valuef(0.0001f)));

    tensor<valuef, 3> Mi;

    for(int i=0; i < 3; i++)
    {
        valuef s1 = 0;

        for(int j=0; j < 3; j++)
        {
            s1 += diff1(aij_raised[i, j], j, scale);
        }

        valuef s2 = 0;

        for(int j=0; j < 3; j++)
        {
            for(int k=0; k < 3; k++)
            {
                s2 += -0.5f * args.cY.invert()[j, k] * diff1(args.cA[j, k], i, scale);
            }
        }

        valuef s3 = 0;

        for(int j=0; j < 3; j++)
        {
            s3 += 6 * dPhi[j] * aij_raised[i, j];
        }

        valuef p4 = -(2.f/3.f) * diff1(args.K, i, scale);

        Mi[i] = s1 + s2 + s3 + p4;
    }

    return Mi;
}
```

This is stored in a buffer, and will be used to damp the evolution equations

### Evolution

#### Evolution equations by Field

##### $\tilde{\gamma}_{ij}$

$$\partial_t \tilde{\gamma}_{ij} = \beta^m \partial_m \tilde{\gamma}_{ij}+ 2 \tilde{\gamma}_{m(i} \partial_{j)} \beta^m - \frac{2}{3} \tilde{\gamma}_{ij} \partial_m \beta^m - 2 \alpha \tilde{A}_{ij}$$


```c++
tensor<valuef, 3, 3> get_dtcY(bssn_args& args, bssn_derivatives& derivs, valuef scale)
{
    using namespace single_source;

    inverse_metric<valuef, 3, 3> icY = args.cY.invert();
    pin(icY);

    ///https://arxiv.org/pdf/1307.7391 specifically for why the trace free aspect
    ///https://arxiv.org/pdf/1106.2254 also see here, after 25
    return lie_derivative_weight(args.gB, args.cY.to_tensor(), scale) - 2 * args.gA * trace_free(args.cA, args.cY, icY);
}
```

The lie derivative function is worth examining. The metric, and trace free curvature are both tensor densities, and the first terms in both equations are actually a [lie derivative](https://en.wikipedia.org/wiki/Lie_derivative#Coordinate_expressions). The definition is for a tensor density with weight $-\frac{2}{3}$

```c++
template<typename T, int N>
inline
tensor<T, N, N> lie_derivative_weight(const tensor<T, N>& B, const tensor<T, N, N>& mT, const T& scale)
{
    tensor<T, N, N> lie;

    for(int i=0; i < N; i++)
    {
        for(int j=0; j < N; j++)
        {
            T sum = 0;
            T sum2 = 0;

            for(int k=0; k < N; k++)
            {
                sum += B[k] * diff1(mT[i, j], k, scale);
                sum += mT[i, k] * diff1(B[k], j, scale);
                sum += mT[j, k] * diff1(B[k], i, scale);
                sum2 += diff1(B[k], k, scale);
            }

            lie[i, j] = sum - (2.f/3.f) * mT[i, j] * sum2;
        }
    }

    return lie;
}
```

##### $\tilde{A}_{ij}$

$$
\begin{align}
\partial_t \tilde{A}_{ij} &= \beta^m \partial_m \tilde{A}_{ij}+ 2 \tilde{A}_{m(i} \partial_{j)} \beta^m - \frac{2}{3} \tilde{A}_{ij} \partial_m \beta^m + \alpha K \tilde{A}_{ij} \\&- 2 \alpha \tilde{A}_{im} \tilde{A}^m_{\;\;j} + W^2 (\alpha \mathcal{R}_{ij} - D_i D_j \alpha)^{TF} \\&+ k_3 \alpha \tilde{D}_{(i} \mathcal{M}_{j)}
\end{align}
$$

```c++
tensor<valuef, 3, 3> get_dtcA(bssn_args& args, bssn_derivatives& derivs, v3f momentum_constraint, const valuef& scale)
{
    using namespace single_source;

    auto icY = args.cY.invert();
    pin(icY);

    auto W2DiDja = calculate_W2DiDja(args, derivs, scale);
    pin(W2DiDja);

    tensor<valuef, 3, 3> with_trace = args.gA * calculate_W2Rij(args, derivs, scale) - W2DiDja;

    tensor<valuef, 3, 3> aij_amj;

    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            tensor<valuef, 3, 3> raised_Aij = icY.raise(args.cA, 0);

            valuef sum = 0;

            for(int m=0; m < 3; m++)
            {
                sum += args.cA[i, m] * raised_Aij[m, j];
            }

            aij_amj[i, j] = sum;
        }
    }

    tensor<valuef, 3, 3> dtcA = lie_derivative_weight(args.gB, args.cA, scale)
                                + args.gA * args.K * args.cA
                                - 2 * args.gA * aij_amj
                                + trace_free(with_trace, args.cY, icY);

    #define MOMENTUM_CONSTRAINT_DAMPING
    #ifdef MOMENTUM_CONSTRAINT_DAMPING
    auto christoff2 = christoffel_symbols_2(icY, derivs.dcY);
    pin(christoff2);

    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            float Ka = 0.1f;

            dtcA[i, j] += Ka * args.gA * 0.5f *
                              (covariant_derivative_low_vec(momentum_constraint, christoff2, scale)[i, j]
                             + covariant_derivative_low_vec(momentum_constraint, christoff2, scale)[j, i]);
        }
    }
    #endif

    return dtcA;
}
```

So, there's several parts here which are slightly different to the equations as written

1. I've moved the $W^2$ term inside the trace free operation
2. $W^2 D_i D_j \alpha$ is calculated, instead of $D_i D_j \alpha$
3. $$W^2 \mathcal{R}_{ij}$$ is calculated instead of $$\mathcal{R}_{ij}$$

###### $$W^2 D_iD_j\alpha$$

$$W^2 D_iD_j\alpha = W^2 \tilde{D}_i \tilde{D}_j \alpha + 1W \partial_{(i} W \partial_{j)} \alpha\ - W \tilde{\gamma}_{ij} \tilde{\gamma}^{mn} \partial_m W \partial_n \alpha$$

```c++
tensor<valuef, 3, 3> calculate_W2DiDja(bssn_args& args, bssn_derivatives& derivs, const valuef& scale)
{
    using namespace single_source;

    auto icY = args.cY.invert();
    pin(icY);

    auto christoff2 = christoffel_symbols_2(icY, derivs.dcY);

    pin(christoff2);

    tensor<valuef, 3, 3> W2DiDja;

    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            valuef v1 = args.W * args.W * double_covariant_derivative(args.gA, derivs.dgA, christoff2, scale)[i, j];

            valuef v2 = args.W * (derivs.dW[i] * diff1(args.gA, j, scale) + derivs.dW[j] * diff1(args.gA, i, scale));

            valuef sum = 0;

            for(int m=0; m < 3; m++)
            {
                for(int n=0; n < 3; n++)
                {
                    sum += icY[m, n] * 2 * args.W * derivs.dW[m] * diff1(args.gA, n, scale);
                }
            }

            valuef v3 = -0.5f * args.cY[i, j] * sum;

            W2DiDja[i, j] = v1 + v2 + v3;
        }
    }

    return W2DiDja;
}
```

###### $$\tilde{\mathcal{R}}_{ij}$$

The curvature tensor:

$$
\tilde{\mathcal{R}}_{ij} = -\frac{1}{2} \tilde{\gamma}^{mn} \partial_m \partial_n \tilde{\gamma}_{ij} + \tilde{\gamma}_{m(i} \partial _{j)} \tilde{\Gamma}^m + \tilde{\Gamma}^m \tilde{\Gamma}_{(ij)m}+ \tilde{\gamma}^{mn} (2 \tilde{\Gamma}^k_{m(i} \tilde{\Gamma}_{j)kn} + \tilde{\Gamma}^k_{im} \tilde{\Gamma}_{kjn})
$$

```c++
tensor<valuef, 3, 3> calculate_cRij(bssn_args& args, bssn_derivatives& derivs, const valuef& scale)
{
    using namespace single_source;

    auto icY = args.cY.invert();
    pin(icY);

    auto christoff1 = christoffel_symbols_1(derivs.dcY);
    auto christoff2 = christoffel_symbols_2(icY, derivs.dcY);

    pin(christoff1);
    pin(christoff2);

    tensor<valuef, 3, 3> cRij;

    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            valuef s1 = 0;

            for(int l=0; l < 3; l++)
            {
                for(int m=0; m < 3; m++)
                {
                    s1 += -0.5f * icY[l, m] * diff2(args.cY[i, j], m, l, derivs.dcY[m, i, j], derivs.dcY[l, i, j], scale);
                }
            }

            valuef s2 = 0;

            for(int k=0; k < 3; k++)
            {
                s2 += 0.5f * (args.cY[k, i] * diff1(args.cG[k], j, scale) + args.cY[k, j] * diff1(args.cG[k], i, scale));
            }

            valuef s3 = 0;

            for(int k=0; k < 3; k++)
            {
                s3 += 0.5f * args.cG[k] * (christoff1[i, j, k] + christoff1[j, i, k]);
            }

            valuef s4 = 0;

            for(int m=0; m < 3; m++)
            {
                for(int l=0; l < 3; l++)
                {
                    valuef inner1 = 0;
                    valuef inner2 = 0;

                    for(int k=0; k < 3; k++)
                    {
                        inner1 += 0.5f * (2 * christoff2[k, l, i] * christoff1[j, k, m] + 2 * christoff2[k, l, j] * christoff1[i, k, m]);
                    }

                    for(int k=0; k < 3; k++)
                    {
                        inner2 += christoff2[k, i, m] * christoff1[k, l, j];
                    }

                    s4 += icY[l, m] * (inner1 + inner2);
                }
            }

            cRij[i, j] = s1 + s2 + s3 + s4;
        }
    }

    pin(cRij);

    return cRij;
}
```

###### $$W^2 \mathcal{R}_{ij}$$

$$
\begin{align}
W^2 \mathcal{R}^W_{ij} &= W (\tilde{D}_i \tilde{D}_j W + \tilde{\gamma}_{ij} \tilde{D}^m \tilde{D}_m W) - 2 \tilde{\gamma}_{ij} \partial^m W \partial_m W \\
W^2 \mathcal{R}_{ij} &= W^2 \tilde{\mathcal{R}}_{ij} + W^2 \mathcal{R}^W_{ij}
\end{align}
$$

```c++
///https://arxiv.org/pdf/1307.7391 (9)
///https://iopscience.iop.org/article/10.1088/1361-6382/ac7e16/pdf 2.6
///this calculates the quantity W^2 * Rij
tensor<valuef, 3, 3> calculate_W2Rij(bssn_args& args, bssn_derivatives& derivs, const valuef& scale)
{
    using namespace single_source;

    auto icY = args.cY.invert();
    pin(icY);

    auto christoff2 = christoffel_symbols_2(icY, derivs.dcY);

    pin(christoff2);

    tensor<valuef, 3, 3> didjW;

    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            didjW[i, j] = double_covariant_derivative(args.W, derivs.dW, christoff2, scale)[j, i];
        }
    }

    tensor<valuef, 3, 3> w2Rphiij;

    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            valuef v1 = args.W * didjW[i, j];
            valuef v2 = 0;

            {
                valuef sum = 0;

                auto raised = icY.raise(didjW, 0);

                for(int l=0; l < 3; l++)
                {
                    sum += raised[l, l];
                }

                v2 = args.cY[i, j] * sum;
            }

            valuef v3 = 0;

            {
                valuef sum = 0;

                for(int l=0; l < 3; l++)
                {
                    sum += icY.raise(derivs.dW)[l] * derivs.dW[l];
                }

                v3 = -2 * args.cY[i, j] * sum;
            }

            w2Rphiij[i, j] = v1 + v2 + v3;
        }
    }

    pin(w2Rphiij);

    return w2Rphiij + calculate_cRij(args, derivs, scale) * args.W * args.W;
}
```

deep joy, but this is the most complex variable to implement

##### $K$

This is a simple one:

$$\partial_t K = \beta^m \partial_m K - W^2 \tilde{\gamma}^{mn} D_m D_n \alpha + \alpha \tilde{A}^{mn} \tilde{A}_{mn} + \frac{1}{3} \alpha K^2
$$

```c++
valuef get_dtK(bssn_args& args, bssn_derivatives& derivs, const valuef& scale)
{
    using namespace single_source;

    auto icY = args.cY.invert();
    pin(icY);

    tensor<valuef, 3, 3> W2DiDja = calculate_W2DiDja(args, derivs, scale);
    pin(W2DiDja);

    tensor<valuef, 3, 3> AMN = icY.raise(icY.raise(args.cA, 0), 1);
    pin(AMN);

    valuef v1 = 0;

    for(int m=0; m < 3; m++)
    {
        v1 += args.gB[m] * diff1(args.K, m, scale);
    }

    return v1 - sum_multiply(icY.to_tensor(), W2DiDja)
              + args.gA * sum_multiply(AMN, args.cA)
              + (1/3.f) * args.gA * args.K * args.K;
}
```

##### $W$

$$\partial_t W = \beta^m \partial_m W + \frac{1}{3} W (\alpha K - \partial_m \beta^m) $$

```c++
///https://iopscience.iop.org/article/10.1088/1361-6382/ac7e16/pdf 2.12 or
///https://arxiv.org/pdf/0709.2160
valuef get_dtW(bssn_args& args, bssn_derivatives& derivs, const valuef& scale)
{
    valuef dibi = 0;

    for(int i=0; i < 3; i++)
    {
        dibi += diff1(args.gB[i], i, scale);
    }

    valuef dibiw = 0;

    for(int i=0; i < 3; i++)
    {
        dibiw += args.gB[i] * diff1(args.W, i, scale);
    }

    return (1/3.f) * args.W * (args.gA * args.K - dibi) + dibiw;
}
```

ez pz

##### $\tilde{\Gamma}^i$

This one translates least well into code, as it doesn't really boil down into abstractions that we could really implement. So its just a pure loopy approach

$$\partial_t \tilde{\Gamma}^i = \beta^m \partial_m \tilde{\Gamma}^i + \frac{2}{3} \tilde{\Gamma}^i \partial_m \beta^m - \tilde{\Gamma}^m \partial_m \beta^i + \tilde{\gamma}^{mn} \partial_m \partial_n \beta^i + \frac{1}{3} \tilde{\gamma}^{im} \partial_m \partial_n \beta^n
$$

```c++
tensor<valuef, 3> get_dtcG(bssn_args& args, bssn_derivatives& derivs, const valuef& scale)
{
    using namespace single_source;

    inverse_metric<valuef, 3, 3> icY = args.cY.invert();
    pin(icY);

    tensor<valuef, 3, 3, 3> christoff2 = christoffel_symbols_2(icY, derivs.dcY);
    pin(christoff2);

    tensor<valuef, 3> dtcG;

    ///dtcG
    {
        tensor<valuef, 3, 3> icAij = icY.raise(icY.raise(args.cA, 0), 1);

        pin(icAij);

        tensor<valuef, 3> Yij_Kj;

        for(int i=0; i < 3; i++)
        {
            valuef sum = 0;

            for(int j=0; j < 3; j++)
            {
                sum += icY[i, j] * diff1(args.K, j, scale);
            }

            Yij_Kj[i] = sum;
        }

        for(int i=0; i < 3; i++)
        {
            valuef s1 = 0;

            for(int j=0; j < 3; j++)
            {
                for(int k=0; k < 3; k++)
                {
                    s1 += 2 * args.gA * christoff2[i, j, k] * icAij[j, k];
                }
            }

            valuef s2 = 2 * args.gA * -(2.f/3.f) * Yij_Kj[i];

            valuef s3 = 0;

            for(int j=0; j < 3; j++)
            {
                s3 += icAij[i, j] * 2 * derivs.dW[j];
            }

            s3 = 2 * (-1.f/4.f) * args.gA / max(args.W, valuef(0.0001f)) * 6 * s3;

            valuef s4 = 0;

            for(int j=0; j < 3; j++)
            {
                s4 += icAij[i, j] * diff1(args.gA, j, scale);
            }

            s4 = -2 * s4;

            valuef s5 = 0;

            for(int j=0; j < 3; j++)
            {
                s5 += args.gB[j] * diff1(args.cG[i], j, scale);
            }

            valuef s6 = 0;

            for(int j=0; j < 3; j++)
            {
                s6 += -args.cG[j] * derivs.dgB[j, i];
            }

            valuef s7 = 0;

            for(int j=0; j < 3; j++)
            {
                for(int k=0; k < 3; k++)
                {
                    s7 += icY[j, k] * diff2(args.gB[i], k, j, derivs.dgB[k, i], derivs.dgB[j, i], scale);
                }
            }

            valuef s8 = 0;

            for(int j=0; j < 3; j++)
            {
                for(int k=0; k < 3; k++)
                {
                    s8 += icY[i, j] * diff2(args.gB[k], k, j, derivs.dgB[k, k], derivs.dgB[j, k], scale);
                }
            }

            s8 = (1.f/3.f) * s8;

            valuef s9 = 0;

            for(int k=0; k < 3; k++)
            {
                s9 += derivs.dgB[k, k];
            }

            s9 = (2.f/3.f) * s9 * args.cG[i];

            dtcG[i] = s1 + s2 + s3 + s4 + s5 + s6 + s7 + s8 + s9;
        }
    }

    return dtcG;
}
```

##### $\alpha$

The harmonic gauge condition:

$$\partial_t \alpha = - 2 \alpha^2 K + \beta^m \partial_m \alpha$$

```c++
valuef get_dtgA(bssn_args& args, bssn_derivatives& derivs, valuef scale)
{
    valuef bmdma = 0;

    for(int i=0; i < 3; i++)
    {
        bmdma += args.gB[i] * diff1(args.gA, i, scale);
    }

    return -args.gA * args.gA * args.K + bmdma;
}
```

Note that bmdma is always 0, because we use a 0 shift gauge

##### $\beta^i$

Unsurprisingly: This is not complicated

$$\beta^i = 0$$

```c++
tensor<valuef, 3> get_dtgB(bssn_args& args, bssn_derivatives& derivs, valuef scale)
{
    return {0,0,0};
}
```

### Kreiss-Oliger dissipation

$$\partial_t U = \partial_t U + \epsilon \frac{h^5}{64} (\frac{\partial^6}{\partial x^6} + \frac{\partial^6}{\partial y^6} + \frac{\partial^6}{\partial z^6})U$$

There are two parts to Kreiss-Oliger

```c++
valuef diff6th(const valuef& in, int idx, const valuef& scale)
{
    auto vars = get_differentiation_variables<7>(in, idx);

    valuef p1 = vars[0] + vars[6];
    valuef p2 = -6 * (vars[1] + vars[5]);
    valuef p3 = 15 * (vars[2] + vars[4]);
    valuef p4 = -20 * vars[3];

    return p1 + p2 + p3 + p4;
}
```

The 6th derivative with second order accuracy: note as mentioned previously I'm omitting the scale factor here, because its redundant

```c++
valuef kreiss_oliger_interior(valuef in, valuef scale)
{
    valuef val = 0;

    for(int i=0; i < 3; i++)
    {
        val += diff6th(in, i, scale);
    }

    int n = 6;
    float p = n - 1;

    int sign = pow(-1, (p + 3)/2);

    int divisor = pow(2, p+1);

    float prefix = (float)sign / divisor;

    return (prefix / scale) * val;
}
```

Because of the omitted scale factor, we end up with a single division by the scale. After this, simply accumulate into your buffer, and multiply by the timestep and the strength:

```c++
as_ref(inout[lid]) = in[lid] + eps.get() * timestep.get() * kreiss_oliger_interior(in[pos, dim], scale.get());
```

Todo: Implicit solving?

## Results

The full code for this article can be found over at todo:

To evaluate how well this is working, we want to check two things

1. Do we preserve the wave reasonably well?
2. Are the constraint errors bounded?

<iframe width="560" height="315" src="https://www.youtube.com/embed/j5i8MsSqX9Y?si=rT_OMCH4CoWm6LcM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

This is for a 128^3 sized simulation, a timestep of 0.001, and with video capturing software that can't handle sRGB correctly. $k_3$ for the momentum constraint damping is set to $0.025$. The precision of our variables in memory is 32-bit floats, and 16-bit[^bit16] floats are used for precalculated derivatives. We can see here that the momentum constraint errors are decreasing, the hamiltonian constraint errors don't blow up horribly, and the nature of the wave we're simulating is preserved - though it does slowly dissipate. Oddly, we do way better here than we should do, according to [this](https://arxiv.org/pdf/1106.2254) paper the bssn formalism should have failed with unbounded constraint errors - but I won't complain too much about things going well

![constraints](/assets/constraintsfinally.png)

[^bit16]: This might seem a slightly surprising choice in software intended to be scientific, but amazingly enough it has very little bearing on the evolution results. It is however easily swappable for exactly this reason, to facilitate testing - and I have extensively tested this with BBH sims. Given that the range of derivatives is inherently small in a smooth solution, fixed point might provide better results than 16-bit floats. The main advantage of 16-bit floats is the reduced memory usage, which will enable us to simulate higher resolution spacetimes in the future. In general, the appropriate storage format for these problems is an interesting question, as floating point wastes a huge amount of precision when we have variables with a known range

Performance here is excellent - with diagnostics (which are not at all optimised) disabled, it takes 16ms/tick (on a 6700xt), and 29ms/tick with them enabled. This is very competitive with a different simulator I wrote for binary black hole collisions, so we're in a good spot for the future

All in all: Great success

# Next time

This article is fairly dry with unexciting results - for good reason. If we tried to cram black hole collisions in here, it'd be even longer and I suspect no amount of cat pictures would save us from the sheer length of this piece. Luckily, future articles should be a *lot* simpler than this one - we've gotten the bulk of the technical work out of the way, which leaves us free to implement cool stuff

If you have any questions about this, or you get lost - please feel free to contact me. This is one of those areas that's brutally difficult to get into as there are no guides, and I'm very happy to help people implement stuff














