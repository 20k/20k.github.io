---
layout: post
title:  "Numerical Relativity 101: How to simulate spacetime itself"
date:   2024-07-21 18:33:23 +0000
categories: C++
---

Papers:

https://arxiv.org/pdf/1307.7391 ccz4 paper, contains some good equation references
https://iopscience.iop.org/article/10.1088/1361-6382/ac7e16/pdf covariant bssn
https://www2.yukawa.kyoto-u.ac.jp/~masaru.shibata/PRD52.5428.pdf cG mentions the motivation behind the gamma term
stability paper https://arxiv.org/pdf/0707.0339


Ok planola time:

1. Basic introduction to the ADM formalism. Need to cover projection, the concept of time + gauge conditions, physical interpretation of the gauge conditions as an eularian observer, and the concept of a hypersurface
2. Introduce the adm equations
3. Introduce the vanilla BSSN equations
4. Talk through all the notation
5. Swap to structuring a simulation
6. Talk through the high level flow
7. Initial conditions: waves
8. Boundary conditions: periodic
9. Evolution equations: Bssn, momentum constraint damping, algebraic constraints, kreiss-oliger dissipation
10. Gauge condition evolution
11. Some kind of diagnostic (semi analytic tests?)

Hi there! Today we're going to really get right into the middle of general relativity. In previous articles, we've been taking a known, analytic solution - like Kerr, wormholes, the schwarzschild metric etc, and doing maths on it. These kinds of solutions are the exception for specific circumstances however, and if we want to do more exciting things - like colliding black holes together - there simply isn't an analytic solution to it

General relativity is relatively complex, and unfortunately the intersection between GR, and the code on your computer, is going to ramp that up a notch. As far as I'm aware, essentially none of this information exists outside of papers, so I'm going to attempt to synthesise this all into a fairly approachable format. Surprisingly enough, you don't need to know all that much about general relativity for this tutorial series, but you do need to have a high level of patience

Without further ado, lets get into it

# Time in general relativity

The key issue for simulating general relativity is that of time. The einstein field equations simply dictate how the 4 dimensional spacetime should look declaratively - but they do not have any concept of evolving forwards in time. The very nature of solving a partial differential equation is to start off with some kind of slice in time, and evolve that slice forwards until we hit our end state, so the puzzle is how we rework the equations of general relativity which do not evolve forwards, into something we can solve as a system of PDEs

Here we'll be looking at the ADM (Arnowitt–Deser–Misner) formalism. It splits up 4d spacetime into a 3+1 split: three spatial dimensions, and it singles out a single time dimension. Do note that this isn't the only way of doing it, and there are a number of successful formalisms, but ADM is the one with the most available information (and the most understandable in my opinion)

## 3+1 split

The idea is that we take a series of 3d spatial slices through our 4d spacetime - each through a 'moment' in time. The time coordinate is arbitrary (and we'll get back to it), but this spatial slicing is fairly arbitrary. From now on, all latin indices $_{ijk}$ have the value 1-3, and refer to quantities on our 3d slice, whereas green indices $_{\mu\nu\kappa}$ take on the value 0-3 and refer to quantities in our 4d spacetime

The way we chop up our spacetime into 3d slices is known as the foliation, and with this we'll introduce our first piece of ADM specific notation: $\alpha$, the lapse, and $\beta^i$, the shift, a 3 component vector. These are also often sometimes labelled $N$ and $N^i$ respectively - though I will not use this notation. Together, these point forwards in time, and make up a 'normal' vector to the 3d surface we're working with

These variables are called the gauge variables, and dictate the foliation of spacetime. They are in general arbitrary. The general line element in ADM is this:

$$ds^2 = -\alpha^2 dt^2 + \gamma_{ij} (dx^i + \beta^i dt)(dx^j + \beta^j)$$

$\gamma_{ij}$ is the 3-metric (often called the induced metric) which is used on any quantity in our 3d slice, and operates like a regular metric tensor. That is to say, if we have some quantity $Q_i$ on our 3d slice, we can raise and lower its indices with $\gamma_{ij}$ as follows

$$Q^i = \gamma^{ij} Q_j\\
Q_i = \gamma_{ij} Q^j$$

If our 4-metric is $g_{\mu\nu}$, then our 3-metric is $\gamma_{ij} = g_{ij}$, ie the spatial parts. While $\gamma^{ij} = (\gamma_{ij})^-1$, $\gamma^{ij} \neq g^{ij}$

## Equations of motion

Once we have this foliation, it is possible to define a lagrangian[^mechanics] for the evolution from one 3d slice to the next. A lagrangian directly leads to a hamiltonian formalism, and equations of motion, and you can see these equations over here:

[^mechanics]: If you don't know lagrangian or hamiltonian mechanics, its not especially important. The only thing to really be aware of is that it introduces some terms known as constraints: The hamiltonian constraint $H=0$, and the momentum constraint $M_i = 0$. These must both be satisfied as our simulation progresses

https://en.wikipedia.org/wiki/ADM_formalism#Lagrangian_formulation

We end up with two PDEs - one for our metric $\gamma_{ij}$, and one for our 'conjugate momentum', $\pi^{ij}$. On top of this, we must also supply evolution equations for the lapse and shift - these are both arbitrary (in theory), and any choice within reason will lead to a valid evolution - we could pick $1$ and $(0,0,0)$ respectively (and this slicing *is* sometimes used)

Unfortunately, there's a big problem

# The ADM equations of motion are bad

Part of the reason why we're not going into this more is that - unfortunately - these equations of motion are fairly useless to us. They are *very* numerically unstable, to the point where for our purposes there's no point trying to solve them. Understanding this issue, and solving it was an outstanding problem for decades, and there is an absolutely enormous amount written in the literature on this topic. A lot of this article series is going to be dealing with the very poor character of the equations

Luckily, because its not 1990 - and because the astrophysics community rocks and makes all this information public - we get to skip to the part where we have a workable set of equations

## Enter: The BSSN formalism

The BSSN formalism is the most widely used formalism - its an ADM derivative, and most formalisms are in some way based on the BSSN formalism. It was the very successful formalism to enter the picture - humorously, nobody really knew why it worked for a very long time after it entered the picture: Todo: Explain

However, there isn't one canonical BSSN formalism. There are instead 3 major variants on the equations:

## Conformal decomposition

Todo: Go through phi, W^2, and X. Also note that people pretty clearly mix up the notation here

## Variables

## Equations of motion

## Gauge conditions

## Constraint equations

## Analysis

### The BSSN equations of motion are bad

So, as written, the BSSN equations of motion are - drumroll - very numerically unstable. The BSSN formalism, as mentioned, has the following constraints:

These two are known as the algebraic constraints

$$\tilde{\gamma} = 1 \\
\tilde{\gamma}^{mn} \tilde{A}_{mn} = 0
$$

This constraint is the christoffel symbol constraint:

$$G^i = \tilde{\Gamma}^i - \tilde{\gamma}^{mn}\tilde{\Gamma}^i_{mn} = 0$$

These two constraints are the hamiltonian constraint, and the momentum constraint respectively, and originate from the lagrangian derivation of the equations of motion

$$H = whatever = 0\\
M_i = whatever = 0$$

I'll summarise two decades of the conventional wisdom from stability analysis here:

1. If the second algebraic constraint is violated, ie the trace of the trace-free extrinsic curvature != 0, the violation blows up and your simulation will immediately fail
2. The christoffel symbol constraint is very important to damp, and can lead to less direct simulation failures
3. Correcting momentum constraint violations is also fairly important - not because they're exponential however, but because it dissipates away noise in your simulation
4. The quantity $\partial_k\tilde{\Gamma}^i_{mn}$ is numerically unstable, though the quantity $\tilde{\Gamma}^i_{mn}$ is not
5. The BSSN formalism inherently generates constraint violations under certain conditions, leading to self-blowup

Additionally:

1. Hamiltonian constraint violations are completely unimportant for stability purposes, and seem to be largely irrelevant for physical accuracy beyond a reasonably justified paranoia
2. The first algebraic constraint is relatively unimportant to enforce, but its easy to do - so we may as well

I have some of my own thoughts here, and I'm not entirely convinced by some of what I've been presented in the literature, but we'll get to that when it becomes relevant

### Stabilising the BSSN equations of motion

This is an enormous topic in itself, and we're going to get into this a lot. As-is, I'm going to present the bare minimum we need to get up off the ground for this article, because in the future we're going to be directly testing all of this. The goal is to make as general of a simulation as possible, so we're not going to use many of the tricks common in the literature - its pretty common to tailor stability modifications to certain scenarios, or dynamically adjust your constants

After each timestep, the following constraints should be enforced:

1. Gamma blah blah. Either do the det, or damp
2. Algebraic blah blah. Either do the det, or damp with the TF

And we must damp the following two constraints as follows

1. Momentum blah blah
2. Sigma blah blah

On top of this to stabilise our equations more generally, we must:

1. Add kreiss oliger dissipation. There will be a segment on this, so don't worry yet

This is enough to get us up and running off the ground

Next up, we need to pick an integrator to integrate our PDEs with

## Picking an integrator

We're going to diverge from the literature significantly here, for very good reason. The BSSN equations are a classically stiff (unstable) set of equations, and they oscillate pretty wildly. Its traditional to use something similar to RK4 to integrate the equations of motion

If you're familiar with solving stiff PDEs, it might seem a bit odd to use RK4 - RK4 is an explicit integrator - and as such has fairly poor stability properties when solving stiff equations. For many kinds of problem, an explicit integrator works great, but not for unstable problems like this. Instead, we're going to use an L-stable implicit integrator which has much more appropriate convergence properties - backwards euler

As far as I'm aware, there's been no testing of fully implicit integration in numerical relativity - the closest is IMEX stepping from the SXS group, but it does not implicitly solve derivatives due to performance constraints

### Solving implicit equations efficiently

Forwards euler has the form:

$$y_{n+1} = y_n + dt f(t_n, y_n)$$

This is the classic 1st order, explicit integrator. It has poor stability properties, and does not have a high order of convergence

Backwards euler has the form:

$$y_{n+1} = y_n + dt f(t_{n+1}, y_{n+1})$$

This is also a 1st order integrator, but it is implicit - we can see that we have $y_{n+1}$ on both sides of the equation. The real trick is that backwards euler is L-stable - which means that it has extremely good stability properties for solving unstable equations, which is exactly what we're looking for. On top of this, backwards euler has a very GPU-friendly form, making it very amenable to GPU architecture, while using a minimal amount of memory. In all my testing, I haven't found an integrator with better properties than this

One notable feature is that with an integrator like this, we need less damping and stability modifications to make the equations work well. The downside is that we're solving our equations in a fundamentally different way to what is standard, so we now have to extensively test things to figure out the character of our solution ourselves

### Fixed point iteration

The issue with an implicit integrator is: How do we actually solve this equation? The straightforward answer is to use fixed point iteration

Todo: Fixed point

This also maps very well to gpu architecture, is numerically stable, and in general is extremely fast and robust. There are tricks to speed up fixed point iteration as well, which we'll get into in the future

## Flow of a simulation

There are three components to any good simulation here

1. Initial conditions
2. Boundary conditions
3. Integrating our PDEs

### Initial conditions

We need to define on our initial slice what our variables are. Here, we're simulating an infinite field of gravitational waves, and our initial variables take the following form:

### Boundary conditions

We're going to be using periodic boundary conditions - at the edge of our simulation, we simply wrap our blhabbfkdfblkjkb

### Integrating the PDEs

Each timestep is broken up into a fixed number of substeps, as we solve the implicit equation for backwards euler. In each substep, we:

1. Calculate our derivatives (todo: Think i need to segment this off more)
2. Calculate the momentum constraint, if we're using momentum constraint damping
3. Evolve our equations to calculate yn + f(yn+1)

After each substep - once we have our final answer, we apply kreiss-oliger dissipation to the result, to remove high frequencies from our grid

#### Kreiss-Oliger

drerrdrfsfdd

## Implementation

We now have all of the theory out of the way. Lets look at the code for our initial conditions first:

### Initial conditions

dfdfdf

### Boundary conditions

The boundary conditions are implemented whenever we need to take a derivative of something, which looks like this

### Evolution equations

This is the real kicker

#### Calculating derivatives

We only need to calculate 1st derivatives and save them when we take the second derivative of something

#### Evolution equations by Field

##### cY

##### cA

##### K

##### W

##### gA

##### G

##### Gauge conditions

Geodesic slicing, a = 1, b = 0
harmonic slicing https://arxiv.org/pdf/2201.08857 a = -a^2 K

1+log = -2 a K
Gamma = blahblah

Note that structurally for backwards euler we need a buffer for our input (yn+1), one for our output (yn + f(yn+1)), and one to hold the last value (yn)

### Kreiss-Oliger

Finally afterwards we numerically dissipate away the noise via kreiss-oliger

## Results