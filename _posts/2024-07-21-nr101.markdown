---
layout: post
title:  "Numerical Relativity 101: How to simulate spacetime itself"
date:   2024-07-21 18:33:23 +0000
categories: C++
---

Papers:

https://arxiv.org/pdf/1307.7391 ccz4 paper, contains some good equation references
https://iopscience.iop.org/article/10.1088/1361-6382/ac7e16/pdf covariant bssn
https://www2.yukawa.kyoto-u.ac.jp/~masaru.shibata/PRD52.5428.pdf cG mentions the motivation behind the gamma term
stability paper https://arxiv.org/pdf/0707.0339
http://wwwteor.mi.infn.it/~molinari/TESI/Danieli_tesi.pdf 4.27 relates conjugate momentum and extrinsic curvature


Ok planola time:

1. Basic introduction to the ADM formalism. Need to cover projection, the concept of time + gauge conditions, physical interpretation of the gauge conditions as an eularian observer, and the concept of a hypersurface
2. Introduce the adm equations
3. Introduce the vanilla BSSN equations
4. Talk through all the notation
5. Swap to structuring a simulation
6. Talk through the high level flow
7. Initial conditions: waves
8. Boundary conditions: periodic
9. Evolution equations: Bssn, momentum constraint damping, algebraic constraints, kreiss-oliger dissipation
10. Gauge condition evolution
11. Some kind of diagnostic (semi analytic tests?)

Hi there! Today we're going to be getting into numerical relativity, which is the art of simulating the evolution of spacetime itself. While there a quite a few known solutions to the equations of general relativity at this point (eg Schwarzschild), many of the most interesting regions of general relativity can only be explored by exploring how spacetime itself evolves dynamically. Binary black hole collisions are one of the most directly relevant applications of this

These kinds of simulations are traditionally run on supercomputers, but luckily you likely have a supercomputer in the form of a GPU[^specs] in your PC. The amount of extra performance we can get over a traditional CPU based implementation is often a few orders of magnitude better than normal

[^specs]: You'll need 8+ GB of vram. This article was written and implemented on a 6700xt, but previous work was on an r9 390

This article is not going to assume you know all that much about general relativity, but it does require a lot of implementing

# Time in general relativity

The key issue for simulating general relativity is that of time, and how we handle this is going to crop up all over the place. The einstein field equations simply dictate how 4 dimensional spacetime should look declaratively - but they do not have any concept of forwards in time. Unlike in virtually every other theory where time is a background parameter, in general relativity there literally isn't any independent notion of time[^timeorientable]

[^timeorientable]: While we're looking at relatively well behaved spacetimes, in the general case relativistic manifolds are not necessarily time orientable, or simply connected - like a wormhole. These are both off the table for us sadly

The puzzle is how to extract some concept of forwards in time from the equations of general relativity, so we can construct a system of partial differential equations, and evolve them forwards to hit an end state

Here we'll be looking at the ADM (Arnowitt–Deser–Misner) formalism. It splits up a 4d spacetime into a 3+1 split: three spatial dimensions, and it singles out a single time dimension. Do note that this isn't the only way of doing it, and there are a number of successful formalisms, but ADM is the one with the most available information (and the most understandable in my opinion)

## 3+1 split

The idea is that we take a series of 3d[^nd] spatial slices through our 4d spacetime - each through a 'moment' in time. The time coordinate is arbitrary (and we'll get back to it), but this spatial slicing is fairly arbitrary. From now on, all latin indices $_{ijk}$ have the value 1-3, and refer to quantities on our 3d slice, whereas greek indices $_{\mu\nu\kappa}$ take on the value 0-3 and refer to quantities in our 4d spacetime

[^nd]: Everything here is also applicable to N dimensional spacetimes, but I know very little about anything other than 4d general relativity

The way we chop up our spacetime into 3d slices is known as the foliation, and with this we'll introduce our first piece of ADM specific notation: $\alpha$, the lapse, and $\beta^i$, the shift, a 3 component vector. These are also often sometimes labelled $N$ and $N^i$ respectively. Together, these point forwards in time, and make up a 'normal' vector to the 3d surface we're working with

These variables are called the gauge variables, which dictates the foliation of spacetime. They are in general arbitrary, and represent an extra degree of freedom. The general line element in ADM is this:

$$ds^2 = -\alpha^2 dt^2 + \gamma_{ij} (dx^i + \beta^i dt)(dx^j + \beta^j)$$

$\gamma_{ij}$ is the 3-metric (often called the induced metric) which is used on any quantity in our 3d slice, and operates like a regular metric tensor. That is to say, if we have some quantity $Q_i$ on our 3d slice, we can raise and lower its indices with $\gamma_{ij}$ as follows

$$Q^i = \gamma^{ij} Q_j\\
Q_i = \gamma_{ij} Q^j$$

If our 4-metric is $g_{\mu\nu}$, then our 3-metric is $\gamma_{ij} = g_{ij}$, ie the spatial parts. While $\gamma^{ij} = (\gamma_{ij})^{-1}$, $\gamma^{ij} \neq g^{ij}$

Lets examine some definitions that fall out of this information dump

## Definitions

The 3-metric, aka the induced metric

$$\gamma_{ij} = g_{ij}$$

We can recover the 4-metric as such:

https://indico.cern.ch/event/505595/contributions/1183661/attachments/1332828/2003830/sperhake.pdf

Todo: ate without table

$$
\begin{align}
g_{00} &= -\alpha + \beta_m \beta^m \\
g_{i0} &= \beta_i\\
g_{0j} &= \beta_j\\
g_{ij} &= \gamma_{ij}\\
\end{align}
$$

$$
\begin{align}
g^{00} &= -\alpha^2 \\
g^{i0} &= \alpha^{-2} \beta^i\\
g^{0j} &= \alpha^{-2} \beta^j\\
g^{ij} &= \gamma^{ij} - \alpha^{-2} \beta^i \beta^j\\
\end{align}
$$

$\Sigma_t$ = spatial hypersurface

Timelike normal:

$$
\begin{align}
n^\mu &= (\frac{1}{\alpha}, -\frac{\beta^i}{\alpha})\\
n_\mu &= (-\alpha, 0, 0, 0)
\end{align}
$$

Our partial derivative operator in time:

$$(\partial_t)^\mu = \alpha n^\mu + \beta^\mu$$

ADM projector, used for taking a 4-quantity and finding the associated projected quantity on the 3-surface ($\delta$ is the kronecker delta):

$$\bot^\mu_{\;\;\nu} = \delta^\mu_{\;\;\nu} + n^\mu n_{\nu}$$

Also sometimes called $\gamma^\mu_{\;\;\nu}$

## Equations of motion

Once we have this foliation, it is possible to define a lagrangian[^mechanics] for the evolution from one 3d slice to the next. A lagrangian directly leads to a hamiltonian formalism, and equations of motion, and you can see these equations over here:

[^mechanics]: If you don't know lagrangian or hamiltonian mechanics, its not especially important. The only thing to really be aware of is that it introduces some terms known as constraints: The hamiltonian constraint $H=0$, and the momentum constraint $M_i = 0$. These must both be satisfied as our simulation progresses

https://en.wikipedia.org/wiki/ADM_formalism#Lagrangian_formulation

We end up with two differential equations - one for our metric $\gamma_{ij}$, and one for our 'conjugate momentum', $\pi^{ij}$. On top of this, we must also supply evolution equations for the lapse and shift - these are both arbitrary (in theory), and any choice within reason will lead to a valid evolution - we could pick $1$ and $(0,0,0)$ respectively (and this slicing is sometimes used)

Unfortunately, there's a big problem

# The ADM equations of motion are bad

Part of the reason why we're not going into this more is that - unfortunately - these equations of motion are fairly useless to us. They are *very* numerically unstable, to the point where for our purposes there's no point trying to solve them. Understanding this issue, and solving it was an outstanding problem for decades, and there is an absolutely enormous amount written in the literature on this topic. A lot of this article series is going to be dealing with the very poor character of the equations

Luckily, because its not 1990 - and because the astrophysics community rocks and makes all this information public - we get to skip to the part where we have a workable set of equations

## Enter: The BSSN formalism

The BSSN formalism is the most widely used formalism - its an ADM derivative, and most practical formalisms are substantially similar to the BSSN formalism. It was the first very successful formalism to enter the picture - though it was unclear precisely why it worked so well for a long time[^seehere]

[^seehere]: [https://arxiv.org/pdf/0707.0339](https://arxiv.org/pdf/0707.0339) this paper contains a lot of useful details, but the short version is that the BSSN equations are a partially constrained evolution system, and managing the constraints is critical to a correct evolution scheme

There isn't one canonical BSSN formalism however, so its time to run through our options

## Conformal decomposition

The two ADM variables are: $\gamma_{ij}$ the metric tensor, and $\pi_{ij}$ the generalised conjugate momentum. The BSSN formalism swaps $\pi_{ij}$ for the related variable $K_{ij}$[^reffy], the extrinsic curvature tensor, as its root variable for ADM

[^reffy]: [Reference](http://wwwteor.mi.infn.it/~molinari/TESI/Danieli_tesi.pdf) 4.27

One of the insights of the BSSN formalism is that we can perform a 'conformal decomposition' of our variables $\gamma_{ij}$ and $K_{ij}$, which renders the equations more numerically stable. The conformal metric tensor is called $\tilde{\gamma}_{ij}$, and is related to the regular metric tensor by a conformal transform - We'll get back to $K_{ij}$. There are three conformal transforms I know of which are in use:

$$
\begin{align}
\tilde{\gamma}_{ij} &= e^{-4\phi} \gamma_{ij}\\
\tilde{\gamma}_{ij} &= \chi \gamma_{ij}\\
\tilde{\gamma}_{ij} &= W^2 \gamma_{ij}\\
\end{align}
$$

All defining the same conformal metric. The definition of the conformal variables in these schemes is as follows:

$$
\begin{align}
\phi &= TODO\\
\chi &= det(\gamma)^{-1/3}\\
W &= det(\gamma)^{-1/6}\\
\end{align}
$$

With this definition, the determinant of our conformal metric tensor is $\tilde{\gamma} = 1$. The inverse of the metric tensors follow a similar relation:

$$
\begin{align}
e^{-4\phi} &\tilde{\gamma}^{ij} &= \gamma^{ij}\\
\chi &\tilde{\gamma}^{ij} &= \gamma^{ij}\\
W^2 &\tilde{\gamma}^{ij} &= \gamma^{ij}\\
\end{align}
$$

The $\phi$ conformal decomposition was the first to be used, but it tends to infinity at the singularity which makes it not ideal - and appears to be falling out of use. The second version to be used was $\chi$ (range, [0,1]) which tends to 0 at the singularity, and is relatively widespread in more modern code. Lastly $W^2$ entered the picture - ostensibly it has better convergence[^converge] than $\chi$, but the really neat part about it is that renders the equations of motion of BSSN in a much less singular form. It also does not suffer from some of the issues that X has, like unphysical negative values

[^converge]: See [High-spin binary black hole mergers](https://arxiv.org/pdf/0709.2160) for this line of reasoning

Because of this, the $W^2$ formalism is clearly the best choice for us. Do note that some papers interchange notation here unfortunately, and eg $\phi$ and $\chi$ are sometimes used to mean *any* conformal factor

## Notation

$\tilde{D}_i$ = The covariant derivative, calculated in the [usual fashion](https://en.wikipedia.org/wiki/Covariant_derivative#Covariant_derivative_by_field_type) from the conformal metric tensor $\tilde{\gamma}_{ij}$. $D_i$ is the covariant derivative associated with $\gamma_{ij}$. Tilde's are very load bearing in the BSSN formalism

$\tilde{\Gamma}^i_{jk}$ are the conformal christoffel symbols of the second kind, calculated in the usual fashion from $\tilde{\gamma_{ij}}$. $\Gamma^i_{jk}$ are the christoffel symbols associated with $\gamma_{ij}$

$\tilde{D}^i$, is used to mean $\tilde{\gamma}^{ij} \tilde{D}_j$. Similarly, $D^i$, is used to mean $\gamma^{ij} D_j$, which is equivalent to $W^2\tilde{\gamma}^{ij} D_j$

While non specific to BSSN, the symmetrising operator $_{()}$ crops up here unintuitively. This is defined as $A_{(ij)}$ = $0.5 (A_{ij} + A_{ji})$. A more complex example is $2\tilde{\Gamma}^k_{m(i} \tilde{\Gamma}_{j)kn} = 0.5 * 2 (\tilde{\Gamma}^k_{mi} \tilde{\Gamma}_{jkn} + \tilde{\Gamma}^k_{mj} \tilde{\Gamma}_{ikn})$


## Variables

The BSSN variables are defined as a conformal decomposition of the ADM variables, $\gamma_{ij}$ and $K_{ij}$, via a conformal factor (here $W^2$). Our full set of variables is defined as follows:

$$
\begin{align}
&\tilde{\gamma}_{ij} &&= W^2 \gamma_{ij}\\
&\tilde{A}_{ij} &&= W^2 (K_{ij} - \frac{1}{3} \gamma_{ij} K)\\
&K &&= \gamma^{mn} K_{mn}\\
&W^2 &&= det(\gamma)^{-1/6}\\
&\tilde{\Gamma}^i &&= \tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}\\
&\alpha_{bssn} &&= \alpha_{adm}\\
&\beta^i_{bssn} &&= \beta^i_{adm}\\
\end{align}
$$

Conformal variables in general have a tilde over them, and have their indices are raised and lowered with the conformal metric tensor. In this definition, the trace of the conformal metric is $\tilde{\gamma} = 1$, and $\tilde{A}_{ij}$ is trace-free. $K$ is the trace of $K_{ij}$

$\tilde{\Gamma}^i$ is analytically equal to $\tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}$, but is evolved numerically instead. This is because $\partial_k \tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn}$ is numerically unstable TODO: CITE

In total, we have ended up with the following constraints, that ideally are satisfied for all time, but in practice will be numerically violated

The momentum constraint:

$$M_i = \tilde{\gamma}^{mn} \tilde{D}_m \tilde{A}_{ni} - \frac{2}{3} \partial_i K - \frac{3}{2} \tilde{A}^m_{\;\;i} \frac{\partial_m \chi}{\chi} = 0$$

The hamiltonian constraint:

$$H = R + \frac{2}{3} K^2 - \tilde{A}^{mn} \tilde{A}_{mn} = 0$$

The algebraic constraints:

$$\begin{align}
\mathcal{S} &= \tilde{\gamma} &= 1\\
\mathcal{A} &= \tilde{\gamma}^{mn} \tilde{A}_{mn} &= 0
\end{align}
$$

And the christoffel symbol constraint:

$$
\mathcal{G}_i = \tilde{\Gamma}^i - \tilde{\gamma}^{mn} \tilde{\Gamma}^i_{mn} = 0
$$

Constraint damping is critical to the BSSN formalism, so we'll get back to these

## Equations of motion

This is the part of the article I've been looking forward to least

Todo: fun

## Gauge conditions

Different evolution equations for the gauge conditions are called for in different situations. Here are the common gauge conditions we'll run into in short order:

### Lapse

#### Geodesic slicing

$$\alpha = 1, \beta^i = 0$$

#### Harmonic

$$\partial_t \alpha = - 2 \alpha^2 K + \beta^m \partial_m \alpha$$

#### 1+log

$$\partial_t \alpha = - \alpha K + \beta^m \partial_m \alpha$$

### Shift

#### Zero shift

$$\beta^i = 0$$

#### Gamma driver (sometimes called Gamma freezing)

So called because it drives $\partial_t\tilde{\Gamma} = 0$

$$\partial_t \beta^i = \frac{3}{4} \tilde{\Gamma}^i + \beta^j \partial_j \beta^i - n \beta^i$$

$n$ is a damping parameter, that is generally set to $2$

### Gauge condition summary

In the test we will be looking at today, we'll be using the harmonic gauge condition, with zero shift. The binary black hole gauge is 1+log + Gamma driver, and will be used virtually exclusively in future articles

## The BSSN equations of motion are bad

So, as written, the BSSN equations of motion are - drumroll - very numerically unstable. I'll summarise two decades of the conventional wisdom from stability analysis here[^stabilityanalysis]:

[^stabilityanalysis]: This footnote could (and likely will) be an entire article in the future, where we'll test out dozens of modifications. Until then, consider checking out [this](https://arxiv.org/pdf/gr-qc/0204002), [this](https://arxiv.org/pdf/0707.0339), and [the ccz4](https://arxiv.org/pdf/1106.2254) paper (a similar, constraint damped formalism) for more information. There's a lot of decent analysis spread over the literature

1. If the second algebraic constraint is violated, ie the trace of the trace-free extrinsic curvature != 0, the violation blows up and your simulation will immediately fail
2. The christoffel symbol constraint is very important to damp, and can lead to less direct simulation failures
3. Correcting momentum constraint violations is very important
4. The quantity $\partial_k\tilde{\Gamma}^i_{mn}$ is numerically unstable, though the quantity $\tilde{\Gamma}^i_{mn}$ is not
5. The BSSN formalism inherently generates constraint violations under certain conditions, leading to self-blowup

Additionally:

1. Hamiltonian constraint violations are completely unimportant for stability purposes, and seem to be largely irrelevant for physical accuracy beyond a reasonably justified paranoia
2. The first algebraic constraint is relatively unimportant to enforce, but its easy to do - so we may as well

### Stabilising the BSSN equations of motion

This is an enormous topic in itself, and we're going to get into this a lot. As-is, I'm going to present the bare minimum we need to get up off the ground for this article, because in the future we're going to be directly testing all of this. The goal is to make as general of a simulation as possible, so we're going to avoid some of the most ad-hoc tricks - its pretty common to tailor stability modifications to certain scenarios, or dynamically adjust your constants

The following constraints *must* be enforced:

1. Aij Algebraic blah blah. Either do the det, or damp with the TF

And we may need to damp the following two constraints as follows

1. Momentum blah blah
2. Gamma blah blah

On top of this to stabilise our equations more generally, we must:

1. Add kreiss oliger dissipation. There will be a segment on this, so don't worry yet

And we'll do this modification because we may as well increase our correctness

1. Gamma blah blah. Either do the det, or damp

This is enough to get us up and running off the ground. When we hit binary black holes we'll need to update this list with an extra stability modification

Next up, we need to pick an integrator to integrate our PDEs with

## Picking an integrator

We're going to diverge from the literature significantly here, for very good reason. The BSSN equations are a classically stiff (unstable) set of equations, and they oscillate pretty wildly. Its traditional to use something similar to RK4 to integrate the equations of motion

If you're familiar with solving stiff PDEs, it might seem a bit odd to use RK4 - RK4 is an explicit integrator - and as such has fairly poor stability properties when solving stiff equations. For many kinds of problem, an explicit integrator works great, but not for unstable problems like this. Instead, we're going to use an L-stable implicit integrator which has much more appropriate convergence properties - backwards euler

As far as I'm aware, there's been no testing of fully implicit integration in numerical relativity - the closest is IMEX stepping from the SXS group, but it does not implicitly solve derivatives due to performance constraints

### Solving implicit equations efficiently

Forwards euler has the form:

$$y_{n+1} = y_n + dt f(t_n, y_n)$$

This is the classic 1st order, explicit integrator. It has poor stability properties, and does not have a high order of convergence

Backwards euler has the form:

$$y_{n+1} = y_n + dt f(t_{n+1}, y_{n+1})$$

This is also a 1st order integrator, but it is implicit - we can see that we have $y_{n+1}$ on both sides of the equation. The real trick is that backwards euler is L-stable - which means that it has extremely good stability properties for solving unstable equations, which is exactly what we're looking for. On top of this, backwards euler has a very GPU-friendly form, making it very amenable to GPU architecture, while using a minimal amount of memory. In all my testing, I haven't found an integrator with better properties than this

One notable feature is that with an integrator like this, we need less damping and stability modifications to make the equations work well. The downside is that we're solving our equations in a fundamentally different way to what is standard, so we now have to extensively test things to figure out the character of our solution ourselves

### Fixed point iteration

The issue with an implicit integrator is: How do we actually solve this equation? The straightforward answer is to use fixed point iteration

Todo: Fixed point

This also maps very well to gpu architecture, is numerically stable, and in general is extremely fast and robust. There are tricks to speed up fixed point iteration as well, which we'll get into in the future

## Flow of a simulation

There are three components to any good simulation here

1. Initial conditions
2. Boundary conditions
3. Integrating our PDEs

### Initial conditions

We need to define on our initial slice what our variables are. Here, we're simulating an infinite field of gravitational waves, and our initial variables take the following form:

### Boundary conditions

We're going to be using periodic boundary conditions - at the edge of our simulation, we simply wrap our blhabbfkdfblkjkb

### Integrating the PDEs

Each timestep is broken up into a fixed number of substeps, as we solve the implicit equation for backwards euler. In each substep, we:

1. Calculate our derivatives (todo: Think i need to segment this off more)
2. Calculate the momentum constraint, if we're using momentum constraint damping
3. Evolve our equations to calculate yn + f(yn+1)

After each substep - once we have our final answer, we apply kreiss-oliger dissipation to the result, to remove high frequencies from our grid

#### Kreiss-Oliger

drerrdrfsfdd

## Implementation

We now have all of the theory out of the way. Lets look at the code for our initial conditions first:

### Initial conditions

dfdfdf

### Boundary conditions

The boundary conditions are implemented whenever we need to take a derivative of something, which looks like this

### Evolution equations

This is the real kicker

#### Calculating derivatives

We only need to calculate 1st derivatives and save them when we take the second derivative of something

#### Evolution equations by Field

##### cY

##### cA

##### K

##### W

##### gA

##### G

##### Gauge conditions

Geodesic slicing, a = 1, b = 0
harmonic slicing https://arxiv.org/pdf/2201.08857 a = -a^2 K

1+log = -2 a K
Gamma = blahblah

Note that structurally for backwards euler we need a buffer for our input (yn+1), one for our output (yn + f(yn+1)), and one to hold the last value (yn)

### Kreiss-Oliger

Finally afterwards we numerically dissipate away the noise via kreiss-oliger

## Results