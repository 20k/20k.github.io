---
layout: post
title:  "Numerical Relativity 102: Simulating binary black hole collisions, quickly"
date:   2024-08-15 12:33:23 +0000
categories: C++
---

Hello! Today we're going to do one of the coolest things in all of physics in my opinion, which is simulating the collision of two black holes. Last time round, we implemented most of what we'll need to simulate this, so today's job is to capitalise on that and finally smash some black holes together

The complicated part of this article is going to be the initial conditions, which means that most of this article is actually going to be how to solve Laplacians quickly on a GPU

# Astrophysics Context

Todo: Go into black holes a bit more, spacetime soliton vs a mass driven affair

Its worth understanding for this article what we're actually trying to simulate. A black hole is an object that is so dense that light can't escape - once this happens, nothing can prevent all the matter that made up the precursor object from reaching the singularity

Eg, consider a star collapsing into a black hole. All the matter shrinks to a point (or, in the case of a spinning black hole: a ring) past the event horizon, until it reaches a singularity. In pure general relativity, this singularity is (essentially^[essentially]) inevitable, and all the black holes we're dealing with today will always contain a singularity. Its also true that the singularity is - mathematically - very poorly behaved and by definition causes all the maths to explode. This makes simulating it a bit tricky. There are two approaches to handling the singularity

[^essentially]: There's some debate around if you could construct an astrophysical black hole which has no singularity, likely containing negative energy, but this isn't relevant for us at all

## Managing the singularity

### Excision

This technique is fairly common, and involves chopping out the singularity entirely, and simply not simulating it. To excise the singularity correctly, you must define a boundary condition inside the black hole itself, then use one sided derivatives on the boundary to correctly simulate it. This technique works well, but involves the complexity of calculating and implementing the boundary condition

### Moving punctures

This is the technique we'll actually be using, and involves a bit of trickery. In the ADM formalism, the gauge conditions define the coordinate system of what you're working with. So the trick is to make sure that the coordinates never actually meet the singularity, and it is treated as a puncture/hole in your simulation grid. This puncture is able to move around with the correct gauge conditions, and so we have a singularity that is never actually quite represented. In moving punctures, the entire simulation domain is actively simulated, unlike with excision where a portion of it is not

In practice, its a lot trickier than this, and it involves a lot of subtleties to get this to work in a physically accurate way. I'm going to keep the crackpot conspiracy theories to the end of this article however

Notably, as the singularity represents a true discontinuity, differentiating across it is technically incorrect

Both of these techniques rely on the fact that the interior, and exterior of a black hole are casually disconnected, and so what the interior of a black hole in theory[^intheory] does not affect the exterior universe

[^intheory]: In practice, this is a hugely complicated topic that is going to make our lives very difficult. Its not that GR is incorrect about this, its that getting this guarantee in a numerical simulation is non trivial

## Binary black hole orbits

The conventional wisdom is that black holes merge after a long inspiralling process, where the nature of gravitational wave emissions slowly circularises the orbits. This means that binary black hole orbits should be nearly perfectly circular by the time the black hole's merge

Unfortunately, this turned out not to be the case, and it appears that there is a significant amount of eccentricity (non circularity) when black holes merge. This really complicates the amount of simulating you have to do

## Unequal mass binaries

With the assumption of circular orbits, equal mass/spin binaries, and the black hole spin being aligned with the plane of the orbit, you can use octant symmetry - ie your simulation is fully symmetric and can be chopped into 8 chunks, with only one of the chunks actually being simulated. If your black holes are of unequal mass but with axis aligned spins, you can use axisymmetry, for a 2x speedup

Personally I'm interested in a very general simulator which can handle anything, so we're going to avoid any symmetry reduction - but given its prevalence in the literature, its worth mentioning here. For equal mass binaries, this would be a flat 8x speedup, so its definitely worth investigating for lower end hardware

## Gravitational waves

The primary purpose of a black hole merger simulator is to find out what the gravitational waves look like, because LIGO (a big gravitational wave detector) can detect certain mass ranges of mergers. We're not going to be pulling out gravitational waves today, but it does mean we need to be careful to be as physical as possible

To perform this matching, as far as I know there's no better way than simply bruteforcing the entire parameter space. For a pair of black holes, you have the following parameters:

todo: no

M1: The mass of the first black hole
M2: The mass of the second black hole
S1: The spin direction - a 3-vector - of the first black hole, normalised
S2: The spin direction of the second black hole
X1: The spin constant of the first black hole
X2: The spin constant of the second black hole
e: The orbital eccentricity

This is a lot of parameters to bruteforce, and its why non circular orbits are such a gigantic pain, because it adds a whole extra dimension to bruteforce

Traditional simulations in this area can take weeks to months, which you'll be glad to know is not going to be the case for us. It'll be about 5-10 minutes for an orbit tops

# What's the plan?

We need a few things to get this to work:

1. A new set of initial conditions
2. New boundary conditions
3. Some modifications to the evolution equations

Of all of these, the initial conditions are by far the most complicated part, so lets get to it

# Initial conditions

I'm going to present you with the easiest set of initial conditions to implement, and we're going to really dig into it. Some discussion about alternatives and further context is given at the end of this article

The paper we're going to implement today is called [A simple construction of initial data for multiple black holes](https://arxiv.org/pdf/gr-qc/9703066). This represents an extremely traditional and widespread way to construct black hole initial conditions - its also a very good paper that you should read. There are a few things that we need to learn right off the bat

1. This doesn't actually make black holes, it makes objects that collapse into black holes. They have no event horizon, or apparent horizon
2. There is no way of a-priori specifying the mass of a black hole
3. There is a maximum value of spin that can be expressed with this technique

Lets get into it

## Implementing

To take a step back, we're trying to find initial values for the following ADM variables:

$$\gamma_{ij}\\
K_{ij}\\
\alpha\\
\beta^i
$$

This technique solves for $\gamma_{ij}$ and $K_{ij}$, and leaves us to guess $\alpha$ and $\beta^i$ (which are, in theory, arbitrary). It falls under the class of initial conditions known as 'conformally flat'. This means it decomposes the metric tensor into a flat conformal metric (an identity matrix), and a conformal factor $\psi$. It also decomposes the extrinsic curvature $K^{ph}_{ij}$ ('physical', this is our $K_{ij}$) into a trace $K=0$, and a trace free part which this paper calls $K_{ij}$ (and other papers often call $$\bar{A}_{ij}$$). This is called a maximal slice (todo: check the _{TT} schenanigans)

The basic idea here is that with the conformally flat + trace free constraint, the momentum constraint $\mathcal{M}_i$ becomes linear, allowing you to calculate the extrinsic curvature $$\bar{A}_{ij}$$ as a sum of each individual black hole's curvature, to calculate the final (conformal) curvature. The shape of the conformal factor $\psi$ is a *guess*, which is then corrected to be physical via a small correction $u$, by solving the laplacian (9) + (10)

## Details

This method has the following parameters:

$m_i$ bare mass parameter of each black hole
$S_i$: A 3-vector representing the spin (angular ADM momentum) of each black hole
$P_i$: A 3-vector representing the linear momentum (ADM mass * velocity) of each black hole
$x_i$ the position of a black hole

I'm labelling the properties of each black hole by its index $i$, eg $P_0$ is the first black hole's momentum vector

Note that bare mass is not the same thing as mass (here: ADM mass), it only correlates to it. A black hole's spin contributes to its mass as well. On top of this, in the general case, the mass of a black hole in a binary pair is of questionable physicalness: see the end of this article for more details. Here, you don't need to worry about this, because we'll be giving the initial conditions in terms of bare mass: just be aware this isn't universal in the literature

I'm going to be using the notation $\bar{A}^{ij}$ to mean what this paper calls $K^{ij}$, because its simply too confusing otherwise. I'm also going to be using $\bar{\gamma}_{ij}$ to mean the conformally flat metric instead of $g_{ab}$, for the same reason. Note that $\bar{\gamma}_{ij}$ and $\tilde{\gamma}_{ij}$ are not the same, there's a lot of quite similar notation here

## Conformal extrinsic curvature

This is straightforward. For every black hole in our spacetime, we calculate the quantity (5), and then sum all of them. After accumulating all of these, I store $\bar{A}^{ab} \bar{A}_{ab}$ as a scalar

For a single black hole, we have this:

$$
\begin{align}
\bar{A}^{ij} &= \frac{3}{2r^2}(P^a n^b + P^b n^a - (\bar{\gamma}^{ab} - n^a n^b) P^c n_c) \\
&+ \frac{3}{r^3}(\epsilon^{acd} S_c n_d n^b + \epsilon^{bcd} S_c n_d n^a)
\end{align}
$$

Where $\epsilon$ is the levi civita symbol[^lcs]. Note that the levi civita symbol, and levi civita tensor have the same notation in the literature and are frequently freely mixed, so its not especially clear what's going on. TODO: I NEED TO CHECK THIS

[^lcs]: [see 13](https://arxiv.org/pdf/2007.14279)

$r$ is the distance in world coordinates (ie not grid coordinates) of the coordinate from the black hole in question. There's a clear problem when $r = 0$, and the solution is to either position your black holes so this is impossible, or clamp to a small value

$n$ is a normal vector, which is calculated as follows: $n^i = x^i/r$, and points away from the black hole in question

If you have no plans to ever work in non cartesian coordinate systems (which we don't), the metric tensor is the identity matrix, and you can ignore the index positions

Next up: We need the conformal factor $\psi$, which involves calculating the correction $u$

## Conformal Factor

alt https://arxiv.org/pdf/1606.04881

https://www.worldscientific.com/doi/pdf/10.1142/S2010194512004321 15? everyone uses this

The conformal factor $\psi$ here is listed as:

$$\psi = \frac{1}{\alpha} + u$$

Where $u > 1$, and $\alpha$ is:

$$\frac{1}{\alpha} = \sum^N_{i=1} \frac{m_i}{2 |\overrightarrow{r} - \overrightarrow{x}_{(i)}|}$$

Note that $r$ in the bottom term is a vector representing our current grid cell's world position. This is twice the distance of our coordinate from our $i$'th black hole

We're going to deviate a little from this definition, and instead calculate the correction as:

$$\psi = \frac{1}{\alpha} + u + 1$$

Where $u > 0$. This is because you have a lot more precision to work with for floating point near $0$ vs near $1$, so this lets us solve for much smaller corrections without needing to resort to double precision. This means our boundary condition will be slightly different ($u=0$ vs $u=1$)

### Solving for $u$

Equations (9) + (10) are a classic laplacian:

todo: check if i need to u+1, i don't think so

$$\begin{align}
&\Delta u + \beta(1 + \alpha u)^{-7} = 0\\
&\beta = \frac{1}{8} \alpha^7 \bar{A}^{ab}\bar{A}_{ab}
\end{align}
$$

If you're planning to implement in a more serious capacity, I'd recommend instead the form given by [this](https://arxiv.org/pdf/1606.04881) paper (18), because that's where we'll end up eventually

#### Solving laplacians in general

Our equations are of the form:

$$\Delta u = F(u)$$

And are a standard laplacian. It is common in numerical relativity to use fairly complicated solutions to solve this, and they come with caveats in terms of what kinds of laplacians are solvable. This is because solvers are CPU based, and as such need very high order convergence to work quickly

Luckily, we're in GPU-land, and so the techniques are both simpler and significantly more performant. If you already know how to solve a laplacian, there is nothing special in this section and you may skip it

#### The boundary condition

At the edge of our grid, we need to set $u$ to something. Asymptotically, $u$ is assumed to have the form $O(\frac{1}{r})$ (todo check), and at infinity in the original definition $u ~= 1$. This means for us, $u = 0$

#### Fixed point iteration / 7 point stencil

A laplacian in flat 3d space is defined as such:

$$\Delta u = \frac{\partial^2 u}{\partial x_0 ^2} + \frac{\partial^2 u}{\partial x_1 ^2} + \frac{\partial^2 u}{\partial x_2 ^2}$$

Ie, it is the sum of second derivatives in each direction. Given a function $f(x)$, we can approximate the second derivative with finite difference coefficients:

$$\frac{\partial^2 f(x)}{\partial x^2} = \frac{f(x - h) - 2 f(x) + f(x + h)}{h^2}$$

Now, we'll treat $u$ as a function over 3d space, ie $u(x, y, z)$, and apply the above discretisation to $\Delta u$:

$$\Delta u = \frac{u(x - h, y, z) - 2 u(x, y, z) + u(x + h, y, z))}{h^2} + \frac{u(x, y - h, z) - 2 u(x, y, z) + u(x, y + h, z))}{h^2}\frac{u(x, y, z - h) - 2 u(x, y, z) + u(x, y, z + h))}{h^2}$$

Collecting all the terms, we get:

$$\frac{u(x - h, y, z) + u(x + h, y, z) + u(x, y - h, z) + u(x, y + h, z) + u(x, y, z - h) + u(x, y, z + h) - 6 u(x, y, z)}{h^2}$$

Our initial equation is $\Delta u = F(u)$, so lets substitute in, and multiply by $h^2$:

$$u(x - h, y, z) + u(x + h, y, z) + u(x, y - h, z) + u(x, y + h, z) + u(x, y, z - h) + u(x, y, z + h) - 6 u(x, y, z) = F(u) h^2$$

The iterative scheme for solving a laplacian (often called 5-point stencil in 2d, or the 7-point stencil in 3d), is given by solving for $u(x, y, z)$:

$$
u(x, y, z) = \frac{u(x - h, y, z) + u(x + h, y, z) + u(x, y - h, z) + u(x, y + h, z) + u(x, y, z - h) + u(x, y, z + h) - F(u) h^2}{6}
$$

For us, $F(u) = -\frac{1}{8} \alpha^7 \bar{A}^{ab} \bar{A}_{ab} (1 + \alpha u)^{-7}$

As a basic idea, this works, and is implementable. This is the form I used for a very long time: you just iterate the above equation until the change in $u$ is sufficiently small. There are two main issues:

1. It can be numerically unstable, particularly for spinning black holes
2. It isn't amazingly fast, it can take ~10s or so in some cases, which is still pretty fast

The first is solved by relaxation: Ie taking our old guess, and our new guess, and taking some fraction between the two: `mix(u, nextu, 0.9)`. This unfortunately reduces the speed of convergence

Luckily, performance issues are easily fixable!

#### Multigrid

The easiest way to speed up performance is with a multigrid solution. Essentially, instead of solving at your final grid resolution, you start off at a low resolution, solve for that, then progressively solve for higher resolutions starting with your low resolution guess upscaled to a higher resolution

This solution works incredibly well for this problem, as there is no high frequency content in $u$, and is a free performance win

#### Red-black

Another technique that gives a free 2x performance increase is red-black iteration. Imagine the laplacian problem in 2d:

todo: picture

Each cell only accesses a adjacent grid cells:

Todo: red/black

This means that when updating, we can skip half the cells in the 'wrong' colour, and then update them next time round instead. Note that we *do* use $u$ here to calculate $F(u)$, which means that the $u$ for the laplacian's and the $u$ for our rhs are at different steps if we use red-black iteration. This does not affect convergence

The iteration scheme to use in 3d is as follows:

Todo: code

## Finishing up

After solving the laplacian, we have a value for $u$, which allows us to calculate $\psi$ directly. From here, we can calculate our ADM variables:

$$\begin{align}
\gamma_{ij} &= \psi^4 \bar{\gamma}_{ij}\\
K_{ij} &= \psi^{-2} \bar{A}_{ij}
\end{align}$$

Where in cartesian coordinates, $$\bar{\gamma}_{ij}$$ is the identity matrix. We already know how to construct our BSSN variables, see the last article

The ADM mass can be estimated via todo. If you need a specific ADM mass, iterate. Binary search. Spin! Woo!

### Gauge

We still don't have an initial solution for our gauge variables however. Universally, $\beta^i = 0$. The lapse $\alpha$ is a tad more interesting - there are two[^twomain] main options:

1. $\alpha = 1$
2. $\alpha = \psi^{-2}$

Both work well and are widely used. The latter is better for black holes, but I've found that it tends to lead to neutron stars exploding spectacularly. We're going to use #2 for this article, as we're strictly dealing with black holes

[^twomain]: In general, these two options are the two simple options that work well, but there are a variety of alternatives

# Boundary conditions

https://arxiv.org/pdf/1503.03436 2.39

Our data is no longer periodic, so we need a new set of boundary conditions. The simplest good-enough boundary condition is called the Sommerfeld boundary condition. This radiates away your data to the asymptotic value at the boundary. This is defined as:

$$\frac{\partial f}{\partial t} = -\frac{v x_i}{\partial x_i} - v \frac{f - f_0}{r}$$

$f$ is our value for whatever field we're radiating, $x_i$ is the cell's position in world coordinates, $v$ is the wave speed, $f_0$ is the asymptotic value, and $r$ is the distance from the centre. This is applied to every tensor component separately, as if they were independent fields

The derivatives at the boundary will be one sided derivatives. All waves have a speed of $v=1$, except for the gauge variables which have a speed of $v=\sqrt{2}$. Todo: Cite

Note that in my experience, Sommerfeld boundary conditions tend to lead to a gradual increase in ADM mass over time, making it somewhat unsuitable for long lived sims

Kreiss-Oliger dissipation also requires a boundary condition - in our case, we'll progressively reduce the order of dissipation as we approach the boundary, as we cannot

## Other boundary conditions

There are alternatives to sommerfeld which I will mention in passing

1. Fixing every field to its asymptotic value at the boundary, and not evolving it. This works if your boundary is far away (as spacetime is asmptotically flat), but requires a lot of extra simulation space
2. Sponge constructions. This damps outgoing waves gradually. Its easy to implement, but the sponge area must be large enough to avoid reflections, which also requires a lot of extra simulation space
3. Constraint preserving boundary conditions. These are the best and allow you to place the boundaries closest to your simulation area (as they introduce the least error), but are more complex to implement. Its on my infinitely long todo list, and there'll likely be an article on this in the future
4. Hybrid schemes. Some schemes assume that $\beta^i = 0$ on the boundary, and evolve $\tilde{\gamma}_{ij}$ freely, then use traditional boundary conditions for the other variables. I've never tested this and have no information on the quality of the result, but it seems to work
5. Compactification. This adjusts the coordinate system so that the grid represents the entire computational domain, and you have no boundary. This is nice theoretically, but in practice seems to be similar to a sponge construction

As a general note, its very common to assume that the speed of the gauge waves is $1$, instead of $\sqrt{2}$

# Modifying the equations

We've now solved the initial conditions, and implemented our boundary conditions, so we just use our equations from the last article, and hit go right?

Unfortunately, the equations as-is cannot simulate binary black hole collisions, and we now enter a very murky area: Ad hoc modifying the equations, so that we can pull off these simulations correctly

In addition to the 'usual' issues, we also suffer from a unique issue: Rather low grid resolution, as well as using 32-bit floats instead of 64-bit doubles. The latter is largely a non issue luckily, but it exacerbates numerical issues with the singularity, by giving us a smaller margin for error[^leeway]

[^leeway]: The determinant of the metric tensor becomes degenerate on a failure of the boundary conditions, leading to infinities turning up. You can handle more degenerate matrices with double precision, giving the gauge conditions more time to catch up

## What's wrong with the equations?

It took a very *very* long time for the field to be able to successfully simulate binary black hole collisions, and even longer to do full circular orbits. There are a variety of issues here, which we'll now go over

1. The gauge conditions resulted in coordinate system stretching, as well as the singularity blowing up in the moving punctures technique (well, before it was discovered)
2. Momentum constraint errors
3. Christoffel symbol constraint errors

The structure of the equations we're using are already adapted to simulating binary black holes by design luckily, but older sets of equations were numerically unstable for these problems

### Gauge conditions

The BSSN equations contain extra degrees of freedom, and one allowance you can make is setting the non gauge variable $K=0$. This is called maximal slicing. Maximal slicing defines a coordinate system that is known (todo: paper) to have singularity avoiding tendencies, and is a good candidate from preventing our equations from exploding due to the presence of a singularity

Setting K=0 allows us to solve for the lapse via an elliptic equation derived from the <CHECKME> constraint. While elliptic equations are tricky to solve with any kind of efficiency, an approximate solution comes in the form of what is called the 1+log gauge - which is an approximate solution to the elliptic equation to drive K=0

Todo: Equation

Additionally, the shift is defined as such:

Todo: Shift

Todo: Explanation of the shift equation

$N$ is a free parameter that represents damping: In general its set to $2$, but it can also take fairly arbitrary values from $1$ to $>10$, or even vary spatially or by time. This parameter has a heavy influence on the paths of the black holes

Taken together, these two gauge conditions are called the moving puncture gauge, and are the standard choice for near equal mass[^nearequalmass] binary black hole collisions

[^nearequalmass]: High mass ratio collisions are an unsolved problem. There might be a future article about exploring higher ratios, but we'd likely need adaptive mesh refinement to make a dent in that problem

### Momentum constraint errors

These can be damped in exactly the same form as in the previous article, via:

Todo: Equation

Traditionally, momentum constraint violations tend to produce high frequency, self reinforcing oscillations, resulting in your simulation blowing up. With an implicit integrator, this is much less of a problem for us, so momentum constraint damping is more about increasing correctness rather than for stability reasons. This makes it much less important for us in this construction, than alternative techniques - which is good, because this damping is heavy on memory bandwidth

### Christoffel symbol errors

The traditional damping scheme is as such:

Todo: Code

## This doesn't work in practice

These modifications would, in theory, work if you were using some kind of adaptive mesh refinement scheme with virtually unlimited accuracy. Unfortunately, the entire point of this article series is to try and do simulations a little faster than that, so instead of relying on more compute power to fix our problems, we need to attack the issues at a more fundamental level

## Lets reproduce a result

Trying to simulate with the above set of modifications is not going to work, and instead today we're going to focus on trying to reproduce a specific result: todo. The no spin test case from this paper. By the end of this article, we will be close enough to demonstrate that the sim we've built isn't totally invalid

In general, exactly reproducing other people's results is an absolute nightmare, and is virtually impossible in the general case. While it isn't hard to get close, the field has a strong issue with being underdocumented, and many of the larger simulators simply do not publish enough information to replicate their results. Simulations also seemingly tend to involve a calibration step, but information on the limitations or inaccuracies of these simulations is nearly impossible to come by due to the nature of academic publishing

I'm going to walk us through how to take our sim from self destructing immediately, to successfully completing over an orbit. We'll get 'close enough' to a result

## First off, lets test the standard modifications

With the above modifications enabled, our sim goes from blowing up immediately, to successfully managing to simulate a collision at roughly T=Todo. Because we're more interested in the paths that the black holes take for the moment, rather than the specific collision time, I'm just eyeballing the numbers here

We can see that rather than the full orbit and a half we'd expect, our black holes barely make an orbit before clearly merging

### The black hole inspiral ends too early

We lose energy too quickly, as we can see that our black holes do not travel the requisite distance. There are a variety of possible issues here

1. Constraint violations, and damping strength (including $N$ for the gauge damping parameter)
2. Kreiss oliger dissipation strength
3. Gauge condition issues

We're going to address kreiss-oliger in a slightly unusual way, and there's going to be a heavy tangent about kreiss oliger dissipation in general

While we could address these issues in any order, I'm going to address these in the order that makes the most noticeable difference to evolution

### Curvature adjusted kreiss-oliger dissipation (CAKO todo?)

The basic idea here, the authors of this (todo) paper argue, is that sharp features near the event horizon of a black hole are natural. This makes a certain amount of sense, and after all - if we're trying to represent as much information on our grid as possible, removing high frequencies in high frequency areas seems like a prime candidate for wasting grid resolution

To adjust this, the authors multiply the damping strength, by the parameter $W$, thus making the damping strength virtually 0 near the event horizon

Testing this directly, we can see that our code immediately unconditionally blows up. It seems that for us, we do need some damping near the event horizon, and instead I alter the damping strength as follows:

todo

#### Kreiss oliger is a bit.. ill defined

One of the difficulties with all of this is trying to prove that your code converges. The idea is, as your grid resolution goes up, your code should become more accurate

The tricky part is that while we use scalar constants for damping factors, many of these factors are actually *not* resolution independent, and should really scale with some factor of the grid resolution

Kreiss-oliger is something that stood out to me as a candidate for making convergence tricky. If we examine the specific form of kreiss oliger, you'll notice that for eg second order kreiss, the form is as such:

todo

This involves a single division by the scale. So as your grid resolution goes down, the prefix term blows up to infinity. What gives?

##### Courant Factors

If you're unfamiliar with the concept of a courant factor, the core concept is that the timestep of any simulation is limited by what's called a courant factor. It is essentially the statement that information can only propagate across your grid at a finite speed, and is mathematically defined as $\frac{\Delta t}{\Delta h} = C$. If you increase your timestep too high, you must have instability, because you're trying to pipe information through your grid faster than your problem allows it to propagate. Similarly, if you change your grid resolution, you must also change your timestep to match

If you compare, the prefix factor of kreiss-oliger *is* the courant factor, and is therefore a constant. It would certainly sound sensible to replace it with a constant, except...

##### We don't have a courant factor

I missed part of the definition of the courant factor, which is that it only applies to *explicit* integrators. Equations which are implicitly integrated do not have a courant factor, as they propagate information faster than with an explicit integrator. In fact, $\frac{\Delta t}{\Delta h}$ can be arbitrarily large, and is only limited by the convergence of your implicit integration technique. With an implicit integrator of sufficient quality - grid resolution, and timestep, are fully decoupled

This raises a tricky question: If we change the size of our grid resolution to being infinitely small, our timestep doesn't necessarily have to change beyond practical convergence limitations. This means that kreiss oliger is somewhat ill defined for us, as there's no reason that the prefix term is fixed to anything useful

To work around these, we're going to set it to a constant, and emulate having a courant factor by calculating our timestep as if we had one. Bear in mind that this is fairly arbitrary, and I suspect is why other people's kreiss-oliger settings do not directly work in our code. Unfortunately, this is the only simulation I've seen which uses fully implicit integration, so I've seen exactly 0 discussion of this issue in the literature[^imexstepping]

[^imexstepping]: The SXS group use imex stepping, which is a semi implicit scheme. They split the equations up, and solve them partially implicitly. Notably however, they do not solve the derivatives implicitly due to the computational cost, which means that this implicit scheme cannot sidestep the courant factor as with a fully implicit scheme - I suspect this is why they see marginal benefits

### Testing CAKO

With kreiss oliger being set up like follows:

Todo:

Our code now makes it to T=todo. One key feature to note is that: With very reduced kreiss oliger dissipation, we're now much more vulnerable to 0 speed modes (which are no longer dissipated away), and so you want to ensure you're using the advecting lapse and shift conditions for the gauge

The last technique we're going to use is actually a new one for me, and is called slow start lapse

### Christoffel symbol errors, round 2

As an observational note, too much christoffel constraint damping tends to lead to very aggressive inspiral, as well as the black holes taking incorrect paths in regions of higher velocity. Too little damping tends to lead to a failure to inspiral at all, or the black holes failing to maintain their shape entirely. The right types, and strength of damping, will lead to correct convergence - though it is tricky to get right, and can feel very arbitrary

We have 3 modifications we're going to be testing here, which I'll list below:

1. Traditional $\sigma$ damping
2. Modifying the evolution equations for $\tilde{\gamma}_{ij}$, via a new scheme
2. Modifying the evolution equations for $\tilde{\Gamma}_{i}$, via a new scheme

There's no strong reason a priori to favour any of these schemes over the other[^cafdamping], so I'll simply refer to them by number and quickly describe the characteristics. This is - depressingly - observational evidence based on running literally 1000s of these simulations and tests

[^cafdamping]: Todo: Cite the stability paper, CAF damping efficacy, lack of constraint propagation analysis in general

1. The $\sigma$ damping scheme tends to be the least robust, and the christoffel symbol constraint is often significantly incorrect. Its too simplistic and loses energy tremendously. For this reason it is frequently combined with other constraint enforcement techniques (eg replacing undifferentiated $\tilde{\Gamma}^i$s with the analytic variable), which we do not use as they lack genericity. It is, observationally, significantly lossy in terms of energy
2. This scheme works well, but isn't sufficiently able to damp away $\tilde{G}^i$ constraint errors, even with high values of the damping coefficient. Higher damping coefficients lead to lowering the courant factor (we'll talk about courant later), which is undesirable
3. This scheme also works reasonably, but is also insufficiently damping

We're going to ditch the traditional $\sigma$ damping, and instead use two key other constraint handling techniques combined, which together provide sufficient stability

Scheme 2. (cY) comes from this paper, and is described as follows

todo

Scheme 3. comes from this paper, and is describe as follows[^papersketch]:

todo

[^papersketch]: I have been unable to reproduce much of the improvements that the authors have claimed for some of their equations, and they do not in general seem applicable to us

Together, these seem to work adequately. I have a strong unproven suspicion that the reason these modifications are better at preserving energy is because they're both multiplifed by a factor of $\alpha$, reducing the damping nearer the black hole and singularity vs the standard damping scheme

Despite the improvement, we're still losing too much energy. We've done about as much as we can to improve the christoffel symbol constraint errors

### Slow start lapse

The initial conditions we have aren't actually for black holes, they're for objects that collapse into black holes. This results in a lot of initial gauge jiggling - particularly for the lapse - that can be bad for the health of the simulation. One solution for this is the so called slow start lapse, where you essentially ease it it gradually over a period of time, eliminating a huge initial collapse in the lapse

As the gauge conditions are arbitrary, this does not (in theory) alter the correctness of the sim:

todo

With xyz set to todo:

# Results

With all of this together, we're able to replicate - close enough - the original paper, and we have now smashed black holes together pretty successfully. There are still some inaccuracies however, which I'll go over now

# Discussion

I have some questions about efficiency of the moving punctures technique as a whole. As mentioned previously, the puncture is a true discontinuity, and differentiating across it is invalid. This (in theory) is a major source of constraint violations, though I've seen suggestions that the event horizon itself is also a source of constraint violations

In the literature, there are *many* ad-hoc techniques used with respect to the puncture, including:

1. Dropping the order of kreiss oliger dissipation within the event horizon
2. Dropping the order of differentiation within the event horizon
3. Using upwinded stencils for differentiation with respect to the shift, which is very dissipative near the singularity where the shift vector undergoes a discontinuity

For me, a lot of this seems like that any kind of differentiation is invalid near the puncture, and so techniques indirectly tend to lower the constraint contamination by reducing the singularity, or reducing the size of stencils to reduce constraint error propagation. In theory - some BSSN formulations have causal constraint propagation, but this breaks down *somewhat* in the face of discretisation, and still relies on you having a sufficiently large event horizon

It would seem ideally what we'd do is use an excision technique instead, instead of relying on these slightly odd techniques and properties. Its on my todo list

# Misc Notes

This section contains pieces of information which are useful to know, but don't really fit into the structure of the article here

## Black holes do not really have mass

There's an unfortunate truth for black holes, which is that there is no unique definition of the mass of a black hole. Its very easy to work out how much mass there is for something like the earth - because we can calculate how much *matter* is present, but black holes here are a vacuum solution - there's no matter of any description in our spacetime. This means that we have to work out what the effect on spacetime a black hole has equivalent to a certain amount of mass

There are multiple definitions of mass that we can use:

1. Bare mass, which has no physical meaning. Higher = more mass, but that's about it
2. ADM mass, which is measured at 'infinity', and measures how much energy is present in the spacetime. Only valid in asymptotically flat spacetimes
3. Komar mass - defined in stationary spacetimes
4. Horizon mass, determined by the area of the event horizon
5. Puncture mass, determined by the size of the correction factor $\psi$ at the puncture, which approximates the ADM mass

Black holes are a form of energy stored in spacetime and are a global phenomenon. Two black holes which are nearby each other store a significant amount of energy in the spacetime between them, increasing their mass (essentially further delocalising them). This makes it hard to pin down exactly how much 'mass' an individual black hole has in a binary pair, because they aren't truly separate phenonema, nor are they truly local phenomena. This means that all of these definitions of mass can disagree

Despite this, its very common to use ADM mass via puncture mass[^internallyflat], and horizon mass with an approximation to its event horizon. We'll be using the former

[^internallyflat]: The asymptotically flat infinity here is the puncture *inside* the black hole. I have no idea how this works, but apparently it does

## Event horizon calculation

Technically, the event horizon is *4d* surface that lightrays cannot escape from. In the literature, it is very common to talk about event horizons, and the size of the event horizon etc, but it is very rare to actually calculate what is actually technically the event horizon

In GR, the only true way to find an event horizon is to trace geodesics around, and see where they become trapped. In our numerical relativity simulation, you can imagine tracing rays backwards in time from the camera as a visualisation, and in theory our rays will become trapped on any event horizons. We could plot when and where they get trapped,and use that to define a 4-surface. There's a small problem though:

There are no true event horizons in our simulation. The issue is twofold:

1. The strict definition of an event horizon requires full knowledge of the future of the black hole
2. The strict definition of an event horizon requires full knowledge of the past of the black hole

Interestingly, because our black holes are not eternal and form from the collapse of a black hole with no horizon, rays fired backwards in time only become briefly suspended on a horizon, and then quite happily escape again. We also clearly cannot simulate an infinite amount of time into the future. We can however approximate a strict event horizon calculation by defining a cutoff time both in the past, and in the future, where we suspect that a horizon has formed, and use that as our technical horizon surface

### Marginally Outer Trapped Surface (MOTS)

Event horizon calculation requires storing the entire history of the simulation and tracing geodesics[^wewilldothis], so it is uncommon as an approach. In the literature, a surface which is hoped to be equivalent/similar is used called the 'marginally outer trapped surface' (MOTS). This is a purely local quantity defined on a specific slice of spacetime. When calculating the black hole's mass via horizon mass, the horizon area is generally calculated via the MOTS

[^wewilldothis]: We're actually going to do this in a future article, in realtime, to produce accurate renderings of binary black hole collisions
