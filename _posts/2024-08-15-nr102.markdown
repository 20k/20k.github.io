---
layout: post
title:  "Numerical Relativity 102: Simulating binary black hole collisions in the BSSN formalism, quickly"
date:   2024-08-15 12:33:23 +0000
categories: C++
---

Hello! Today we're going to do one of the coolest things in all of physics in my opinion, which is simulating the collision of two black holes. Last time round (todo: link), we implemented most of what we'll need to simulate this, so today's job is to capitalise on that and finally smash some black holes together

This article consists of two primary parts:

1. Solving our new initial conditions for black holes, via a simple laplacian
2. Modifying the equations to render them suitable for simulating black holes

This article was significantly delayed due to illness sadly \:(

# Astrophysics Context

Its worth taking a minute to understand what a black hole actually is, and why its such a difficult road to simulate them numerically. In general relativity, a black hole is a form of *soliton* - it is a stable, self supporting configuration of spacetime, that propagates itself indefinitely. It does not require any matter to support it, not necessarily to even form it, and the spacetimes we are dealing with here are vacuum solutions. There's no matter of any description here, and that will be a future article

Black holes have two primary distinguishing features compared to other compact objects:

1. They have an event horizon, out of which nothing can escape
2. They form a singularity at their centre. A spinning black hole has a ring singularity (ringularity), and a non spinning black hole's singularity is a point

The event horizon itself acts completely normally in general relativity, but it is the singularity which causes problems for us. It is a true mathematical error, and we cannot simulate it by any means. Because it will always form, we must come up with some tricks for dealing with it. These notably fall into one of two categories:

1. Excision, where we simply do not simulate the interior of a black hole near the singularity
2. The technique known as "moving punctures", where we rely on coordinate system trickery to keep our spacetime away from the discontinuity, which moves around our simulation grid

We're going to be relying on the second technique in this article

![ringularity](/assets/ringularity.png)

## Managing the singularity

### Excision

This technique is fairly common, and involves chopping out the singularity entirely, and simply not simulating it. To excise the singularity correctly, you must define a boundary condition inside the black hole itself, then use one sided derivatives on the boundary. This technique works well, but involves the complexity of calculating and implementing the boundary condition - as well as defining *where* you should apply it

### Moving punctures

This is the technique we'll actually be using, and involves a bit of trickery with our coordinate system. In the ADM formalism, the gauge conditions define our coordinate system (via our gauge variables, ie the lapse $\alpha$ and shift $\beta^i$). The trick is to make sure that the coordinates never actually meet the singularity, and it is instead treated as a puncture/hole in your simulation grid. This puncture is able to move around with the correct gauge conditions, and so we have a singularity that is never actually quite simulated explicitly. In moving punctures, the entire simulation domain is actively simulated, unlike with excision where a portion of it is not

In practice, its a lot trickier than this, and it involves a lot of subtleties to get this to work in a physically accurate way. Notably, as the singularity represents a true discontinuity, differentiating across it is technically incorrect. This means we must rely on the causality[^intheory] of the event horizon to prevent these errors from escaping into our wider universe, and keep those errors trapped inside the black hole itself

[^intheory]: In practice, this is a hugely complicated topic that is going to make our lives very difficult. Its not that GR is incorrect about this, its that getting this guarantee in a numerical simulation is non trivial. While general relativity is a casual theory (where the maximum speed of causality is C), it is not a priori true as a result that *errors* in the BSSN formalism we use are causal. It also just so happens that - in theory - errors in BSSN *are* causal with the standard formalism - except we're using some modifications. This means that theoretically we're on slightly shakey grounds. In practice, error propagation is inherently non causal because of errors introduced by discretisation and discrete timestepping, which means we need a margin of error within our event horizon for errors to remain trapped within it

## Binary black hole orbits

The conventional wisdom is that black holes merge after a long inspiralling process, where the nature of gravitational wave emissions slowly circularises the orbits. This means that binary black hole orbits should be nearly perfectly circular by the time the black hole's merge

Unfortunately, this turned out not to be the case, and it appears that there is a significant amount of eccentricity (non circularity) when black holes merge. This really complicates the amount of simulating you have to do, and so more than ever being able to simulate a large number of test cases is very important.

## Unequal mass binaries

With the assumption of circular orbits, equal mass/spin binaries, and the black hole spin being aligned with the plane of the orbit, you can use octant symmetry - ie your simulation is fully symmetric and can be chopped into 8 chunks, with only one of the chunks actually being simulated. If your black holes are of unequal mass but with axis aligned spins, you can use axisymmetry, for a 2x speedup

Personally I'm interested in a very general simulator which can handle anything, so we're going to avoid any symmetry reduction - but given its prevalence in the literature, its worth mentioning here. For equal mass binaries, this would be a flat 8x speedup, so its definitely worth investigating for lower end hardware

## Gravitational waves

The reason for doing any of this is that tools like LIGO are able to detect gravitational waves from merging compact objects, and gives us a measure of the gravitational waves passing through spacetime. The only way to reverse engineer what objects collided, is to simulate those gravitational waves ourselves via simulations like this, and then match up what set of parameters most closely matches our simulation

This means that physical accuracy here is paramount, and we're going to have to be very careful with how we operate in general. We won't be pulling out gravitational waves today however, but it'll be coming up shortly

To perform this matching, as far as I know there's no better way than simply bruteforcing the entire parameter space. For a pair of black holes in an ADM simulation, you have the following parameters:

M1: The mass of the first black hole
M2: The mass of the second black hole
S1: The spin direction - a 3-vector - of the first black hole, normalised
S2: The spin direction of the second black hole
X1: The spin constant of the first black hole
X2: The spin constant of the second black hole
e: The orbital eccentricity

This is a lot of parameters to bruteforce, and its why non circular orbits are such a gigantic pain, because it adds a whole extra dimension to bruteforce

Traditional simulations in this area can take weeks to months, which you'll be glad to know is not going to be the case for us. It'll be about 5-10 minutes for an orbit tops

We'll be digging into the nitty gritty details of what these parameters mean in a physical sense shortly

# What's the plan?

We need a few things to get this to work:

1. A new set of initial conditions
2. New boundary conditions
3. Some modifications to the evolution equations

Of all of these, the initial conditions are by far the most complicated part code-wise, so lets get to it

# Initial conditions

I'm going to present you with the easiest set of initial conditions to implement, and we're going to really dig into it. Some discussion about alternatives and further context is given at the end of this article

The paper we're going to implement today is called [A simple construction of initial data for multiple black holes](https://arxiv.org/pdf/gr-qc/9703066). This represents an extremely traditional and widespread way to construct black hole initial conditions - its also a very good paper that you should read. There are a few things that we need to learn right off the bat

1. This doesn't actually make black holes, it makes objects that collapse into black holes. They have no event horizon, or apparent horizon initially. This is commonly described as a wormhole collapsing into a trumpet - a trumpet being the final configuration of a black hole in numerical relativity
2. There is no way of a-priori specifying the 'real' (we'll get into this shortly) mass of a black hole
3. There is a maximum value of spin that can be expressed with this technique
4. This works for any number of black holes, but astrophysically there's virtually no way to have a 3 body interaction. Funwise however, there is plenty of reason

Lets get into it

## Implementing

To take a step back, we're trying to find initial values for the following ADM variables:

$$\gamma_{ij}\\
K_{ij}\\
\alpha\\
\beta^i
$$

This technique solves for $\gamma_{ij}$ and $K_{ij}$, and leaves us to guess $\alpha$ and $\beta^i$. Because the latter two variables are gauge variables, we can (in theory[^ofcourse]) pick them arbitrarily. It falls under the class of initial conditions known as 'conformally flat'. This means it decomposes the metric tensor into a flat conformal metric (the identity matrix in cartesian coordinates), and a conformal/scalar factor $\psi$. It also decomposes the extrinsic curvature $K^{ph}_{ij}$ ('physical', this is our $K_{ij}$) into a trace $K=0$, and a trace free part which this paper calls $K_{ij}$ (and other papers often call $$\bar{A}_{ij}$$). This is called a maximal slice

[^ofcourse]: Of course, as you can probably guess, its very important what their value is in practice for numerical reasons

Notationally, this is already a hot mess, and this is a standard issue in the field as the notation has evolved over time. Here's the notation for what we're using:

| Standard Adm | Paper Notation | Meaning |
|-|-|-|
| $\gamma_{ij}$ | $g_{ab}^{ph}$ | The 3-metric tensor |
| $K_{ij}$ | $K_{ab}^{ph}$ | Extrinsic curvature |

And for the decomposition

| Our notation | Paper Notation | Meaning |
|-|-|-|
| $\psi$ | $\psi$ | The conformal factor |
| $\bar{\gamma}_{ij}$ | $g_{ab}$ | The conformally flat 3-metric |
| $\bar{A}_{ij}$ | $K_{ab}$ | The conformal extrinsic curvature |

The basic idea here is that when you impose conformal flatness, and maximal slicing, the momentum constraint $\mathcal{M}_i$ becomes linear, allowing you to calculate the extrinsic curvature $$\bar{A}_{ij}$$ as a sum of each individual black hole's curvature, to calculate the final (conformal) extrinsic curvature. The shape of the conformal factor $\psi$ is a *guess*, which is then corrected to be physical via a small correction $u$, by solving the laplacian (9) + (10)

## Details

This method has the following parameters:

$m$: the bare mass parameter of a black hole
$S^i$: A 3-vector representing the spin (angular ADM momentum) of a black hole
$P^i$: A 3-vector representing the linear momentum (ADM mass * velocity) of a black hole
$x^i$: A 3-vector representing the position of a black hole in world coordinates

Note that bare mass is not the same thing as mass (here: ADM mass), it only correlates to it. A black hole's spin and momentum contributes to its ADM mass as well. On top of this, in the general case, the mass of a black hole in a binary pair is of questionable physicalness: see the end of this article for more details. Here, you don't need to worry about this, because we'll be giving the initial conditions in terms of bare mass: just be aware this isn't universal in the literature, and you are expected to binary search bare mass parameters to achieve your final ADM mass

So far, we have the following variable definitions:

$$
\begin{align}
&\bar{\gamma}_{ij} &&= \gamma_{ij} \psi^{-4} = \delta_{ij} = I \\
&\bar{A}_{ij} &&= K_{ij} \psi^2 \\
&\psi && = ?
\end{align}
$$

Todo: Explain dimensionless spin

## Conformal extrinsic curvature

This is straightforward. For every black hole in our spacetime, we calculate the quantity (5), and then sum over our black holes. After accumulating all of these, I store $\bar{A}^{ab} \bar{A}_{ab}$ as a scalar

For a single black hole, we have this:

$$
\begin{align}
\bar{A}^{ij} &= \frac{3}{2r^2}(P^a n^b + P^b n^a - (\bar{\gamma}^{ab} - n^a n^b) P^c n_c) \\
&+ \frac{3}{r^3}(\epsilon^{acd} S_c n_d n^b + \epsilon^{bcd} S_c n_d n^a)
\end{align}
$$

| Symbol | Meaning |
|-|-|
| $\epsilon$ | The levi civita symbol |
| $r$ | The distance in world coordinates (ie not grid coordinates) from your current position, to the black hole in question |
| $n$ | a normal vector, which is calculated as: $n^k = x^k/r$, and points away from the black hole in question |

There's a clear problem when $r = 0$, and the solution is to either position your black holes so this is impossible, or clamp to a small value

If you have no plans to ever work in non cartesian coordinate systems (which we don't), the metric tensor $\bar{\gamma}_{ij}$ is the identity matrix, and you can ignore the index positions. Otherwise, it raises and lowers indices as normal

For multiple black holes, you simply sum all your $\bar{A}^{ij}$ terms

Code wise: This is all straightforward to implement:

```c++
//levi-civita symbol
tensor<valuef, 3, 3, 3> get_eijk()
{
    auto eijk_func = [](int i, int j, int k)
    {
        if((i == 0 && j == 1 && k == 2) || (i == 1 && j == 2 && k == 0) || (i == 2 && j == 0 && k == 1))
            return 1;

        if(i == j || j == k || k == i)
            return 0;

        return -1;
    };

    tensor<valuef, 3, 3, 3> eijk;

    for(int i=0; i < 3; i++)
        for(int j=0; j < 3; j++)
            for(int k=0; k < 3; k++)
                eijk[i, j, k] = eijk_func(i, j, k);

    return eijk;
}

tensor<valuef, 3, 3> get_aIJ(v3f world_pos, v3f bh_pos, v3f angular_momentum, v3f momentum)
{
    tensor<valuef, 3, 3, 3> eijk = get_eijk();
    tensor<valuef, 3, 3> aij;
    metric<valuef, 3, 3> flat;

    for(int i=0; i < 3; i++)
    {
        flat[i, i] = 1;
    }

    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            valuef r = (world_pos - bh_pos).length();

            r = max(r, valuef(1e-6f));

            tensor<valuef, 3> n = (world_pos - bh_pos) / r;

            tensor<valuef, 3> momentum_lo = flat.lower(momentum);
            tensor<valuef, 3> n_lo = flat.lower(n);

            aij[i, j] += (3 / (2.f * r * r)) * (momentum_lo[i] * n_lo[j] + momentum_lo[j] * n_lo[i] - (flat[i, j] - n_lo[i] * n_lo[j]) * sum_multiply(momentum, n_lo));

            ///spin
            valuef s1 = 0;
            valuef s2 = 0;

            for(int k=0; k < 3; k++)
            {
                for(int l=0; l < 3; l++)
                {
                    s1 += eijk[k, i, l] * angular_momentum[l] * n[k] * n_lo[j];
                    s2 += eijk[k, j, l] * angular_momentum[l] * n[k] * n_lo[i];
                }
            }

            aij[i, j] += (3 / (r*r*r)) * (s1 + s2);
        }
    }

    return aij;
}
```

Next up: We need the conformal factor $\psi$, which involves calculating the correction $u$

## Conformal Factor

The conformal factor $\psi$ here is listed as:

$$\psi = \frac{1}{\alpha} + u$$

Where $u > 1$, and $\alpha$ is:

$$\frac{1}{\alpha} = \sum^N_{i=1} \frac{m_i}{2 |\overrightarrow{r} - \overrightarrow{x}_{(i)}|}$$

Note that $r$ in the bottom term is a vector representing our current grid cell's world position. The bottom expression is twice the distance of our coordinate from our $i$'th black hole. The $\alpha$ here has nothing to do with the gauge variable lapse also notated by $\alpha$

We're going to deviate a little from this definition, and instead calculate the correction as:

$$\psi = \frac{1}{\alpha} + u + 1$$

Where $u > 0$. This is because you have a lot more precision to work with for floating point near $0$ vs near $1$, so this lets us solve for much smaller corrections without needing to resort to double precision. This means our boundary condition will be slightly different ($u=0$, instead of $u=1$)

### Solving for $u$

Equations (9) + (10) are a classic laplacian:

$$\begin{align}
&\Delta u + \beta(1 + \alpha (u + 1))^{-7} = 0\\
&\beta = \frac{1}{8} \alpha^7 \bar{A}^{ab}\bar{A}_{ab}
\end{align}
$$

This is the slightly more complex version of the equation. Instead, we'll actually implement (3) directly, as is common practice (eg [(18)](https://arxiv.org/pdf/1606.04881), or [(13)](https://www.worldscientific.com/doi/pdf/10.1142/S2010194512004321))

$$ \Delta \psi + \frac{1}{8} \bar{A}^{ij} \bar{A}_{ij} \psi^{-7} = 0 $$

To solve this, we simply expand out the laplacian, as $ \psi = \frac{1}{\alpha} + u + 1$, and $ \Delta (\frac{1}{\alpha} + u + 1) = \Delta u $. This gives our actual form of the equation that we'll be implementing:

$$\begin{align}
&\Delta u + \frac{1}{8} \bar{A}^{ij} \bar{A}_{ij} (\frac{1}{\alpha} + u + 1)^{-7} = 0\\
\end{align}
$$

Though in practice you might retain the $\psi$ notation in the $^{-7}$ portion

#### Solving laplacians in general

Our equations are of the form:

$$\Delta u = F(u)$$

And are a standard laplacian. If you already know how to solve a laplacian, there is absolutely nothing special in this section (beyond that we're doing it on a GPU[^gpufast]) and you may skip it

[^gpufast]: This is one of the cases where using a GPU provides transformational performance boost - we can get away with a much simpler implementation. In standard numerical relativity, it is common to use much more complex schemes because they have to converge in a reasonable time on a CPU

#### The boundary condition

At the edge of our grid, we need to set $u$ to something. Asymptotically, $u$ is assumed to have the form $O(\frac{1}{r})$, and at infinity in the original definition $u = 1$. This means for us, $u = 0$

#### Fixed point iteration / 7 point stencil

The question now is: How do you actually solve our laplacian equation? This segment is going to deal with the theory

A laplacian in flat 3d space (which luckily we are in) is defined as such:

$$\Delta u = \frac{\partial^2 u}{\partial x_0 ^2} + \frac{\partial^2 u}{\partial x_1 ^2} + \frac{\partial^2 u}{\partial x_2 ^2}$$

Ie, it is the sum of second derivatives in each direction. Given a function $f(x)$, we can approximate the second derivative with finite difference coefficients:

$$\frac{\partial^2 f(x)}{\partial x^2} = \frac{f(x - h) - 2 f(x) + f(x + h)}{h^2}$$

Where $h$ is the spacing between samples. Now, we'll treat $u$ as a function over 3d space, ie $u(x, y, z)$, and apply the above discretisation to $\Delta u$:

$$\Delta u = \frac{u(x - h, y, z) - 2 u(x, y, z) + u(x + h, y, z))}{h^2} + \frac{u(x, y - h, z) - 2 u(x, y, z) + u(x, y + h, z))}{h^2} + \frac{u(x, y, z - h) - 2 u(x, y, z) + u(x, y, z + h))}{h^2}$$

Collecting all the terms, we get:

$$\Delta u = \frac{u(x - h, y, z) + u(x + h, y, z) + u(x, y - h, z) + u(x, y + h, z) + u(x, y, z - h) + u(x, y, z + h) - 6 u(x, y, z)}{h^2}$$

Our initial equation is $\Delta u = F(u)$, so lets substitute in our discretisation above, and multiply by $h^2$:

$$u(x - h, y, z) + u(x + h, y, z) + u(x, y - h, z) + u(x, y + h, z) + u(x, y, z - h) + u(x, y, z + h) - 6 u(x, y, z) = F(u) h^2$$

The iterative scheme for solving a laplacian (called a 5-point stencil in 2d, or a 7-point stencil in 3d), is given by solving for $u(x, y, z)$:

$$
u(x, y, z) = \frac{u(x - h, y, z) + u(x + h, y, z) + u(x, y - h, z) + u(x, y + h, z) + u(x, y, z - h) + u(x, y, z + h) - F(u) h^2}{6}
$$

Simply apply this repeatedly to a grid, and you've solved your laplacian. For us, $F(u) = -\frac{1}{8} \bar{A}^{ij} \bar{A}_{ij} (\frac{1}{\alpha} + u + 1)^{-7}$ (note the minus sign)

As a basic idea, this works, and is implementable. This is the form I used for a very long time: you just iterate the above equation until the change in $u$ is sufficiently small. There are two main issues:

1. It can be numerically unstable, particularly for spinning black holes
2. It isn't amazingly fast, it can take ~10s or so in some cases, which is still pretty fast

The first is solved by relaxation: Ie taking our old guess, and our new guess, and taking some fraction between the two: `mix(u, nextu, 0.9f)`. This reduces the speed of convergence, but ameliorates numerical stability problems

With that in mind, lets have a look at our first laplacian solver:

##### Code

```c++
///takes a linear id, and maps it to a 3d coordinate
v3i get_coordinate(valuei id, v3i dim)
{
    using namespace single_source;

    valuei x = id % dim.x();
    valuei y = (id / dim.x()) % dim.y();
    valuei z = id / (dim.x() * dim.y());

    pin(x);
    pin(y);
    pin(z);

    return {x, y, z};
}

auto laplace_basic = [](execution_context& ectx, buffer_mut<valuef> in, buffer_mut<valuef> out,
                        buffer<valuef> cfl, buffer<valuef> aij_aIJ,
                        literal<valuef> lscale, literal<v3i> ldim,
                        buffer_mut<valuei> still_going, literal<valuef> relax)
{
    using namespace single_source;

    valuei lid = value_impl::get_global_id(0);

    auto dim = ldim.get();

    if_e(lid >= dim.x() * dim.y() * dim.z(), [&]{
        return_e();
    });

    v3i pos = get_coordinate(lid, dim);

    //the boundary condition is implicit and does not need to be set
    //ie the buffers that we pass in are initialized to the boundary condition of 0
    if_e(pos.x() == 0 || pos.y() == 0 || pos.z() == 0 ||
         pos.x() == dim.x() - 1 || pos.y() == dim.y() - 1 || pos.z() == dim.z() - 1, [&] {

        return_e();
    });

    //cfl[lid] is 1 + 1/alpha, in[lid] is our guess for u
    valuef rhs = -(1.f/8.f) * pow(cfl[lid] + in[lid], -7.f) * aij_aIJ[lid];

    valuef h2f0 = lscale.get() * lscale.get() * rhs;

    //terms to calculate the derivatives
    valuef uxm1 = in[pos - (v3i){1, 0, 0}, dim];
    valuef uxp1 = in[pos + (v3i){1, 0, 0}, dim];
    valuef uym1 = in[pos - (v3i){0, 1, 0}, dim];
    valuef uyp1 = in[pos + (v3i){0, 1, 0}, dim];
    valuef uzm1 = in[pos - (v3i){0, 0, 1}, dim];
    valuef uzp1 = in[pos + (v3i){0, 0, 1}, dim];

    //derivatives in each direction
    valuef Xs = uxm1 + uxp1;
    valuef Ys = uyp1 + uym1;
    valuef Zs = uzp1 + uzm1;

    //our next guess for u
    valuef u0n1 = (1/6.f) * (Xs + Ys + Zs - h2f0);

    //old guess for u
    valuef u = in[pos, dim];

    //relaxation, output
    as_ref(out[lid]) = mix(u, u0n1, relax.get());

    valuef etol = 1e-6f;

    //set a flag if the change in our value of u is still too high
    if_e(fabs(u0n1 - u) > etol, [&]{
        //store a 1 in address 0
        still_going.atom_xchg_e(valuei(0), valuei(1));
    });
};
```

There are potentially a few atypical features here:

1. The relaxation parameter is dynamic
2. We use a `still_going` parameter. This sets an atomic to `1` (the `0` is an address offset) if the change in cell value is greater than an error tolerance. This is so that we can dynamically terminate when we've converged enough

#### Red-black

Another technique that gives a free 2x performance increase is red-black iteration. For a standard 2d laplacian, each grid cell only accesses its neighbours. We can perform a red-black colouring to demonstrate this in 2 dimensions:

![redblack2d](/assets/redblack2d.png)

We can stretch this concept to 3 dimensions directly as well:

![redblack3d](/assets/redblack3d.png)

This means, we can simply update all red cells, then all black cells in an alternating fashion. Because the adjacent cells are one step into the future compared to the naive scheme, this is actually roughly a free 2x speedup in terms of iteration speed, and a further increase in speed due to halving the size of your problem per-iteration[^peritspeedup]

[^peritspeedup]: The latter potential of the gains in the algorithm presented here are only realised on a CPU, as GPU architecture means that the culled threads are still alive an consuming resources. A less naive execution and memory ordering would bring another 2x speedup, but in essence: its not really worth it after we go down the multigrid rabbithole

There's a slight wrinkle - our equations are not of the form $\Delta u = 0$, but instead $\Delta u = F(u)$. This means that we do touch a cell of the 'wrong' colour when updating, and it'll be one tick out of date. That is to say, if the stencil component of our RHS is given by $D(u)$, and our iteration variable is $k$, our iteration equation is:

$$u_{k+2} = D(u_{k+1}) - \frac{F(u_k)h^2}{6}$$

This does not affect convergence in practice, but its worth keeping in mind that this is slightly nonstandard. Code wise, red-black iteration is a simple change, and there are only two major differences:

1. We can use one buffer, instead of 2
2. We simply terminate half our threads early

```c++
auto laplace_rb_mg = [](execution_context& ectx, buffer_mut<valuef> inout, buffer<valuef> cfl, buffer<valuef> aij_aIJ,
                        literal<valuef> lscale, literal<v3i> ldim, literal<valuei> iteration,
                        buffer_mut<valuei> still_going, literal<valuef> relax)
{
    using namespace single_source;

    valuei lid = value_impl::get_global_id(0);

    auto dim = ldim.get();

    if_e(lid >= dim.x() * dim.y() * dim.z(), [&]{
        return_e();
    });

    v3i pos = get_coordinate(lid, dim);

    if_e(pos.x() == 0 || pos.y() == 0 || pos.z() == 0 ||
            pos.x() == dim.x() - 1 || pos.y() == dim.y() - 1 || pos.z() == dim.z() - 1, [&] {

        return_e();
    });

    //NEW: notice that as we offset in the z direction, our red-black scheme is simply offset in the x direction by 1
    valuei lix = pos.x() + (pos.z() % 2);
    valuei liy = pos.y();

    //NEW!
    if_e(((lix + liy) % 2) == (iteration.get() % 2), [] {
        return_e();
    });

    valuef rhs = -(1.f/8.f) * pow(cfl[lid] + inout[lid], -7.f) * aij_aIJ[lid];

    valuef h2f0 = lscale.get() * lscale.get() * rhs;

    valuef uxm1 = inout[pos - (v3i){1, 0, 0}, dim];
    valuef uxp1 = inout[pos + (v3i){1, 0, 0}, dim];
    valuef uym1 = inout[pos - (v3i){0, 1, 0}, dim];
    valuef uyp1 = inout[pos + (v3i){0, 1, 0}, dim];
    valuef uzm1 = inout[pos - (v3i){0, 0, 1}, dim];
    valuef uzp1 = inout[pos + (v3i){0, 0, 1}, dim];

    valuef Xs = uxm1 + uxp1;
    valuef Ys = uyp1 + uym1;
    valuef Zs = uzp1 + uzm1;

    valuef u0n1 = (1/6.f) * (Xs + Ys + Zs - h2f0);

    valuef u = inout[pos, dim];

    as_ref(inout[lid]) = mix(u, u0n1, relax.get());

    valuef etol = 1e-6f;

    if_e(fabs(u0n1 - u) > etol, [&]{
        still_going.atom_xchg_e(valuei(0), valuei(1));
    });
};
```

Sometimes, the code is simpler than the explanation! Luckily, there is no race condition with respect to $F(u)$, as we're the only thread that both reads and modifies that value due to the red-black iteration - otherwise we'd have to use two buffers again

#### Multigrid

One other easy way to speed up performance is via a multigrid solution. Essentially, instead of solving at your final grid resolution, you start off at a low resolution, solve for that, then progressively solve for higher resolutions. This is as simple as running our laplacian solving procedure at a single resolution, then upscaling that guess to a higher resolution. Do note you'll need to re-enforce the boundary condition:

```c++
valuef get_scaled_coordinate(valuei in, valuei dimension_upper, valuei dimension_lower)
{
    valuei upper_centre = (dimension_upper - 1)/2;

    valuei upper_offset = in - upper_centre;

    valuef scale = (valuef)(dimension_upper - 1) / (valuef)(dimension_lower - 1);

    ///so lets say we have [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] with a dimension of 13
    ///this gives a middle value of 6, which is the 7th value
    ///Then we want to scale it to a dimension of 7
    ///to get [0:0, 1:0.5, 2:1, 3:1.5, 4:2, 5:2.5, 6:3, 7:3.5, 8:4, 9:4.5, 10:5, 11:5.5, 12:6]
    ///so its just a straight division by the scale

    return (valuef)in / scale;
}

v3f get_scaled_coordinate_vec(v3i in, v3i dimension_upper, v3i dimension_lower)
{
    v3f ret;

    for(int i=0; i < 3; i++)
        ret[i] = get_scaled_coordinate(in[i], dimension_upper[i], dimension_lower[i]);

    return ret;
}

void upscale_buffer(execution_context& ctx, buffer<valuef> in, buffer_mut<valuef> out, literal<v3i> in_dim, literal<v3i> out_dim)
{
    using namespace single_source;

    valuei lid = value_impl::get_global_id(0);

    auto dim = out_dim.get();

    if_e(lid >= dim.x() * dim.y() * dim.z(), [&]{
        return_e();
    });

    v3i pos = get_coordinate(lid, dim);

    v3f lower_pos = get_scaled_coordinate_vec(pos, dim, in_dim.get());

    if_e(pos.x() == 0 || pos.y() == 0 || pos.z() == 0 ||
         pos.x() == dim.x() - 1 ||  pos.y() == dim.y() - 1 || pos.z() == dim.z() - 1, [&]{

        return_e();
    });

    ///trilinear interpolation
    as_ref(out[pos, dim]) = buffer_read_linear(in, lower_pos, in_dim.get());
}
```

This solution works incredibly well for this problem, as there is no high frequency content in $u$, and is a free performance win. There are no changes to the laplacian kernel here (almost as if this were planned), the only thing that we really need to worry about is the *host* side, and what grid resolutions to pick. I've found that lower grid resolutions can be somewhat numerically unstable[^pleasenotethat], so the relaxation parameter is dynamic. The code I use for generating this isn't too complex:

```c++
std::vector<t3i> dims;
std::vector<float> relax;

int max_refinement_levels = 5;

///generate a dims array which gets progressively larger, eg
///63, 95, 127, 197, 223
///is generated in reverse
for(int i=0; i < max_refinement_levels; i++)
{
    float div = pow(1.25, i + 1);
    ///exact params here are pretty unimportant
    float rel = mix(0.7f, 0.3f, (float)i / (max_refinement_levels-1));

    t3i next_dim = (t3i)((t3f)dim / div);

    //i use strictly odd grid sizes
    if((next_dim.x() % 2) == 0)
        next_dim += (t3i){1,1,1};

    dims.insert(dims.begin(), next_dim);
    relax.insert(relax.begin(), rel);
}

dims.insert(dims.begin(), {51, 51, 51});
relax.insert(relax.begin(), 0.3f);

dims.push_back(dim);
relax.push_back(0.8f);
```

There's probably some theory here on how to best generate grid sizes and relaxation parameters, so if you're aware of a better scheme do let me know. None of the exact parameters here are particularly important in practice: it only matters that the last grid resolution is the one we actually want to solve for. As usual, I'm pulling out the relevant snippets here, and the full code is available at TODO: EXACT LINK TO LAPLACIAN STUFF

[^pleasenotethat]: Please note that I'm not super confident in this result, as its been a while since I explicitly tested that

#### Further performance work

At this point, I simply stopped - because the performance is much better than it needs to be. If you'd like further speedups, here's where to go!

1. The in and out buffers should be compacted to remove the strided memory accesses as a result of red-black iteration
2. A list of active points should be provided as a buffer (or calculated), to avoid half of our threads simply immediately terminating due to red-black iteration
3. 16-bit fixed point would be acceptable precision for many of the lower grid sizes especially
4. The relaxation parameters are very pessimistic

## Finishing up

After solving the laplacian, we have a value for $u$, which allows us to calculate $\psi$ directly, as $\psi = \frac{1}{\alpha} + u + 1$. From here, we can calculate our ADM variables:

$$\begin{align}
\gamma_{ij} &= \psi^4 \bar{\gamma}_{ij}\\
K_{ij} &= \psi^{-2} \bar{A}_{ij}
\end{align}$$

Where if you remember: in cartesian coordinates, $$\bar{\gamma}_{ij}$$ is the identity matrix (or the flat metric). We already know how to construct our BSSN variables from here, see the last article for details

The ADM mass can be estimated via a standard approach, defined [here](https://arxiv.org/pdf/gr-qc/0610128) (6):

$$M_i = m_i (1 + u_i + \sum_{i \neq j} \frac{m_j}{2 d_{ij}})$$

Bear in mind that this paper uses the same convention for $u$ as we do. $d_{ij}$ is the world coordinate distance between our black holes

```c++
std::vector<float> extract_adm_masses(cl::context& ctx, cl::command_queue& cqueue, cl::buffer u_buf, t3i u_dim, float scale)
{
    std::vector<float> ret;

    ///https://arxiv.org/pdf/gr-qc/0610128 6
    for(const black_hole_params& bh : params_bh)
    {
        ///Mi = mi(1 + ui + sum(m_j / 2d_ij) i != j
        t3f pos = world_to_grid(bh.position, u_dim, scale);

        cl::buffer u_read(ctx);
        u_read.alloc(sizeof(cl_float));

        cl::args args;
        args.push_back(u_buf, pos, u_dim, u_read);
        //trilinear interpolation of u
        cqueue.exec("fetch_linear", args, {1}, {1});

        float u = u_read.read<float>(cqueue).at(0);

        float sum = 0;

        for(const black_hole_params& bh2 : params_bh)
        {
            if(&bh == &bh2)
                continue;

            sum += bh2.bare_mass / (2 * (bh2.position - bh.position).length());
        }

        float adm_mass = bh.bare_mass * (1 + u + sum);

        ret.push_back(adm_mass);
    }

    return ret;
}
```

This equation can be used to solve for a specific ADM mass by binary searching. There is no way a priori to get a specific ADM mass, so this is the only way to do it

### Gauge

We still don't have an initial solution for our gauge variables however. Universally, $\beta^i = 0$. The lapse $\alpha$ is a tad more interesting - there are two[^twomain] main options:

1. $\alpha = 1$
2. $\alpha = \psi^{-2}$

Both work well and are widely used. The latter is better for black holes, but I've found that it tends to lead to neutron stars exploding spectacularly. We're going to use #2 for this article, as we're strictly dealing with black holes

[^twomain]: In general, these two options are the two simple options that work well, but there are a variety of alternatives

This is the end of the initial conditions now, hooray! Here is a celebratory picture of a cat:

Todo: wesley is a cat

# Boundary conditions

In our first go at this, our data was periodic and simply wrapped around the boundary - which was our boundary condition. Here, our simulation is no longer periodic, so we need a new set of boundary conditions. The simplest good-enough boundary condition is called the Sommerfeld boundary condition. This radiates away your data to the asymptotic value at the boundary. [This is defined as follows](https://arxiv.org/pdf/1503.03436) [2.39]:

$$\frac{\partial f}{\partial t} = -\frac{v x_i}{r} \frac{\partial f}{\partial x_i} - v \frac{f - f_0}{r}$$

$f$ is our variable for whatever field we're radiating, $x_i$ is the cell's position in world coordinates, $v$ is the wave speed (usually 1), $f_0$ is the asymptotic value, and $r$ is the distance from the origin in world coordinates. $\frac{\partial f}{\partial x_i}$ is the standard derivative of the function in each direction $\partial x_i$ (which you will need to implement as one sided derivatives). This equation is applied to every tensor component separately, as if they were independent fields

The derivatives at the boundary will be one sided derivatives. Here's a table of values

| Variable | $f_0$ | $v$ |
|-|-|-|
| $\tilde{\gamma}_{00} $ | 1 | 1 |
| $\tilde{\gamma}_{10} $ | 0 | 1 |
| $\tilde{\gamma}_{20} $ | 0 | 1 |
| $\tilde{\gamma}_{11} $ | 1 | 1 |
| $\tilde{\gamma}_{21} $ | 0 | 1 |
| $\tilde{\gamma}_{22} $ | 1 | 1 |
| $\tilde{A}_{ij}$ | 0 | 1 |
| $W$ | 1 | 1 |
| $\alpha$ | 1 | $\sqrt{2}$ [see (36)](https://arxiv.org/pdf/gr-qc/0206072) |
| $\beta^i$ | 0 | $1$ in practice, [see (51)](https://arxiv.org/pdf/gr-qc/0206072) |

Its common to use a wave speed of $v=1$ for all variables

Code wise, this is very straightforward:

```c++
//2nd order derivatives, including handling for boundary conditions
valuef diff1_boundary(single_source::buffer<valuef> in, int direction, const valuef& scale, v3i pos, v3i dim)
{
    using namespace single_source;

    v3i offset;
    offset[direction] = 1;

    derivative_data d;
    d.scale = scale;
    d.pos = pos;
    d.dim = dim;

    ///If we're at the boundary, do one sided derivatives
    ///otherwise shell out to diff1, which must handle near boundary points correctly
    mut<valuef> val = declare_mut_e(valuef(0.f));

    value<bool> left = pos[direction] == 1;
    value<bool> right = pos[direction] == dim[direction] - 2;

    if_e(left, [&]{
        as_ref(val) = (-3.f * in[pos, dim] + 4 * in[pos + offset, dim] - in[pos + 2 * offset, dim]) / (2 * scale);
    });

    if_e(right, [&] {
        as_ref(val) = (3.f * in[pos, dim] - 4 * in[pos - offset, dim] + in[pos - 2 * offset, dim]) / (2 * scale);
    });

    if_e(!(left || right), [&]{
        as_ref(val) = diff1(in[pos, dim], direction, d);
    });

    return declare_e(val);
}


auto sommerfeld = [&](execution_context&, buffer<valuef> base, buffer<valuef> in, buffer_mut<valuef> out, literal<valuef> timestep,
                      literal<v3i> ldim,
                      literal<valuef> scale,
                      literal<valuef> wave_speed,
                      literal<valuef> asym,
                      buffer<v3i> positions,
                      literal<valuei> position_num) {

    using namespace single_source;

    valuei lid = value_impl::get_global_id(0);

    pin(lid);

    v3i dim = ldim.get();

    if_e(lid >= position_num.get(), [&] {
        return_e();
    });

    v3i pos = positions[lid];

    pin(pos);

    v3f world_pos = grid_to_world((v3f)pos, dim, scale.get());

    valuef r = world_pos.length();

    auto sommerfeld = [&](single_source::buffer<valuef> f, const valuef& f0, const valuef& v)
    {
        valuef sum = 0;

        for(int i=0; i < 3; i++)
        {
            sum += world_pos[i] * diff1_boundary(f, i, scale.get(), pos, dim);
        }

        return (-sum - (f[pos, dim] - f0)) * (v/r);
    };

    valuef dt_boundary = sommerfeld(in, asym.get(), wave_speed.get());

    //base + dt_boundary * timestep
    as_ref(out[pos, dim]) = apply_evolution(base[pos, dim], dt_boundary, timestep.get());
};
```

Note that the boundary conditions lead to errors that gradually contaminate our simulation, and cause errors. In general, this is often solved by simply placing them far away, but its a problem in longer lived simulations. More complex boundary conditions are a future topic

There are two other boundary conditions we need now as well:

1. Kreiss-Oliger dissipation. In our case, we'll progressively reduce the order of dissipation as we approach the boundary
2. Derivatives. Here, we reduce the order of derivatives as we approach the boundary

## Kreiss-Oliger boundary

Here, we're simply reducing the order of kreiss-oliger as we approach the boundary. The implementation is straightforward:

```c++
valuei boundary_distance = distance_to_boundary(pos, dim);

//no dissipation on the boundary itself
if_e(boundary_distance == 0, [&] {
    as_ref(out[lid]) = in[lid];

    return_e();
});

auto do_kreiss = [&](int order)
{
    as_ref(out[lid]) = in[lid] + eps.get() * kreiss_oliger_interior(in[pos, dim], order);
};

int max_kreiss = 4;

for(int i=1; i < max_kreiss; i++)
{
    if_e(boundary_distance == i, [&] {
        do_kreiss(i * 2);
    });
}

if_e(boundary_distance >= max_kreiss, [&] {
    do_kreiss(max_kreiss * 2);
});
```

And the interior function:

```c++
valuef kreiss_oliger_interior(valuef in, int order)
{
    valuef val = 0;

    for(int i=0; i < 3; i++)
    {
        //these are *not* divided by the scale, as it'll cancel out later
        if(order == 2)
            val += diff2nd(in, i);
        if(order == 4)
            val += diff4th(in, i);
        if(order == 6)
            val += diff6th(in, i);
        if(order == 8)
            val += diff8th(in, i);
        if(order == 10)
            val += diff10th(in, i);
    }

    int n = order;
    float p = n - 1;

    int sign = pow(-1, (p + 3)/2);

    int divisor = pow(2, p+1);

    float prefix = (float)sign / divisor;

    return prefix * val;
}
```

The maximum grid cell offset touched by a derivative is half the derivative rank: Eg the second derivative `diff2nd` has the definition:

```c++
valuef diff2nd(const valuef& in, int idx)
{
    auto vars = get_differentiation_variables<3>(in, idx);

    valuef p1 = vars[0] + vars[2];

    return p1 - 2 * vars[1];
}
```

And touches cells at `position +- 1`, And the 10th derivative `diff10th` has the definition:

```c++
valuef diff10th(const valuef& in, int idx)
{
    auto vars = get_differentiation_variables<11>(in, idx);

    valuef p1 = vars[0] + vars[10];
    valuef p2 = -10.f * vars[1] - 10.f * vars[9];
    valuef p3 = 45.f * vars[2] + 45.f * vars[8];
    valuef p4 = -120.f * vars[3] - 120.f * vars[7];
    valuef p5 = 210.f * vars[4] + 210.f * vars[6];
    valuef p6 = -252.f * vars[5];

    return p1 + p2 + p3 + p4 + p5 + p6;
}
```

and touches cells at `position +- 5`

## Derivative Boundary Conditions

Here, we drop the derivative order (ie the precision) as we approach the boundary

```c++
valuef diff1(const valuef& in, int direction, const derivative_data& d)
{
    valuef second;

    {
        ///4th order derivatives
        std::array<valuef, 5> vars = get_differentiation_variables<5>(in, direction);

        valuef p1 = -vars[4] + vars[0];
        valuef p2 = 8.f * (vars[3] - vars[1]);

        second = (p1 + p2) / (12.f * d.scale);
    }

    valuef first;

    {
        std::array<valuef, 3> vars = get_differentiation_variables<3>(in, direction);

        first = (vars[2] - vars[0]) / (2.f * d.scale);
    }

    valuei width = distance_to_boundary(d.pos[direction], d.dim[direction]);

    return ternary(width >= 2, second, first);
}
```

Note that our diff1 function does not need to deal with derivatives *on* the boundary, as we never evaluate our evolution equations there (which is the role of sommerfeld)

The ternary operator is being used here to avoid a conditional branch on the memory accesses, which is tremendously slow. The simulation grid is padded so that there are no out of memory accesses

# Modifying the equations

We've now solved the initial conditions, and implemented our boundary conditions: so we just use our equations from the last article, and hit go right?

\<iframe width="560" height="315" src="https://www.youtube.com/embed/fnGNOeT2So8?si=Zspf1_cJZCBt4dhL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

(Please note, from now on: All videos show the value of lapse)

Todo: Sad cat picture

Unfortunately, the equations as-is cannot simulate binary black hole collisions, and we now enter a very murky area: Ad hoc modifying the equations, so that we can pull off these simulations correctly

In addition to the 'usual' issues, we also suffer from our own set of unique issues: Rather low grid resolution, 16-bit derivatives, as well as using 32-bit floats instead of 64-bit doubles. The latter is largely a non issue luckily, but it exacerbates numerical issues with the singularity, by giving us a smaller margin for error[^leeway]

Note that all tests are conducted with a grid resolution of $213^3$, encompassing a region with a width of $30$

[^leeway]: The determinant of the metric tensor becomes degenerate on a failure of the boundary conditions, leading to infinities turning up. You can handle more degenerate matrices with double precision, giving the gauge conditions more time to catch up

## What's wrong with the equations?

It took a very *very* long time for the field to be able to successfully simulate binary black hole collisions, and even longer to do full circular orbits. There are a variety of issues here, which we'll now go over

1. Bad gauge conditions can result in coordinate system stretching, as well as the singularity blowing up in the moving punctures technique (well, before it was discovered)
2. Momentum constraint errors
3. Christoffel symbol constraint errors
4. Algebraic constraint errors (which we solved in the last article)

The structure of the equations we're using are already adapted to simulating black holes by design, but older sets of equations were very numerically unstable for these problems. We're skipping well over 40 years worth of research in this article, and its still a very active field

### Gauge conditions

The BSSN equations contain extra degrees of freedom, and one allowance you can make is setting the variable $K=0$. This is called maximal slicing. Maximal slicing defines a coordinate system that is [known](https://arxiv.org/pdf/gr-qc/0206072) to have singularity avoiding tendencies, and is a good candidate for preventing our equations from exploding due to the presence of a singularity

Setting K=0 allows us to solve for the lapse via an [elliptic equation](https://arxiv.org/pdf/gr-qc/0206072) (32). While elliptic equations are tricky to solve with any kind of efficiency, an approximate solution comes in the form of what is called the 1+log gauge - which will drive $K=0$ over time: see [17](https://arxiv.org/pdf/gr-qc/0605030)

$$\partial_t \alpha = -2 \alpha K + \beta^i \partial_i \alpha$$

Additionally, the shift is defined as such [see here](https://arxiv.org/pdf/gr-qc/0605030) (26), in what is called the gamma driver/freezing condition (as it drives the christoffel symbol to $0$):

$$\partial_t \beta^i = \frac{3}{4} \tilde{\Gamma}^i + \beta^j \partial_j \beta^i - \eta \beta^i$$

$\eta$ is a free parameter that represents a gauge damping term: In general it is set to $\frac{2}{M}$, but it can also take fairly arbitrary values from $1$ to $>10$, or even vary spatially or by time. This parameter has a heavy influence on the paths of the black holes

Taken together, these two gauge conditions are called the moving puncture gauge, and are the standard choice for near equal mass[^nearequalmass] binary black hole collisions

[^nearequalmass]: High mass ratio collisions are an unsolved problem. There might be a future article about exploring higher ratios, but we'd likely need adaptive mesh refinement to make a dent in that problem

#### There are many gauge conditions

Here I'm presenting what I'd consider to be the most standard gauge conditions, but even a brief look through the papers linked above will show that there are a *wide* variety of gauge conditions. The most relevant notes here are:

1. The $\beta^k \partial_k$ terms are frequently dropped, as what they compensate for (a zero speed mode) is frequently addressed implicitly via kreiss-oliger dissipation - as it inherently dissipates away zero lapse and shift points
2. There is an alternate two-variable from of the gamma driver equation, which appears to be strictly equivalent, but is expensive for us in terms of performance + memory
3. These gauge conditions only work for relatively equal mass compact object collisions. For cosmological simulations or very unequal mass binaries, you'd again need different gauge conditions - there's no one size fits all condition

### Code

Implementing both of these gauge conditions is straightforward:

```c++
valuef get_dtgA(bssn_args& args, const derivative_data& d)
{
    valuef bmdma = 0;

    for(int i=0; i < 3; i++)
    {
        bmdma += args.gB[i] * diff1(args.gA, i, d);
    }

    ///https://arxiv.org/pdf/gr-qc/0206072
    return -2 * args.gA * args.K + bmdma;
}

tensor<valuef, 3> get_dtgB(bssn_args& args, const derivative_data& d)
{
    ///https://arxiv.org/pdf/gr-qc/0605030 (26)
    tensor<valuef, 3> djbjbi;

    for(int i=0; i < 3; i++)
    {
        valuef sum = 0;

        for(int j=0; j < 3; j++)
        {
            sum += args.gB[j] * diff1(args.gB[i], j, d);
        }

        djbjbi[i] = sum;
    }

    ///gauge damping parameter, commonly set to 2
    float N = 2.f;

    return (3/4.f) * args.cG + djbjbi - N * args.gB;
}
```

The new gauge conditions on their own do not allow our simmulation to live longer, though they are a necessary component

### Momentum constraint errors

Todo: Need to check momentum constraint definition, specifically if its (YA),i or Y (A,i)

These can be damped in exactly the same form as in the previous article, via eg [4.9](https://arxiv.org/pdf/gr-qc/0204002):

$$\partial_t \tilde{A}_{ij} = \partial_t \tilde{A}_{ij} + k_m \alpha \tilde{D}_{(i}\mathcal{M}_{j)}$$

Traditionally, momentum constraint violations tend to produce high frequency, self reinforcing oscillations, resulting in your simulation blowing up. With an implicit integrator this doesn't happen, so momentum constraint damping is more about increasing correctness rather than for stability reasons. This makes it much less important for us in this construction

If you are using something like RK4, you will need this modification for stability reasons. We'll be using it here to increase our correctness

### Christoffel symbol errors

Fixing up this constraint is much more important to the long running of our sim. The traditional damping schemes are as following:

[see (45)](https://arxiv.org/pdf/gr-qc/0209066)

$$
\begin{align}
s &= step(\partial_m \beta^m)\\
\\
\partial_t \alpha &= \partial_t \alpha - (\chi * s + \frac{2}{3}) \mathcal{G}^i \partial_m \beta^m\\
\end{align}
$$

or a similar modification: [(2.22)](https://arxiv.org/pdf/0711.3575v2)

$$
\partial_t \alpha = \partial_t \alpha + k_g \alpha \mathcal{G}^i \\
$$

Where $\chi$ > 0, and $k_g$ < 0. `step` is a function that returns $1$ if its argument is $> 0$, otherwise $0$[^slightlysubtle]. We'll be looking at the former case in this article, but both modifications are similar

[^slightlysubtle]: There are a few possible choices of modification here. The reason for the step function choice, is that the equations are only inherently unstable when $\partial_m \beta^m > 0$, and $\chi$ must have the same sign as that

## Lets try it

With $\chi = 0.5$ and $k_m = 0.04$, and the non standard kreiss-oliger $\epsilon = 0.05$ (more on this later), we get something that looks like this:

\<iframe width="560" height="315" src="https://www.youtube.com/embed/PT2GXno2dMQ?si=oV5KRitGM6XZgq5t" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

While we can play around with the constants a bit to adjust the results, at our resolution and scale, its essentially impossible to get this to work correctly. This kind of error is *very* common

These modifications would, in theory, work if you were using some kind of adaptive mesh refinement scheme with very high accuracy. Unfortunately, the entire point of this article series is to try and do simulations a little faster than that, so instead of relying on more compute power to fix our problems, we're going to see if we can get a little more bang for our buck

## Lets reproduce a result

Up until now I've been presenting this more informally to illustrate the kinds of errors we'll run into, which is why I haven't laid out exactly what we're doing or reproducing

Today we're going to focus on trying to reproduce a specific result: The no spin test case from [this paper](https://arxiv.org/pdf/gr-qc/0610128). If you'd like some more test cases, check out [this](https://arxiv.org/pdf/gr-qc/0602026) paper as well. Here, I'll lay out the test parameters:

| | |BH 0|BH 1|
|-|-|-|-|
|Bare mass|$m$|0.483|0.483|
|Momentum|$P^0$|-0.133|0.133|
|Angular momentum|$S^i$|0|0|
|Position|$x^1$|3.257|-3.257|
|ADM mass|$M$|0.505[^youcanusethis]|0.505|

[^youcanusethis]: This can be used to check the accuracy of your laplacian solver. You should be able to get very close to this value, eg 0.5045

The black holes are expected to collide at roughly T=160, after approximately 1.8 orbits. For our sim, we'll have the following test parameters

```c++
resolution = {213, 213, 213}
simulation_width = 30
h = (simulation_width / (resolution.x - 1)), aka scale
n (the damping parameter) = 2
todo: eps for kreiss-oliger
```

The goal here is to demonstrate specific types of errors, the general approach that you might use to fix them, and to give you an overview of what kinds of issues you're going to run into. Note that until we implement gravitational waves detection, there's little point trying to calibrate our sim *exactly* via the mk 1 eyeball, so we're going to settle for "looks decent, would work better with a higher resolution and some constant tweaks"

Note that the boundaries here are sufficiently close that they'll cause problems quite quickly. Further away boundaries require higher resolutions in general

Now we'll go through how to diagnose some of our simulations problems:

1. Christoffel symbol errors
2. Kreiss-Oliger dissipation
3. Gauge condition issues

### Black Hole Inspiral Failure

Our black holes fail to inspiral correctly. One of the easiest ways to diagnose the cause, is to run the simulation with different values of $\chi$, our christoffel symbol damping term:

$\chi = 0$

\<iframe width="560" height="315" src="https://www.youtube.com/embed/gx7zo_5y_VM?si=UpmWNmzCqiIl4QyP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

$\chi = 0.9$

\<iframe width="560" height="315" src="https://www.youtube.com/embed/GrFBknjdK7Y?si=nkM7MG2VT8-dpiIM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

While we can see that higher damping somewhat improves the situation, its clearly still borked. We need a better christoffel damping scheme

Please note, we are not using the semi-standard analytic christoffel substitution (more on this soon)

#### Christoffel damping schemes

There are a variety of schemes available for improving the christoffel symbols, of which I'm picking a few interesting ones:

| Informal Name | Modification | Notes | Source |
|-|-|-|-|
|Christoffel Substitution|$\tilde{\Gamma}^i = \tilde{\gamma}^{jk} \tilde{\Gamma}^i_{jk}$ | Only where $\tilde{\Gamma}$ is not differentiated. Very widespread in the literature | [After 32](https://arxiv.org/pdf/gr-qc/0206072) |
|Christoffel lapse modification| $\partial_t \tilde{\Gamma}^i = \partial_t \tilde{\Gamma}^i + k_{\tilde{\Gamma}} \alpha \mathcal{G}^i$ | $k_{\tilde{\Gamma}} < 0$ | [2.22](https://arxiv.org/pdf/0711.3575v1) |
|Metric Modification| $\partial_t \tilde{\gamma}_{ij} = \partial_t \tilde{\gamma}_{ij} + k_{\gamma} \alpha \tilde{\gamma}_{k(i} \tilde{D}_{j)} \mathcal{G}^k$ | $k_{\gamma} < 0$ | [2.21](https://arxiv.org/pdf/0711.3575v1), or [table II](https://arxiv.org/pdf/gr-qc/0204002)|

Please note that the informal names are something I've purely jotted down for clarity

##### The christoffel substitution

This is easy enough to implement. There are only 4 instances where this substitution can be applied:

1. In $\mathcal{R}_{ij}$
2. Two in $\partial_t \tilde{\Gamma}^i$
3. $\partial_t \beta_i$

The analytic value is straightforward to calculate:

```c++
auto icY = cY.invert();

single_source::pin(icY);

tensor<valuef, 3> cG_out;

tensor<valuef, 3, 3, 3> christoff2 = christoffel_symbols_2(icY, derivs.dcY);

for(int i=0; i < 3; i++)
{
    valuef sum = 0;

    for(int m=0; m < 3; m++)
    {
        for(int n=0; n < 3; n++)
        {
            sum += icY[m, n] * christoff2[i, m, n];
        }
    }

    cG_out[i] = sum;
}

return cG_out;
```

This modification can help significantly. With $\chi = 0.9$, as well as higher momentum constraint damping ($0.06$), we can achieve our first actual inspiral:

\<iframe width="560" height="315" src="https://www.youtube.com/embed/bSCgNtP4_Nw?si=Z3qPixa1X0K7XjnC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Compare with $\chi = 0.5$

\<iframe width="560" height="315" src="https://www.youtube.com/embed/7yZP8W2j4mc?si=y0mLe_t7ELq7htaG" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Overall, this modification is.. ok. In general, it tends to work well for octant symmetry, and causes problems with unequal mass binaries. Additionally, the increase in momentum constraint damping is a bit suspect, as in other simulations this value has caused the black holes to evaporate

All in all: Not great, not terrible. We won't be using this modification on a permanent basis, but its worth considering testing this with some of the later modifications

##### Christoffel lapse modification

This modification is fairly easy to calculate. $\mathcal{G}^i$ is conventionally defined as:

$$\mathcal{G}^i = \tilde{\Gamma}^i - \tilde{\gamma}^{jk} \tilde{\Gamma}^i_{jk}$$

Implementation wise, we're going to calculate $k_{\tilde{\Gamma}} \alpha \mathcal{G}^i$

```c++
tensor<valuef, 3> calculated_cG;

for(int i=0; i < 3; i++)
{
    valuef sum = 0;

    for(int m=0; m < 3; m++)
    {
        for(int n=0; n < 3; n++)
        {
            sum += icY[m, n] * christoff2[i, m, n];
        }
    }

    calculated_cG[i] = sum;
}

tensor<valuef, 3> Gi = args.cG - calculated_cG;

float lapse_cst = -<SOME_CONSTANT>;

dtcG += lapse_cst * args.gA * Gi;
```

This modification has been a tricky one to try and show off. In general, either the value of lapse_cst is too low (eg $-0.05$), resulting in no discernable change. Or, its too high, resulting in numerical instability. Combining it with and without the other modifications up until this point results in very little happening in general

In general, my experience of this is that its one we can simply discard. I could have ditched this from the article entirely, but this kind of stuff will make up a very large quantity of what you're run into here

##### Metric modification

For this, we'd like to calculate the following quantity:

$$k_{\gamma} \alpha \tilde{\gamma}_{k(i} \tilde{D}_{j)} \mathcal{G}^k$$

Most implementations are unlikely to have the capability to directly perform a numerical derivative on $\mathcal{G}^k$ (and even if you could, its a bad idea for performance). Lets break this up a bit:

One of the easiest ways to fix this is to perform all your derivatives, and covariant derivatives with dual numbers. While you could work it out precisely by hand (and its likely there'd be a performance benfit to doing so as some terms cancel out), it works reasonably well

Todo: BiG_lo *is* faster, because we can use christoff1 instead of chistoff2, and christoff1 requires one less metric invert

This does however make the implementation a bit of a mess:

```c++
tensor<valuef, 3, 3> d_cGi;

//this loop purely deals with calculating the derivatives of Gi
//we already know that dGi = d(cGi - analytic value) = dcGi - d(analytic value)
//m is the differentiation direction, and the first index is always the derivative
for(int m=0; m < 3; m++)
{
    //derivatives of metric tensor
    tensor<dual<valuef>, 3, 3, 3> d_dcYij;

    //metric tensor
    unit_metric<dual<valuef>, 3, 3> d_cYij;

    //fill in metric + its dual components
    for(int i=0; i < 3; i++)
    {
        for(int j=0; j < 3; j++)
        {
            d_cYij[i, j].real = args.cY[i, j];
            d_cYij[i, j].dual = derivs.dcY[m, i, j];
        }
    }

    pin(d_cYij);

    //metric inverse, as a dual
    auto dicY = d_cYij.invert();

    pin(dicY);

    //fill in the metric derivatives + its dual components (the first derivatives of the metric derivatives, ie second derivatives)
    for(int k=0; k < 3; k++)
    {
        for(int i=0; i < 3; i++)
        {
            for(int j=0; j < 3; j++)
            {
                d_dcYij[k, i, j].real = derivs.dcY[k, i, j];
                d_dcYij[k, i, j].dual = diff1(derivs.dcY[k, i, j], m, d);
            }
        }
    }

    pin(d_dcYij);

    //differentiate christoffel symbols
    auto d_christoff2 = christoffel_symbols_2(dicY, d_dcYij);

    pin(d_christoff2);

    tensor<dual<valuef>, 3> dcGi_G;

    for(int i=0; i < 3; i++)
    {
        dual<valuef> sum = 0;

        for(int j=0; j < 3; j++)
        {
            for(int k=0; k < 3; k++)
            {
                sum += dicY[j, k] * d_christoff2[i, j, k];
            }
        }

        //now we have the analytic derivative of the analytic christoffel symbol
        dcGi_G[i] = sum;
    }

    pin(dcGi_G);

    for(int i=0; i < 3; i++)
    {
        d_cGi[m, i] = diff1(args.cG[i], m, d) - dcGi_G[i].dual;
    }
}

auto christoff2 = christoffel_symbols_2(icY, derivs.dcY);

tensor<valuef, 3> calculated_cG;

for(int i=0; i < 3; i++)
{
    valuef sum = 0;

    for(int m=0; m < 3; m++)
    {
        for(int n=0; n < 3; n++)
        {
            sum += icY[m, n] * christoff2[i, m, n];
        }
    }

    calculated_cG[i] = sum;
}

pin(christoff2);

tensor<valuef, 3> Gi = args.cG - calculated_cG;

tensor<valuef, 3, 3> cD = covariant_derivative_high_vec(Gi, d_cGi, christoff2);

pin(cD);

for(int i=0; i < 3; i++)
{
    for(int j=0; j < 3; j++)
    {
        valuef sum = 0;

        for(int k=0; k < 3; k++)
        {
            sum += 0.5f * (args.cY[k, i] * cD[k, j] + args.cY[k, j] * cD[k, i]);
        }

        float cK = -<SOME_CONSTANT>;

        dtcY.idx(i, j) += cK * args.gA * sum;
    }
}
```

Notes to self: This segment is just for me

$$
\begin{align}
&k_{\gamma} \alpha \tilde{\gamma}_{k(i} \tilde{D}_{j)} \mathcal{G}^k\\
&k_{\gamma} \alpha \frac{1}{2} (\tilde{\gamma}_{ki} \tilde{D}_{j} \mathcal{G}^k + \tilde{\gamma}_{kj} \tilde{D}_{i} \mathcal{G}^k) \\
&\tilde{\gamma}_{ka} \tilde{D}_{b} \mathcal{G}^k = \tilde{\gamma}_{ka} (\partial_b \mathcal{G}^k + \tilde{\Gamma}^k_{bc}\mathcal{G}^c) |\\
&= \tilde{D}_{b} \mathcal{G}_a\\
&\mathcal{G}^i = \tilde{\Gamma}^i - \tilde{\gamma}^{jk} \tilde{\Gamma}^i_{jk}\\
&\mathcal{G}_i = \tilde{\Gamma}_i - \tilde{\gamma}^{jk} \tilde{\Gamma}_{ijk}\\
\end{align}
$$

With the constant set to $-1$, we get this:

\<iframe width="560" height="315" src="https://www.youtube.com/embed/Ii7TIueUBD8?si=3x7QWgtzRJ3DWScl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

This is clearly quite dissipative. Every scheme we have currently is either too dissipative, or fails to circularly inspiral in a sane way. This early inspiral is currently caused by our low grid resolution: once we've allievated that, we'll play around with the constants a bit and fine tune everything, to see if we can get relatively close to our target merge time. We're going to be leaving this new modification on in future tests, as it works well in my experience

In general, there's a bit of a balance between the different modifications. I'm trying to avoid hopping back and forth between different techniques, or repeatedly fine tuning before we've put all the techniques together as much as possible, but you can see how its tricky

#### Slow start lapse

The initial conditions we have aren't actually for black holes, they're for objects that collapse into black holes. This results in a lot of initial gauge jiggling as they collapse into black holes - particularly for the lapse - that can be bad for the health of the simulation. Particularly because we have a low resolution and bad boundary conditions, this tends to lead to a lot of errors in the initial phase of the simulation, which can make it operate more poorly

One solution for this is the so called slow start lapse, where you essentially ease the lapse in more gradually over a period of time, eliminating a huge initial collapse in the lapse

As the gauge conditions are arbitrary, this does not (in theory) alter the correctness of the sim. The implementation of this is given by [(27)](https://arxiv.org/pdf/2404.01137)

$$\partial_t \alpha = \partial_t \alpha - W (h e^{-\frac{t^2}{2 \sigma^2}}) (\alpha - W)$$

Where $h$ is a scaling factor for the gaussian, $t$ is our elapsed simulation time, and $\sigma$ is a damping timescale. Here, $h = \frac{3}{5}$, and $\sigma = 20$

With this implemented, we get this result:

\<iframe width="560" height="315" src="https://www.youtube.com/embed/G2BuUYI2RfY?si=pShkwJcdFWtGGLJc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Which is slightly better. Testing $h=1$, we get this:

\<iframe width="560" height="315" src="https://www.youtube.com/embed/t3reNTQdfxk?si=MA-SMO1PSCW8LYOK" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Which is very interesting. In general, this modification is a free win, so we'll be leaving this on as well (with $h=\frac{3}{5}$, until we fine tune). Its likely that this modification makes a significant difference for two reasons

1. Our low resolution exacerbates numerical errors caused by the initial collapse in the lapse
2. A large initial lapse pulse reflects off the close boundaries, causing further errors

Do note, due to the nature of inspiral, as our modifications improve the situation it can result in disproportionately large differences in the end merger time despite what you might notionally consider a linear improvement in quality

#### Curvature adjusted kreiss-oliger dissipation (CAKO)

If you remember, kreiss-oliger is a method of removing high frequency oscillations from your grid, and it damps out numerical errors. The basic idea with our next modification [here, (20)](https://arxiv.org/pdf/2404.01137) is that sharp features near the event horizon of a black hole are likely natural, and require less smoothing. This makes a certain amount of sense, and after all - if we're trying to represent as much information on our grid as possible, removing high frequencies in high frequency areas seems like a prime candidate for wasting grid resolution

To adjust this, the authors multiply the damping strength, by the parameter $W$, thus making the damping strength virtually 0 near the event horizon

\<iframe width="560" height="315" src="https://www.youtube.com/embed/U9lUl0QVVE0?si=cHefuAscEsUY5wOc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Testing this directly doesn't work. I suspect that our code is too low resolution, and there are signs of numerical error - prior experience on my end is that the black holes are likely obviously borked visually, we just don't have the debugging tools to see it. Additionally, a full 0 damp method is liable to generate singularity explosions. For use do need some damping near the black holes, and instead I clamp the minimum damping to $max(W, \epsilon)$

In practice, I've fond that this $\epsilon$ parameter is very dependent on grid resolution, and can be anywhere from 0.1 to 0.5, with lower resolutions needing higher values

Implementing this is extremely straightforward, and our kreiss-oliger step becomes:

```c++
as_ref(out[lid]) = in[lid] + eps.get() * kreiss_oliger_interior(in[pos, dim], order) * max(W[lid], valuef(0.5));
```

This gives us a result that looks like this:

\<iframe width="560" height="315" src="https://www.youtube.com/embed/xzyrHY0W2lY?si=oQNw7aKDIE-7GFc5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Now making it a lot further than previously. One feature to note is that: With very reduced kreiss oliger dissipation, we're now much more vulnerable to 0 speed modes (which are no longer dissipated away), and so you want to ensure you're using the advecting lapse and shift conditions for the gauge

### Fine tuning

So far, we've got a collection of different modifications, some of which have different parameters. The ones we're potentially going to adjust are:

| Symbol | Description | Current value |
|-|-|-|
| $\epsilon$| Minimum kreiss-oliger strength factor | $0.5$ |
| $\chi$ | Damping strength for the $\tilde{\Gamma}^i$ modification | $0.5$ |
| $h$ | Strength of the initial lapse damping | $\frac{3}{5}$ |
| $k_{\gamma}$ | Strength of the $\tilde{\gamma}_{ij}$ christoffel damping | $-0.1$ |

There will now be a brief intermission

Todo: Cat picture

So, with the following parameters:

| Symbol | Description | Current value |
|-|-|-|
| $\epsilon$| Minimum kreiss-oliger strength factor | $0.5$ |
| $\chi$ | Damping strength for the $\tilde{\Gamma}^i$ modification | $0.55$ |
| $h$ | Strength of the initial lapse damping | $\frac{4}{5}$ |
| $k_{\gamma}$ | Strength of the $\tilde{\gamma}_{ij}$ christoffel damping | $-0.09$ |

We get this:

\<iframe width="560" height="315" src="https://www.youtube.com/embed/HtiloPOk2vA?si=mS6oJDJ1sGEyAhsk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Which is pretty neat. Further tuning beyond this is a bit pointless because we have no real metrics to actual validate that we're making it more accurate, which is beyond the scope of this specific article

# Results

With all of this together, we're able to replicate - close enough - the original paper, and we have now smashed black holes together pretty successfully. There are still clearly some inaccuracies, and the fine tuning nature of the damping constants is a bit concerning, but it works reasonable well for such a low resolution. The inspiral looks smooth and monotonic (ie no sharp jumps), which is ideal. It isn't scientific grade by any means, but it should be a solid base for getting there in a future article

# Discussion

Overall, trying to get black hole paths to replicably follow sane trajectories has occupied a pretty major part of what I've worked on in general, and I've spent a significant amount of time on characterising this issue. The moving punctures technique as a whole is slightly sketchy numerically: as mentioned previously, the puncture is a true discontinuity, and differentiating across it is invalid. This (in theory) is a major source of constraint violations, though I've seen suggestions that the event horizon itself is also a source of constraint violations

In the literature, there are *many* ad-hoc techniques used with respect to the puncture, including:

1. Dropping the order of kreiss oliger dissipation within the event horizon
2. Dropping the order of differentiation within the event horizon
3. Using upwinded stencils for differentiation with respect to the shift, which is very dissipative near the singularity where the shift vector undergoes a discontinuity
4. Hiding the puncture, by placing it at the boundary between grid cells

A lot of this seems like that any kind of differentiation is invalid near the puncture, and so techniques indirectly tend to lower the constraint contamination by reducing the singularity, or reducing the size of stencils to reduce constraint error propagation. In theory - some BSSN formulations have causal constraint propagation, but this breaks down *somewhat* in the face of discretisation, and still relies on you having a sufficiently large event horizon

It be intresting to explore an excision technique instead, and see how it affects the inspiralling and constraint violation behaviour. Its on the todo list

# Misc Notes

This section contains pieces of information which are useful to know, but don't really fit into the structure of the article here

## Black holes do not really have mass

There's an unfortunate truth for black holes, which is that there is no unique definition of the mass of a black hole. Its very easy to work out how much mass there is for something like the earth - because we can calculate how much *matter* is present, but black holes here are a vacuum solution - there's no matter of any description in our spacetime. This means that we have to work out what the effect on spacetime a black hole has equivalent to a certain amount of mass

There are multiple definitions of mass that we can use:

1. Bare mass, which has no physical meaning. Higher = more mass, but that's about it
2. ADM mass, which is measured at 'infinity', and measures how much energy is present in the spacetime. Only valid in asymptotically flat spacetimes
3. Komar mass - defined in stationary spacetimes
4. Horizon mass, determined by the area of the event horizon
5. Puncture mass, determined by the size of the correction factor $\psi$ at the puncture, which approximates the ADM mass

Black holes are a form of energy stored in spacetime and are a global phenomenon. Two black holes which are nearby each other store a significant amount of energy in the spacetime between them, increasing their mass (essentially further delocalising them). This makes it hard to pin down exactly how much 'mass' an individual black hole has in a binary pair, because they aren't truly separate phenonema, nor are they truly local phenomena. This means that all of these definitions of mass can disagree

Despite this, its very common to use ADM mass via puncture mass[^internallyflat], and horizon mass with an approximation to its event horizon. We'll be using the former

[^internallyflat]: The asymptotically flat infinity here is the puncture *inside* the black hole. I have no idea how this works, but apparently it does

## Event horizon calculation

Technically, the event horizon is *4d* surface that lightrays cannot escape from. In the literature, it is very common to talk about event horizons, and the size of the event horizon etc, but it is very rare to actually calculate what is actually technically the event horizon

In GR, the only true way to find an event horizon is to trace geodesics around, and see where they become trapped. In our numerical relativity simulation, you can imagine tracing rays backwards in time from the camera as a visualisation, and in theory our rays will become trapped on any event horizons. We could plot when and where they get trapped,and use that to define a 4-surface. There's a small problem though:

There are no true event horizons in our simulation. The issue is twofold:

1. The strict definition of an event horizon requires full knowledge of the future of the black hole
2. The strict definition of an event horizon requires full knowledge of the past of the black hole

Interestingly, because our black holes are not eternal and form from the collapse of a black hole with no horizon, rays fired backwards in time only become briefly suspended on a horizon, and then quite happily escape again. We also clearly cannot simulate an infinite amount of time into the future. We can however approximate a strict event horizon calculation by defining a cutoff time both in the past, and in the future, where we suspect that a horizon has formed, and use that as our technical horizon surface

### Marginally Outer Trapped Surface (MOTS)

Event horizon calculation requires storing the entire history of the simulation and tracing geodesics[^wewilldothis], so it is uncommon as an approach. In the literature, a surface which is hoped to be equivalent/similar is used called the 'marginally outer trapped surface' (MOTS). This is a purely local quantity defined on a specific slice of spacetime. When calculating the black hole's mass via horizon mass, the horizon area is generally calculated via the MOTS

[^wewilldothis]: We're actually going to do this in a future article, in realtime, to produce accurate renderings of binary black hole collisions

## Kreiss oliger is a bit.. ill defined

One of the difficulties with all of this is trying to prove that your code converges. The idea is, as your grid resolution goes up, your code should become more accurate

The tricky part is that while we use scalar constants for damping factors, many of these factors are actually *not* resolution independent, and should really scale with some factor of the grid resolution

Kreiss-oliger is something that stood out to me as a candidate for making convergence tricky. If we examine the specific form of kreiss oliger, you'll notice that for eg second order kreiss, the form is as such:

todo

This involves a single division by the scale. So as your grid resolution goes down, the prefix term blows up to infinity. What gives?

### Courant Factors

If you're unfamiliar with the concept of a courant factor, the core concept is that the timestep of any simulation is limited by what's called a courant factor. It is essentially the statement that information can only propagate across your grid at a finite speed, and is mathematically defined as $\frac{\Delta t}{\Delta h} = C$. If you increase your timestep too high, you must have instability, because you're trying to pipe information through your grid faster than your problem allows it to propagate. Similarly, if you change your grid resolution, you must also change your timestep to match

If you compare, the prefix factor of kreiss-oliger *is* the courant factor, and is therefore a constant. It would certainly sound sensible to replace it with a constant, except...

### We don't have a courant factor

I missed part of the definition of the courant factor, which is that it only applies to *explicit* integrators. Equations which are implicitly integrated do not have a courant factor, as they propagate information faster than with an explicit integrator. In fact, $\frac{\Delta t}{\Delta h}$ can be arbitrarily large, and is only limited by the convergence of your implicit integration technique. With an implicit integrator of sufficient quality - grid resolution, and timestep, are fully decoupled

This raises a tricky question: If we change the size of our grid resolution to being infinitely small, our timestep doesn't necessarily have to change beyond practical convergence limitations. This means that kreiss oliger is somewhat ill defined for us, as there's no reason that the prefix term is fixed to anything useful

To work around these, we're going to set it to a constant, and emulate having a courant factor by calculating our timestep as if we had one. Bear in mind that this is fairly arbitrary, and I suspect is why other people's kreiss-oliger settings do not directly work in our code. Unfortunately, this is the only simulation I've seen which uses fully implicit integration, so I've seen exactly 0 discussion of this issue in the literature[^imexstepping]

[^imexstepping]: The SXS group use imex stepping, which is a semi implicit scheme. They split the equations up, and solve them partially implicitly. Notably however, they do not solve the derivatives implicitly due to the computational cost, which means that this implicit scheme cannot sidestep the courant factor as with a fully implicit scheme - I suspect this is why they see only marginal benefits

## A brief evidence-free survey of modifications

In this article, I've picked a few modifications to implement to the equations, and these are based off a reasonable amount of testing. In general, there are a very diverse range of modifications that are available, and if you're planning on testing some of them, this segment is a very brief overview of my own experiences with them. There's no data of any description here, this is intended purely so that if you test out modifications yourself and notice something, you can take a look and go "ok neat its not just me"

Momentum constraint damping: Good for explicit integrators, makes a small difference to implicitly integrated binary black hole collisions. Too high damping can result in black holes traveling in straight lines

Hamiltonian constraint damping: Makes a relatively small difference

Christoffel damping: Super important in general. Too low damping means that black holes blow up into an odd triangular shape, too high damping prevents inspiral

The trace free $$\tilde{A}$$ constraint: Critically important, slightly more robust to enforce it rather than damp it. Not damping results in instant explosions

The trace free $$\tilde{\gamma}$$ constraint: Relatively unimportant

### Specific modifications

$\sigma$ christoffel damping: This is useful but too limited for our purposes

The $\tilde{\gamma}$ modification used in this article: It may present some problems for hydrodynamic collapses (NaNs), but this is a low evidence observation

The $\tilde{\Gamma}$ modification used in this article: It seems to work well, but the source paper presents no derivation which is not ideal

Substituting undifferentiated $$\tilde{\Gamma}^i$$ terms for their analytic equivalent: From a theoretical perspective, its worth noting that only the derivatives of $$\tilde{\Gamma}^i$$ are numerically unstable, but its very ad-hoc. Absolutely no papers mention which specific terms they substitute (and there are many ambiguities), so its a very sketchy modification in general. I've found myself that it can exacerbate numerical stability problems (while improving the christoffel symbol constraint). It has been said that this modification only works in symmetric equal mass collisions. In general, it seems to be more problematic than its worth

Upwinded derivatives: These can remove the need for kreiss-oliger dissipation, but my best guess is that its because they're actually very dissipative numerically. Its basically doing the same thing as kreiss-oliger (dissipating noise), but without the controllability or more robust theoretical background of kreiss-oliger. I've never had especially good results, and there is a discontinuity with $\beta^i$ at the singularity which means that upwinded derivatives may simply be numerically dissipating it

Reducing derivative order near the singularity: This appears to better preserve energy at low resolutions, but this is not something I've robustly tested

Various gauge conditions: Other than the damping parameter, I've never noticed much difference between these. Advection is commonly disabled, and can cause extra dissipation when enabled, but its somewhat counteracted here by CAKO

The gauge damping parameter $N$: This is a fairly critical one and has a strong influence on the black hole's path. Its commonly set to twice the adm mass of the system or $2$, but it is *very* arbitrary. Higher values = less accurate, in theory

## Other boundary conditions

There are alternatives to sommerfeld which I will mention in passing

1. Fixing every field to its asymptotic value at the boundary, and not evolving it. This works if your boundary is far away (as spacetime is asmptotically flat), but requires a lot of extra simulation space
2. Sponge constructions. This damps outgoing waves gradually. Its easy to implement, but the sponge area must be large enough to avoid reflections, which also requires a lot of extra simulation space
3. Constraint preserving boundary conditions. These are the best and allow you to place the boundaries closest to your simulation area (as they introduce the least error), but are more complex to implement. Its on my infinitely long todo list, and there'll likely be an article on this in the future
4. Hybrid schemes. Some schemes assume that $\beta^i = 0$ on the boundary, and evolve $\tilde{\gamma}_{ij}$ freely, then use traditional boundary conditions for the other variables. I've never tested this and have no information on the quality of the result, but it seems to work
5. Compactification. This adjusts the coordinate system so that the grid represents the entire computational domain, and you have no boundary. This is nice theoretically, but in practice seems to be similar to a sponge construction

As a general note, its very common to assume that the speed of the gauge waves is $1$, instead of $\sqrt{2}$

### Paper review

#### Improved fast-rotating black hole evolution simulations with modified Baumgarte-Shapiro-Shibata-Nakamura formulation

[Link](https://arxiv.org/pdf/1507.00570)

(1) has never worked for me even slightly

(5) has issues, but can be reasonably effective if $\beta_i$ is lowered with the conformal metric, instead of the full metric (ie there's an extra factor of $w^2$). Its somewhat problematic overall as a modification, and is generally worse than the $\gamma_{ij}$ modification we use in this article I've found

(6) is very heavily dissipative, and does not improve the quality of any result that I've tested

(10) and (11) only bring benefit for CPU simulations, and have no real performance benefit on a GPU due to the extra code complexity. Given the instability prsent, its not worth using

#### Modifications for numerical stability of black hole evolution

[Link](https://arxiv.org/pdf/1205.5111). Same authors as above

(61) does not work particularly well

#### Advantages of modified ADM formulation: constraint propagation analysis of Baumgarte-Shapiro-Shibata-Nakamura system

[Link](https://arxiv.org/pdf/gr-qc/0204002)

This paper contains a wealth of useful ideas. 4.11 is too expensive to use. The modifications invlving the traces $\mathcal{A}$ and $\mathcal{S}$ are not using. $\kappa \alpha \mathcal{G}^i$ does not unfortunately make a better $\sigma$ modification. The undifferentiated hamiltonian damping terms do work reasonably well in this paper. The simpler forms of the christoffel damping don't work all that well unfortunately

This paper presents a strong theoretical background for many of the modifications that are used that can often seem very ad-hoc, so its worth a read

#### Notes on formalisms

The conformal covariant BSSN formalism is in general less stable than non covariant BSSN

ccz4 and other constraint damped formalisms don't *really* help all that much with our problems here. They're also a lot more hungry in terms of performance, which makes them not a silver bullet in my experience