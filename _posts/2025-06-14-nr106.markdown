---
layout: post
title:  "Numerical Relativity 106: There are some particles and we're going to simulate them innit"
date:   2025-06-14 12:33:23 +0000
categories: C++
published: false
---

Hi! Today we'll be examining how to do N body particle dynamics in numerical relativity. While newtonian N body simulations are quite straightforward, general relativity as always tends to complicate things just a little.  I'll be presenting you with the most straightforward way to get ~10 million particles working at a decent clip, as well as investigating some critical mistakes to avoid

In general, I try and keep a reasonable flow to these articles. Because they're oriented around a solution to an overarching problem rather than a specific technical issue - they often meander between different topics

That is more true than ever for this article. There are a variety of critical but entirely unrelated problems to solve and it'll get downright wacky in places - so I'm presenting this as a choose your own adventure. They'll be listed in-order of how you should tackle them if you're implementing this

I'll be dealing with the following topics:

1. The C++ spec defect around floating point, and the bugs it causes in real world code. We'll uncover a bug in a major scientific toolkit here, how you should adjust your code to compensate for how floats work, and why fixing this is important
2. The equations we'll be using for this article, and checking out why some alternative approaches aren't that great
3. The deep joys of the dirac delta function, and how to implement it correctly
4. Initial conditions
5. Integration, and a guide on implementation techniques
6. Some miscellaneous performance techniques

At some point we'll be converting a float to a double, back to a float, and then finally to a long. Its just that kind of day

# C++ is sometimes a very poor numerical language

In C++, it is impossible to implement a type that behaves exactly like a `float`. This was a surprising fact to me when I discovered it - for a language that sells itself as having zero overhead, this is a strange omission. To take a simple example, let's have a look at the following two pieces of code:

```c++
float func_1(float a, float b, float c)
{
    return a * b + c;
}

float func_2(float a, float b, float c)
{
    float mul = a*b;
    return mul + c;
}
```

I suspect that most people are not aware that these two pieces of code are strictly non equivalent in C/C++. This isn't a theoretical problem either, if you compile these two functions you'll discover that they really do generate different assembly, and return different results. This isn't an IEEE float thing, this is just pure C++-itus

C++ allows *expressions* to have what is called floating point contraction applied to them. This means that `a * b + c` is legally allowed to be transformed to `fma(a, b, c)`, and evaluated in higher precision implicitly. Because in `func_2` they're two separate expressions, this optimisation is not allowed. This might seem like a minor problem - most of the time we don't actually care - but today its going to cause us severe problems

## Exactly reproducible floating point results

Lets examine the test setup we'll be using today. The specific nature of the particle dynamics isn't important yet:

todo: picture with red arrows

These particles will have roughly circular orbits around each other. You'll notice that there's an implicit symmetry here: if we give the particles exactly equal but opposite velocities, the position of each particle is a reflection. For example, if we have two x coordinates $x_1$ and $x_2$, and our grid size is $d_x$, $x_1 = d_x - x_2 - 1$

In virtually all simulations, this will fail to hold almost immediately. In fact, over time, it'll start to not hold rather dramatically

Todo: Video

Intuitively, exactly symmetric initial conditions should hold for all time. The fact that our simulation breaks down like this is rather worrying - orbital mechanics are chaotic, and so errors compound rather than damp away. A small asymmetry turns into a large asymmetry, which is a clear symptom of a systematic error in our simulation that we want to eliminate

## Sources of asymmetry

Tracking this down can be a pain, and this article is going to be very pedantic about IEEE. Before we even get to the particle dynamics, there are three major sources of asymmetry that can perturb a simulation:

1. Derivative calculations
2. Kreiss-Oliger
3. The initial conditions laplace solver

To illustrate the problem, we're using 4th order derivatives. A seemingly sane sensible person implements it something like this:

```c++
float diff(const std::vector<float>& points, int idx, float scale)
{
    return (points[idx - 2] - 8 * points[idx - 1] + 8 * points[idx + 1] - points[idx + 2]) / (12 * scale);
}
```

How this could introduce asymmetry is straightforward: compilers interpret these expressions left to right, and so we get the following expression:

```c++
(((points[idx - 2] - 8 * points[idx - 1]) + 8 * points[idx + 1]) - points[idx + 2])
```

The rounding here is asymmetric with respect to `idx`. If we have a central grid point about which our data is mirrored, say $10$, we want the evaluation of $idx = 9$ and $idx = 11$ to return *exactly* the same results. Plugging that in, we get the following:

```c++
float diff9(const std::vector<float>& points, float scale)
{
    return (((points[7] - 8 * points[8]) + 8 * points[10]) - points[11]) / (12 * scale);
}

float diff11(const std::vector<float>& points, float scale)
{
    return (((points[9] - 8 * points[10]) + 8 * points[12]) - points[13]) / (12 * scale);
}
```

Lets substitute in the mirrored values into `diff11` to check our algorithm:

```c++
float diff11(const std::vector<float>& points, float scale)
{
    return (((points[11] - 8 * points[10]) + 8 * points[8]) - points[7]) / (12 * scale);
}
```

The rounding is done in a different order compared to `diff9`, which is one reason why we'll get different results on both sides of the grid

### Fixing this

The most logical way to fix this is to group equidistant grid cells:

```c++
float diff(const std::vector<float>& points, int idx, float scale)
{
    return ((points[idx - 2] - points[idx + 2]) + (-8 * points[idx - 1] + 8 * points[idx + 1])) / (12 * scale);
}
```

Substituting this through will show that its all hunky dory. The only problem now is that we have to deal with C++-itus, as the right hand side will get transformed to `fma(-8, points[idx-1], 8 * points[idx + 1])`. So instead of the simple solution, the *actually* correct version of this is as follows:

```c++
float diff(const std::vector<float>& points, int idx, float scale)
{
    float vm2 = points[idx - 2];
    float vp2 = -points[idx + 2];
    float vm1 = -8 * points[idx - 1];
    float vp1 = 8 * points[idx + 1];

    float v2 = lm2 + lp2;
    float v1 = vm1 + vp1;
    float top = v1 + v2;

    return top / (12 * scale);
}
```

Kreiss-Oliger is fixed in precisely the same fashion, by grouping equidistant derivatives with respect to `idx`. The more complex case is the initial conditions

## Floating point symmetric interpolation

During our initial conditions, we have to implement linear interpolation for the upscaling step - and you might think to reach for `std::lerp`. If you go read through the spec, you'll discover that a lot of work was put into it to make it work well. Unfortunately, it is inappropriate for our use case: it does not guarantee the following property:

```c++
std::lerp(a, b, t) = std::lerp(b, a, 1-t)
```

I know this seems like we're messing about in the weeds, but these errors compound over time. The correct way in theory to implement *exactly* symmetric linear interpolation for us is as follows:

```c++
float interpolate(float a, float b, float t)
{
    return (1-t) * a + (1-(1-t)) * b;
}
```

For this to be actually correct we have to work around floating point contraction:

```c++
float interpolate(float a, float b, float t)
{
    float imx = 1-t;
    float imimx = 1-imx;

    float p1 = imx * a;
    float p2 = imimx * b;

    return p1 + p2;
}
```

The laplacian also suffers from the same problem as Kreiss-Oliger and the derivatives, and can be solved in the same fashion. You should also be aware: for a value $x$ representing a grid cell which can have a fractional component, the value $d_x - x - 1$ likely cannot be represented exactly enough. You'll either have to split your values into an integral component representing the cell, and a fractional component representing the offset within it - or store your floats in the range $[-(d_x-1)/2, (d_x-1)/2]$ (for odd grid cell sizes) instead of the range $[0, d_x - 1]$ as is more traditional

## Takeaways

This is my least favourite part of the C++ spec, as it severely complicates a lot of code. While some compilers support simply disabling FP contraction, its hugely important for performance on the GPU in some cases. Importantly, CUDA **does not** implement the pragma required (as it is not officially part of the C++ spec), and GPU compilers love FMAs

This issue causes bugs in real-world code, eg [here's](https://github.com/GRTLCollaboration/GRChombo/blob/130140afe972701575740685343ea28f6ab8234c/Source/BoxUtils/FourthOrderDerivatives.hpp#L42) one example of a numerical relativity toolkit which falls into the double whammy trap of both incorrect ordering of operations, and floating point contraction. The joys!

I have never seen a single discussion of this anywhere. There are some extremely cryptic references to this problem in the literature: many simulations run in a symmetry mode, where the simulation grid is chopped in half (or 8ths) and implicitly mirrored to cut down on simulation costs - there are reports that the simulation for unknown reasons is more stable when running in a mirrored mode, vs a non mirrored mode (which is presumably implicitly symmetric). You can probably see that a systematic asymmetry being applied over a long running simulation has the potential to cause an upset

The code bundled with this article has been updated and checked to make sure this problem isn't present

# Equations

Its time to change tack, and go check out the equations we'll be using today. There are three sets of equations to pick from:

1. The [ADM geodesic equations (12-13)](https://arxiv.org/pdf/1611.07906), which you may remember from the rendering article
2. A scheme based on [4-momentum (2.27)](https://arxiv.org/pdf/1904.07841)
3. A scheme based on the [4-velocity (2.9)](https://scispace.com/pdf/3d-numerical-simulation-of-black-hole-formation-using-43u5c2u959.pdf)

The third formalism is by far the best and that's what we'll be using, for the following reasons:

1. It does not involve the extrinsic curvature $\tilde{A}_{ij}$ compared to #1, which is hugely good for performance. There are also only three evolution equations vs 4
2. The 4-momentum is a poor choice of evolution variable. We'll be dealing with particles with extremely tiny masses, so with an extremely small 4-momentum, the velocity will be very inaccurate

Given a 4-velocity $u^\mu$, the spatial components $u_i$ are evolved as follows:

$$\begin{align}
\frac{d u_i}{dt} &= - \alpha u^0 \partial_i \alpha + u_j \partial_i \beta^j - \frac{u_j u_k}{2 u^0} \partial_i \gamma^{jk}\\
u^0 &= \frac{\sqrt{1 + \gamma^{ij} u_i u_j}}{\alpha}\\
\end{align}
$$

The particle's position is evolved as follows (via $\frac{dx^i}{dt} = \frac{u^i}{u^0}$):

$$
\frac{dx^i}{dt} = -\beta^i + \frac{\gamma^{ij} u_j}{u^0}
$$

This formalism  also has the advantage of being very simple, and is derived directly from the geodesic equation: $u^\mu \nabla_\mu u_\nu = 0$. So far, so good

Note: You'll need to interpolate the quantities above based on the position $x^i$, using the symmetric interpolation we used earlier

## Source terms

Matter interacts with the metric in general relativity via the stress-energy tensor. The 4d stress energy tensor for a single particle labelled by $\mathrm{idx}$ is (2.5):

$$
T_{\mu\nu}^{\mathrm{idx}} = m_{\mathrm{idx}} \alpha^{-1} W^3 \delta^{(3)}(x^i - x_{\mathrm{idx}}^i)\frac{u_\mu^\mathrm{idx} u_\nu^\mathrm{idx}}{u_\mathrm{idx}^0}\\
$$

You simply sum the stress energy tensors for every particle, ie $$T_{\mu\nu}^{\mathrm{full}} = \sum^N_{\mathrm{idx} = 1} T_{\mu\nu}^{\mathrm{idx}}$$

|Symbol | Meaning |
|-|-|
|$m_{\mathrm{idx}}$ | The rest mass of a particle |
|$\alpha$ | Lapse, evaluated at $x^i$ |
| $W$ | Conformal factor, evaluated at $x^i$ |
| $\delta^{(3)}(x)$ | The 3d dirac delta function. This a very non trivial term, and there's a whole section dedicated just to this |
| $x^i$ | The current coordinate position you're looking at |
| $x^i_{\mathrm{idx}}$ | The position of the particle |
| $u^{\mathrm{idx}}_\mu$ | The 4-velocity of a particle, lowered |
| $u^0_{\mathrm{idx}}$ | The 0th component of the 4-velocity of a particle, which is additionally the Lorentz factor. Calculating this must use interpolated quantities at $x^{i}_{\mathrm{idx}}$, not $x^i$ |

### Projected source terms

We can't directly plug the stress energy tensor in however, as we need the ADM projections of the stress energy tensor $T^{\mathrm{idx}}_{\mu\nu}$. These are (2.14, 2.15, 2.20):

$$
\begin{align}
\rho_H &= m u^0 \alpha W^3 \delta^{(3)}(x^i - x^i_{\mathrm{idx}})\\
S_i &= m u_i W^3 \delta^{(3)}(x^i - x^i_{\mathrm{idx}})\\
S_{ij} &= m \frac{u_iu_j}{u^0} \alpha^{-1} W^3 \delta^{(3)}(x^i - x^i_{\mathrm{idx}})\\
\end{align}
$$

These are summed across all particles

#### An oddity in the literature

One thing to note is that in the literature, when performing the ADM projection of your velocity (which we don't do), you end up with something like:

$$u^\mu = \Gamma(n^\mu + V^\mu)$$

$\Gamma$ is sometimes called a lorentz factor (or, in a similar decomposition: energy). Setting $^\mu=0$, you get:

$$u^0 = \frac{\Gamma}{\alpha}$$

This was quite confusing to me for a while, as you end up different equations for the stress-energy tensor, and $\Gamma$ is not directly the lorentz factor

# The Dirac Delta Function $\delta$ is the final boss of C++/IEEE

The dirac delta function has a very obtuse and non useful definition:

$$\delta(x) =
\begin{align}
\begin{cases}
0, \; &x \neq 0 \\
\infty, \; &x = 0
\end{cases}
\end{align}
$$

The slightly more useful definition of the dirac delta function is as follows:

$$\int^\infty_{-\infty} \delta(x) \; dx = 1$$

This is the key property of the dirac delta function. It has an integral of $1$, and yet it only applies to a single infinitesimal point, with no area. This may seem very curious, but take the example a pointlike particle's contribution to gravity. We know that it has infinitesimal extent as it is pointlike, and so where it interacts with the stress energy tensor must also be an infinitesimal area

This still seems clearly absurd, but the maths works out - and its often described more as an integration trick. People will frequently say, you never actually need to implement this, because it'll disappear. Unfortunately, ours doesn't, and how it is implemented is frequently on some quite shakey grounds

## Implementing the dirac delta function

Numerically, its clear that this function is unworkable. So instead, we want to implement something that approximates the dirac delta function - in essence, you smear it out over a small area. We might demand that this new function has a few properties:

1. It is compactly supported, ie it has finite extent
2. It is smooth-ish
3. This function has an integral of 1

Lets examine a basic dirac delta discretisation, as a triangle:

$$\Delta(x) =
\begin{align}
\begin{cases}
1 - x, \; &\mid x\mid < 1 \\
0, \; \mathrm{otherwise}
\end{cases}
\end{align}
$$

This function spans the range $-1$ to $1$. The integral of $1-x$ from $0$ to $1$ is $0.5$, so its area is correct. This is a valid, albeit not very smooth, discretisation

## Low grid resolutions

One of the things we most want with the dirac delta function - when discretised - is to preserve the *volume*. That means that this sum:

$$\sum_1^n \Delta(x_n) \; h$$

Should evaluate to exactly $1$, where $h$ is the grid spacing. Now, our naive discretisation like this doesn't actually guarantee that - it only guarantees in the limit as $h \mathrel{-}> 0$. The accuracy of our integral is dependent on the size of the dirac delta discretisation having a sufficient number of grid cells, but the accuracy of our simulation goes down as the number of grid cells covered by the discretisation goes up (as it less accurately approximates the dirac delta function)

Given that a larger dirac discretisation is both less accurate (we're approximating something pointlike!), and will be our #1 performance constraint, making this as precise as possible is very important

The remedy is to calculate the exact amount of the dirac delta function that intersects with our cell. We can do this by integrating:

$$\sigma = \frac{\int_{x - \frac{h}{2}}^{x + \frac{h}{2}} \Delta(y) dy}{h}$$

Where $y$ is our integration variable between the bounds $x - \frac{dx}{2}$ and $x + \frac{dx}{2}$. This leads to an exact conservation of the dirac delta, and the difference can be quite significant. More importantly, it eliminates systematic sampling errors, which is always a strong problem

This also generalises to $n$ dimensions - in 3d:

$$\sigma = \frac{\int_{z - \frac{h}{2}}^{z + \frac{h}{2}} \int_{y - \frac{h}{2}}^{y + \frac{h}{2}} \int_{x - \frac{h}{2}}^{x + \frac{h}{2}} \Delta(\textbf{V}) d\textbf{V}}{h^3}$$

## Of course, its not that simple

In the equation above, $x,y,z$ represent world coordinates of our cell's coordinate position, *not* grid cell coordinates. This means that we will be integrating between arbitrary bounds represented in floating point - and our results won't be symmetric. Eg, if our world is $30$ wide, $h = 0.1$ , and we're looking at two world points at $1$ and $29$, integrating from $0.9 - 1.1$ won't return the same result as $28.9 - 29.1$, as these are inherently not representable with the exact same inaccuracies

Because of this, and because the dirac delta function is defined in this same coordinate system, the above equation is actually a non starter as it is not symmetric, and will introduce systematic asymmetric errors in your simulation. To fix this is quite the pain and, we'll now need a specific dirac delta discretisation in 3d to work with

## The actual dirac delta discretisation

We'll be using xyz